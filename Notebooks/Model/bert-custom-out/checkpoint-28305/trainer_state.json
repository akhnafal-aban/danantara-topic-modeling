{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 28305,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0035329447094152974,
      "grad_norm": 2.683615207672119,
      "learning_rate": 4.996643702526056e-05,
      "loss": 8.0108,
      "step": 20
    },
    {
      "epoch": 0.007065889418830595,
      "grad_norm": 2.7797539234161377,
      "learning_rate": 4.99311075781664e-05,
      "loss": 7.9355,
      "step": 40
    },
    {
      "epoch": 0.010598834128245893,
      "grad_norm": 2.2575297355651855,
      "learning_rate": 4.9895778131072254e-05,
      "loss": 7.868,
      "step": 60
    },
    {
      "epoch": 0.01413177883766119,
      "grad_norm": 3.354442834854126,
      "learning_rate": 4.98604486839781e-05,
      "loss": 7.7754,
      "step": 80
    },
    {
      "epoch": 0.01766472354707649,
      "grad_norm": 2.8683931827545166,
      "learning_rate": 4.982511923688394e-05,
      "loss": 7.7259,
      "step": 100
    },
    {
      "epoch": 0.021197668256491786,
      "grad_norm": 2.4664928913116455,
      "learning_rate": 4.9789789789789795e-05,
      "loss": 7.6384,
      "step": 120
    },
    {
      "epoch": 0.024730612965907083,
      "grad_norm": 2.6690356731414795,
      "learning_rate": 4.975446034269564e-05,
      "loss": 7.5842,
      "step": 140
    },
    {
      "epoch": 0.02826355767532238,
      "grad_norm": 2.3379483222961426,
      "learning_rate": 4.971913089560148e-05,
      "loss": 7.5088,
      "step": 160
    },
    {
      "epoch": 0.03179650238473768,
      "grad_norm": 2.0631320476531982,
      "learning_rate": 4.968380144850733e-05,
      "loss": 7.4531,
      "step": 180
    },
    {
      "epoch": 0.03532944709415298,
      "grad_norm": 2.1972339153289795,
      "learning_rate": 4.9648472001413184e-05,
      "loss": 7.4322,
      "step": 200
    },
    {
      "epoch": 0.03886239180356828,
      "grad_norm": 2.569148540496826,
      "learning_rate": 4.9613142554319024e-05,
      "loss": 7.3419,
      "step": 220
    },
    {
      "epoch": 0.04239533651298357,
      "grad_norm": 1.9342833757400513,
      "learning_rate": 4.957781310722487e-05,
      "loss": 7.3203,
      "step": 240
    },
    {
      "epoch": 0.04592828122239887,
      "grad_norm": 1.8245689868927002,
      "learning_rate": 4.9542483660130725e-05,
      "loss": 7.2736,
      "step": 260
    },
    {
      "epoch": 0.049461225931814165,
      "grad_norm": 2.3579227924346924,
      "learning_rate": 4.9507154213036565e-05,
      "loss": 7.2336,
      "step": 280
    },
    {
      "epoch": 0.052994170641229466,
      "grad_norm": 1.9170620441436768,
      "learning_rate": 4.947182476594241e-05,
      "loss": 7.1919,
      "step": 300
    },
    {
      "epoch": 0.05652711535064476,
      "grad_norm": 2.291919708251953,
      "learning_rate": 4.9436495318848266e-05,
      "loss": 7.1488,
      "step": 320
    },
    {
      "epoch": 0.06006006006006006,
      "grad_norm": 1.5693137645721436,
      "learning_rate": 4.940116587175411e-05,
      "loss": 7.166,
      "step": 340
    },
    {
      "epoch": 0.06359300476947535,
      "grad_norm": 1.8989877700805664,
      "learning_rate": 4.9365836424659954e-05,
      "loss": 7.1372,
      "step": 360
    },
    {
      "epoch": 0.06712594947889065,
      "grad_norm": 2.2817015647888184,
      "learning_rate": 4.933050697756581e-05,
      "loss": 7.048,
      "step": 380
    },
    {
      "epoch": 0.07065889418830595,
      "grad_norm": 2.1504359245300293,
      "learning_rate": 4.929517753047165e-05,
      "loss": 7.0512,
      "step": 400
    },
    {
      "epoch": 0.07419183889772125,
      "grad_norm": 2.077514410018921,
      "learning_rate": 4.9259848083377495e-05,
      "loss": 7.0414,
      "step": 420
    },
    {
      "epoch": 0.07772478360713655,
      "grad_norm": 1.7506104707717896,
      "learning_rate": 4.922451863628334e-05,
      "loss": 6.9792,
      "step": 440
    },
    {
      "epoch": 0.08125772831655184,
      "grad_norm": 2.340592861175537,
      "learning_rate": 4.9189189189189196e-05,
      "loss": 6.9879,
      "step": 460
    },
    {
      "epoch": 0.08479067302596714,
      "grad_norm": 1.9074310064315796,
      "learning_rate": 4.9153859742095037e-05,
      "loss": 6.9388,
      "step": 480
    },
    {
      "epoch": 0.08832361773538244,
      "grad_norm": 1.8146430253982544,
      "learning_rate": 4.9118530295000884e-05,
      "loss": 6.95,
      "step": 500
    },
    {
      "epoch": 0.09185656244479774,
      "grad_norm": 2.0426697731018066,
      "learning_rate": 4.908320084790674e-05,
      "loss": 6.9269,
      "step": 520
    },
    {
      "epoch": 0.09538950715421304,
      "grad_norm": 1.9483922719955444,
      "learning_rate": 4.904787140081258e-05,
      "loss": 6.9056,
      "step": 540
    },
    {
      "epoch": 0.09892245186362833,
      "grad_norm": 1.7405184507369995,
      "learning_rate": 4.9012541953718425e-05,
      "loss": 6.9338,
      "step": 560
    },
    {
      "epoch": 0.10245539657304363,
      "grad_norm": 1.8126312494277954,
      "learning_rate": 4.897721250662428e-05,
      "loss": 6.9242,
      "step": 580
    },
    {
      "epoch": 0.10598834128245893,
      "grad_norm": 1.6868547201156616,
      "learning_rate": 4.894188305953012e-05,
      "loss": 7.0131,
      "step": 600
    },
    {
      "epoch": 0.10952128599187423,
      "grad_norm": 2.1148905754089355,
      "learning_rate": 4.8906553612435966e-05,
      "loss": 6.9411,
      "step": 620
    },
    {
      "epoch": 0.11305423070128952,
      "grad_norm": 1.6967788934707642,
      "learning_rate": 4.8871224165341813e-05,
      "loss": 6.9004,
      "step": 640
    },
    {
      "epoch": 0.11658717541070482,
      "grad_norm": 1.9239510297775269,
      "learning_rate": 4.883589471824766e-05,
      "loss": 6.8109,
      "step": 660
    },
    {
      "epoch": 0.12012012012012012,
      "grad_norm": 1.9013781547546387,
      "learning_rate": 4.880056527115351e-05,
      "loss": 6.8233,
      "step": 680
    },
    {
      "epoch": 0.12365306482953542,
      "grad_norm": 1.7753396034240723,
      "learning_rate": 4.8765235824059355e-05,
      "loss": 6.8247,
      "step": 700
    },
    {
      "epoch": 0.1271860095389507,
      "grad_norm": 1.9389570951461792,
      "learning_rate": 4.87299063769652e-05,
      "loss": 6.7884,
      "step": 720
    },
    {
      "epoch": 0.13071895424836602,
      "grad_norm": 2.0524394512176514,
      "learning_rate": 4.869457692987105e-05,
      "loss": 6.8285,
      "step": 740
    },
    {
      "epoch": 0.1342518989577813,
      "grad_norm": 1.9262992143630981,
      "learning_rate": 4.8659247482776896e-05,
      "loss": 6.8442,
      "step": 760
    },
    {
      "epoch": 0.13778484366719662,
      "grad_norm": 2.000373363494873,
      "learning_rate": 4.862391803568274e-05,
      "loss": 6.8265,
      "step": 780
    },
    {
      "epoch": 0.1413177883766119,
      "grad_norm": 2.0496535301208496,
      "learning_rate": 4.858858858858859e-05,
      "loss": 6.8238,
      "step": 800
    },
    {
      "epoch": 0.1448507330860272,
      "grad_norm": 2.342803478240967,
      "learning_rate": 4.855325914149444e-05,
      "loss": 6.8442,
      "step": 820
    },
    {
      "epoch": 0.1483836777954425,
      "grad_norm": 1.8198450803756714,
      "learning_rate": 4.8517929694400285e-05,
      "loss": 6.6539,
      "step": 840
    },
    {
      "epoch": 0.1519166225048578,
      "grad_norm": 2.0712907314300537,
      "learning_rate": 4.848260024730613e-05,
      "loss": 6.8567,
      "step": 860
    },
    {
      "epoch": 0.1554495672142731,
      "grad_norm": 1.8040601015090942,
      "learning_rate": 4.844727080021198e-05,
      "loss": 6.7614,
      "step": 880
    },
    {
      "epoch": 0.1589825119236884,
      "grad_norm": 1.934409499168396,
      "learning_rate": 4.8411941353117826e-05,
      "loss": 6.8481,
      "step": 900
    },
    {
      "epoch": 0.16251545663310368,
      "grad_norm": 2.131542205810547,
      "learning_rate": 4.837661190602367e-05,
      "loss": 6.8285,
      "step": 920
    },
    {
      "epoch": 0.166048401342519,
      "grad_norm": 2.0100247859954834,
      "learning_rate": 4.834128245892952e-05,
      "loss": 6.759,
      "step": 940
    },
    {
      "epoch": 0.16958134605193428,
      "grad_norm": 1.9129152297973633,
      "learning_rate": 4.830595301183537e-05,
      "loss": 6.7976,
      "step": 960
    },
    {
      "epoch": 0.1731142907613496,
      "grad_norm": 2.2595643997192383,
      "learning_rate": 4.8270623564741214e-05,
      "loss": 6.8251,
      "step": 980
    },
    {
      "epoch": 0.17664723547076489,
      "grad_norm": 2.5192370414733887,
      "learning_rate": 4.823529411764706e-05,
      "loss": 6.7284,
      "step": 1000
    },
    {
      "epoch": 0.18018018018018017,
      "grad_norm": 2.3034186363220215,
      "learning_rate": 4.819996467055291e-05,
      "loss": 6.7646,
      "step": 1020
    },
    {
      "epoch": 0.18371312488959549,
      "grad_norm": 2.1964306831359863,
      "learning_rate": 4.8164635223458756e-05,
      "loss": 6.819,
      "step": 1040
    },
    {
      "epoch": 0.18724606959901077,
      "grad_norm": 1.839044213294983,
      "learning_rate": 4.81293057763646e-05,
      "loss": 6.7918,
      "step": 1060
    },
    {
      "epoch": 0.1907790143084261,
      "grad_norm": 1.7109206914901733,
      "learning_rate": 4.809397632927045e-05,
      "loss": 6.9406,
      "step": 1080
    },
    {
      "epoch": 0.19431195901784137,
      "grad_norm": 1.9614646434783936,
      "learning_rate": 4.80586468821763e-05,
      "loss": 6.7679,
      "step": 1100
    },
    {
      "epoch": 0.19784490372725666,
      "grad_norm": 2.235799551010132,
      "learning_rate": 4.8023317435082144e-05,
      "loss": 6.7764,
      "step": 1120
    },
    {
      "epoch": 0.20137784843667197,
      "grad_norm": 2.2181169986724854,
      "learning_rate": 4.798798798798799e-05,
      "loss": 6.8625,
      "step": 1140
    },
    {
      "epoch": 0.20491079314608726,
      "grad_norm": 2.1500282287597656,
      "learning_rate": 4.795265854089384e-05,
      "loss": 6.8,
      "step": 1160
    },
    {
      "epoch": 0.20844373785550255,
      "grad_norm": 2.2127017974853516,
      "learning_rate": 4.7917329093799685e-05,
      "loss": 6.7504,
      "step": 1180
    },
    {
      "epoch": 0.21197668256491786,
      "grad_norm": 2.309072494506836,
      "learning_rate": 4.788199964670553e-05,
      "loss": 6.7252,
      "step": 1200
    },
    {
      "epoch": 0.21550962727433315,
      "grad_norm": 2.300018072128296,
      "learning_rate": 4.784667019961138e-05,
      "loss": 6.7796,
      "step": 1220
    },
    {
      "epoch": 0.21904257198374846,
      "grad_norm": 1.8594917058944702,
      "learning_rate": 4.781134075251723e-05,
      "loss": 6.7327,
      "step": 1240
    },
    {
      "epoch": 0.22257551669316375,
      "grad_norm": 1.849372148513794,
      "learning_rate": 4.7776011305423074e-05,
      "loss": 6.7828,
      "step": 1260
    },
    {
      "epoch": 0.22610846140257904,
      "grad_norm": 2.0380654335021973,
      "learning_rate": 4.7740681858328914e-05,
      "loss": 6.7214,
      "step": 1280
    },
    {
      "epoch": 0.22964140611199435,
      "grad_norm": 2.030566930770874,
      "learning_rate": 4.770535241123477e-05,
      "loss": 6.7144,
      "step": 1300
    },
    {
      "epoch": 0.23317435082140964,
      "grad_norm": 2.119579553604126,
      "learning_rate": 4.7670022964140615e-05,
      "loss": 6.6236,
      "step": 1320
    },
    {
      "epoch": 0.23670729553082495,
      "grad_norm": 1.930311679840088,
      "learning_rate": 4.7634693517046455e-05,
      "loss": 6.794,
      "step": 1340
    },
    {
      "epoch": 0.24024024024024024,
      "grad_norm": 2.0227980613708496,
      "learning_rate": 4.759936406995231e-05,
      "loss": 6.8195,
      "step": 1360
    },
    {
      "epoch": 0.24377318494965552,
      "grad_norm": 2.4552981853485107,
      "learning_rate": 4.7564034622858156e-05,
      "loss": 6.6986,
      "step": 1380
    },
    {
      "epoch": 0.24730612965907084,
      "grad_norm": 2.314754009246826,
      "learning_rate": 4.7528705175764e-05,
      "loss": 6.7862,
      "step": 1400
    },
    {
      "epoch": 0.2508390743684861,
      "grad_norm": 1.7696623802185059,
      "learning_rate": 4.749337572866985e-05,
      "loss": 6.8351,
      "step": 1420
    },
    {
      "epoch": 0.2543720190779014,
      "grad_norm": 1.8178997039794922,
      "learning_rate": 4.74580462815757e-05,
      "loss": 6.9114,
      "step": 1440
    },
    {
      "epoch": 0.25790496378731675,
      "grad_norm": 2.987628936767578,
      "learning_rate": 4.7422716834481545e-05,
      "loss": 6.6977,
      "step": 1460
    },
    {
      "epoch": 0.26143790849673204,
      "grad_norm": 2.294813871383667,
      "learning_rate": 4.738738738738739e-05,
      "loss": 6.742,
      "step": 1480
    },
    {
      "epoch": 0.2649708532061473,
      "grad_norm": 2.178117513656616,
      "learning_rate": 4.735205794029324e-05,
      "loss": 6.7989,
      "step": 1500
    },
    {
      "epoch": 0.2685037979155626,
      "grad_norm": 1.9481407403945923,
      "learning_rate": 4.7316728493199086e-05,
      "loss": 6.7282,
      "step": 1520
    },
    {
      "epoch": 0.2720367426249779,
      "grad_norm": 2.557772159576416,
      "learning_rate": 4.7281399046104927e-05,
      "loss": 6.7631,
      "step": 1540
    },
    {
      "epoch": 0.27556968733439324,
      "grad_norm": 2.007690906524658,
      "learning_rate": 4.724606959901078e-05,
      "loss": 6.7718,
      "step": 1560
    },
    {
      "epoch": 0.27910263204380853,
      "grad_norm": 1.8125827312469482,
      "learning_rate": 4.721074015191663e-05,
      "loss": 6.8032,
      "step": 1580
    },
    {
      "epoch": 0.2826355767532238,
      "grad_norm": 2.6257681846618652,
      "learning_rate": 4.717541070482247e-05,
      "loss": 6.7575,
      "step": 1600
    },
    {
      "epoch": 0.2861685214626391,
      "grad_norm": 1.847007155418396,
      "learning_rate": 4.714008125772832e-05,
      "loss": 6.6774,
      "step": 1620
    },
    {
      "epoch": 0.2897014661720544,
      "grad_norm": 2.057526111602783,
      "learning_rate": 4.710475181063417e-05,
      "loss": 6.7296,
      "step": 1640
    },
    {
      "epoch": 0.29323441088146973,
      "grad_norm": 2.338334798812866,
      "learning_rate": 4.706942236354001e-05,
      "loss": 6.714,
      "step": 1660
    },
    {
      "epoch": 0.296767355590885,
      "grad_norm": 2.844679594039917,
      "learning_rate": 4.703409291644586e-05,
      "loss": 6.651,
      "step": 1680
    },
    {
      "epoch": 0.3003003003003003,
      "grad_norm": 2.067380666732788,
      "learning_rate": 4.699876346935171e-05,
      "loss": 6.6679,
      "step": 1700
    },
    {
      "epoch": 0.3038332450097156,
      "grad_norm": 2.4735758304595947,
      "learning_rate": 4.696343402225755e-05,
      "loss": 6.7184,
      "step": 1720
    },
    {
      "epoch": 0.3073661897191309,
      "grad_norm": 2.1934547424316406,
      "learning_rate": 4.69281045751634e-05,
      "loss": 6.6747,
      "step": 1740
    },
    {
      "epoch": 0.3108991344285462,
      "grad_norm": 2.6521692276000977,
      "learning_rate": 4.689277512806925e-05,
      "loss": 6.7244,
      "step": 1760
    },
    {
      "epoch": 0.3144320791379615,
      "grad_norm": 2.6712405681610107,
      "learning_rate": 4.685744568097509e-05,
      "loss": 6.7609,
      "step": 1780
    },
    {
      "epoch": 0.3179650238473768,
      "grad_norm": 2.6045799255371094,
      "learning_rate": 4.682211623388094e-05,
      "loss": 6.752,
      "step": 1800
    },
    {
      "epoch": 0.3214979685567921,
      "grad_norm": 1.894092321395874,
      "learning_rate": 4.678678678678679e-05,
      "loss": 6.6854,
      "step": 1820
    },
    {
      "epoch": 0.32503091326620737,
      "grad_norm": 2.3336503505706787,
      "learning_rate": 4.675145733969263e-05,
      "loss": 6.7256,
      "step": 1840
    },
    {
      "epoch": 0.3285638579756227,
      "grad_norm": 2.59755277633667,
      "learning_rate": 4.671612789259848e-05,
      "loss": 6.7418,
      "step": 1860
    },
    {
      "epoch": 0.332096802685038,
      "grad_norm": 2.143507480621338,
      "learning_rate": 4.6680798445504334e-05,
      "loss": 6.7495,
      "step": 1880
    },
    {
      "epoch": 0.3356297473944533,
      "grad_norm": 2.303274631500244,
      "learning_rate": 4.6645468998410174e-05,
      "loss": 6.7115,
      "step": 1900
    },
    {
      "epoch": 0.33916269210386857,
      "grad_norm": 2.8902015686035156,
      "learning_rate": 4.661013955131602e-05,
      "loss": 6.6505,
      "step": 1920
    },
    {
      "epoch": 0.34269563681328385,
      "grad_norm": 2.390597343444824,
      "learning_rate": 4.6574810104221875e-05,
      "loss": 6.7099,
      "step": 1940
    },
    {
      "epoch": 0.3462285815226992,
      "grad_norm": 2.6740565299987793,
      "learning_rate": 4.653948065712772e-05,
      "loss": 6.768,
      "step": 1960
    },
    {
      "epoch": 0.3497615262321145,
      "grad_norm": 2.3432562351226807,
      "learning_rate": 4.650415121003356e-05,
      "loss": 6.7047,
      "step": 1980
    },
    {
      "epoch": 0.35329447094152977,
      "grad_norm": 2.5104739665985107,
      "learning_rate": 4.646882176293941e-05,
      "loss": 6.7879,
      "step": 2000
    },
    {
      "epoch": 0.35682741565094506,
      "grad_norm": 1.8455244302749634,
      "learning_rate": 4.6433492315845264e-05,
      "loss": 6.6336,
      "step": 2020
    },
    {
      "epoch": 0.36036036036036034,
      "grad_norm": 2.9170889854431152,
      "learning_rate": 4.6398162868751104e-05,
      "loss": 6.7523,
      "step": 2040
    },
    {
      "epoch": 0.3638933050697757,
      "grad_norm": 2.563228130340576,
      "learning_rate": 4.636283342165695e-05,
      "loss": 6.6808,
      "step": 2060
    },
    {
      "epoch": 0.36742624977919097,
      "grad_norm": 1.9611750841140747,
      "learning_rate": 4.6327503974562805e-05,
      "loss": 6.6355,
      "step": 2080
    },
    {
      "epoch": 0.37095919448860626,
      "grad_norm": 2.197523593902588,
      "learning_rate": 4.6292174527468646e-05,
      "loss": 6.7347,
      "step": 2100
    },
    {
      "epoch": 0.37449213919802155,
      "grad_norm": 2.1910033226013184,
      "learning_rate": 4.625684508037449e-05,
      "loss": 6.7089,
      "step": 2120
    },
    {
      "epoch": 0.37802508390743683,
      "grad_norm": 2.0366368293762207,
      "learning_rate": 4.6221515633280347e-05,
      "loss": 6.7212,
      "step": 2140
    },
    {
      "epoch": 0.3815580286168522,
      "grad_norm": 2.1675894260406494,
      "learning_rate": 4.618618618618619e-05,
      "loss": 6.634,
      "step": 2160
    },
    {
      "epoch": 0.38509097332626746,
      "grad_norm": 2.282759189605713,
      "learning_rate": 4.6150856739092034e-05,
      "loss": 6.6878,
      "step": 2180
    },
    {
      "epoch": 0.38862391803568275,
      "grad_norm": 2.030883550643921,
      "learning_rate": 4.611552729199788e-05,
      "loss": 6.6864,
      "step": 2200
    },
    {
      "epoch": 0.39215686274509803,
      "grad_norm": 2.156773567199707,
      "learning_rate": 4.608019784490373e-05,
      "loss": 6.6743,
      "step": 2220
    },
    {
      "epoch": 0.3956898074545133,
      "grad_norm": 2.8790721893310547,
      "learning_rate": 4.6044868397809575e-05,
      "loss": 6.6713,
      "step": 2240
    },
    {
      "epoch": 0.3992227521639286,
      "grad_norm": 2.1730294227600098,
      "learning_rate": 4.600953895071542e-05,
      "loss": 6.6626,
      "step": 2260
    },
    {
      "epoch": 0.40275569687334395,
      "grad_norm": 2.567319393157959,
      "learning_rate": 4.597420950362127e-05,
      "loss": 6.6186,
      "step": 2280
    },
    {
      "epoch": 0.40628864158275924,
      "grad_norm": 2.4160470962524414,
      "learning_rate": 4.593888005652712e-05,
      "loss": 6.6283,
      "step": 2300
    },
    {
      "epoch": 0.4098215862921745,
      "grad_norm": 1.8792709112167358,
      "learning_rate": 4.5903550609432964e-05,
      "loss": 6.6903,
      "step": 2320
    },
    {
      "epoch": 0.4133545310015898,
      "grad_norm": 2.195021152496338,
      "learning_rate": 4.586822116233881e-05,
      "loss": 6.6125,
      "step": 2340
    },
    {
      "epoch": 0.4168874757110051,
      "grad_norm": 2.20939564704895,
      "learning_rate": 4.583289171524466e-05,
      "loss": 6.6067,
      "step": 2360
    },
    {
      "epoch": 0.42042042042042044,
      "grad_norm": 2.6526105403900146,
      "learning_rate": 4.5797562268150505e-05,
      "loss": 6.5557,
      "step": 2380
    },
    {
      "epoch": 0.4239533651298357,
      "grad_norm": 1.6384385824203491,
      "learning_rate": 4.576223282105635e-05,
      "loss": 6.6324,
      "step": 2400
    },
    {
      "epoch": 0.427486309839251,
      "grad_norm": 2.1516504287719727,
      "learning_rate": 4.57269033739622e-05,
      "loss": 6.6926,
      "step": 2420
    },
    {
      "epoch": 0.4310192545486663,
      "grad_norm": 2.0730748176574707,
      "learning_rate": 4.5691573926868046e-05,
      "loss": 6.6667,
      "step": 2440
    },
    {
      "epoch": 0.4345521992580816,
      "grad_norm": 2.39707088470459,
      "learning_rate": 4.5656244479773893e-05,
      "loss": 6.6622,
      "step": 2460
    },
    {
      "epoch": 0.4380851439674969,
      "grad_norm": 2.1859898567199707,
      "learning_rate": 4.562091503267974e-05,
      "loss": 6.6986,
      "step": 2480
    },
    {
      "epoch": 0.4416180886769122,
      "grad_norm": 2.583988904953003,
      "learning_rate": 4.558558558558559e-05,
      "loss": 6.6022,
      "step": 2500
    },
    {
      "epoch": 0.4451510333863275,
      "grad_norm": 2.321049213409424,
      "learning_rate": 4.5550256138491435e-05,
      "loss": 6.6774,
      "step": 2520
    },
    {
      "epoch": 0.4486839780957428,
      "grad_norm": 2.142622709274292,
      "learning_rate": 4.551492669139728e-05,
      "loss": 6.437,
      "step": 2540
    },
    {
      "epoch": 0.4522169228051581,
      "grad_norm": 3.4719231128692627,
      "learning_rate": 4.547959724430313e-05,
      "loss": 6.5808,
      "step": 2560
    },
    {
      "epoch": 0.4557498675145734,
      "grad_norm": 3.1651856899261475,
      "learning_rate": 4.5444267797208976e-05,
      "loss": 6.4727,
      "step": 2580
    },
    {
      "epoch": 0.4592828122239887,
      "grad_norm": 3.0700454711914062,
      "learning_rate": 4.540893835011482e-05,
      "loss": 6.5334,
      "step": 2600
    },
    {
      "epoch": 0.462815756933404,
      "grad_norm": 2.4575114250183105,
      "learning_rate": 4.537360890302067e-05,
      "loss": 6.6784,
      "step": 2620
    },
    {
      "epoch": 0.4663487016428193,
      "grad_norm": 2.4252641201019287,
      "learning_rate": 4.533827945592652e-05,
      "loss": 6.6603,
      "step": 2640
    },
    {
      "epoch": 0.46988164635223456,
      "grad_norm": 2.4538395404815674,
      "learning_rate": 4.5302950008832365e-05,
      "loss": 6.5085,
      "step": 2660
    },
    {
      "epoch": 0.4734145910616499,
      "grad_norm": 2.6124327182769775,
      "learning_rate": 4.526762056173821e-05,
      "loss": 6.5961,
      "step": 2680
    },
    {
      "epoch": 0.4769475357710652,
      "grad_norm": 3.1055290699005127,
      "learning_rate": 4.523229111464406e-05,
      "loss": 6.698,
      "step": 2700
    },
    {
      "epoch": 0.4804804804804805,
      "grad_norm": 2.473533868789673,
      "learning_rate": 4.5196961667549906e-05,
      "loss": 6.4662,
      "step": 2720
    },
    {
      "epoch": 0.48401342518989576,
      "grad_norm": 2.3743975162506104,
      "learning_rate": 4.516163222045575e-05,
      "loss": 6.51,
      "step": 2740
    },
    {
      "epoch": 0.48754636989931105,
      "grad_norm": 2.372114419937134,
      "learning_rate": 4.51263027733616e-05,
      "loss": 6.5766,
      "step": 2760
    },
    {
      "epoch": 0.4910793146087264,
      "grad_norm": 3.3628714084625244,
      "learning_rate": 4.509097332626745e-05,
      "loss": 6.5705,
      "step": 2780
    },
    {
      "epoch": 0.4946122593181417,
      "grad_norm": 2.380267858505249,
      "learning_rate": 4.5055643879173294e-05,
      "loss": 6.568,
      "step": 2800
    },
    {
      "epoch": 0.49814520402755696,
      "grad_norm": 3.4718165397644043,
      "learning_rate": 4.502031443207914e-05,
      "loss": 6.556,
      "step": 2820
    },
    {
      "epoch": 0.5016781487369723,
      "grad_norm": 2.359971761703491,
      "learning_rate": 4.498498498498498e-05,
      "loss": 6.4647,
      "step": 2840
    },
    {
      "epoch": 0.5052110934463876,
      "grad_norm": 2.4599246978759766,
      "learning_rate": 4.4949655537890836e-05,
      "loss": 6.4877,
      "step": 2860
    },
    {
      "epoch": 0.5087440381558028,
      "grad_norm": 2.4026238918304443,
      "learning_rate": 4.491432609079668e-05,
      "loss": 6.5932,
      "step": 2880
    },
    {
      "epoch": 0.5122769828652182,
      "grad_norm": 2.431063652038574,
      "learning_rate": 4.487899664370252e-05,
      "loss": 6.637,
      "step": 2900
    },
    {
      "epoch": 0.5158099275746335,
      "grad_norm": 3.1459619998931885,
      "learning_rate": 4.484366719660838e-05,
      "loss": 6.6219,
      "step": 2920
    },
    {
      "epoch": 0.5193428722840487,
      "grad_norm": 2.3631317615509033,
      "learning_rate": 4.4808337749514224e-05,
      "loss": 6.4949,
      "step": 2940
    },
    {
      "epoch": 0.5228758169934641,
      "grad_norm": 3.9190890789031982,
      "learning_rate": 4.477300830242007e-05,
      "loss": 6.6466,
      "step": 2960
    },
    {
      "epoch": 0.5264087617028793,
      "grad_norm": 2.9166815280914307,
      "learning_rate": 4.473767885532592e-05,
      "loss": 6.5314,
      "step": 2980
    },
    {
      "epoch": 0.5299417064122947,
      "grad_norm": 2.465254068374634,
      "learning_rate": 4.4702349408231765e-05,
      "loss": 6.499,
      "step": 3000
    },
    {
      "epoch": 0.53347465112171,
      "grad_norm": 2.0298469066619873,
      "learning_rate": 4.466701996113761e-05,
      "loss": 6.4557,
      "step": 3020
    },
    {
      "epoch": 0.5370075958311252,
      "grad_norm": 2.7058956623077393,
      "learning_rate": 4.463169051404345e-05,
      "loss": 6.6197,
      "step": 3040
    },
    {
      "epoch": 0.5405405405405406,
      "grad_norm": 2.5936856269836426,
      "learning_rate": 4.459636106694931e-05,
      "loss": 6.5585,
      "step": 3060
    },
    {
      "epoch": 0.5440734852499558,
      "grad_norm": 3.034044027328491,
      "learning_rate": 4.4561031619855154e-05,
      "loss": 6.6543,
      "step": 3080
    },
    {
      "epoch": 0.5476064299593711,
      "grad_norm": 2.38102126121521,
      "learning_rate": 4.4525702172760994e-05,
      "loss": 6.5549,
      "step": 3100
    },
    {
      "epoch": 0.5511393746687865,
      "grad_norm": 2.8260974884033203,
      "learning_rate": 4.449037272566685e-05,
      "loss": 6.6116,
      "step": 3120
    },
    {
      "epoch": 0.5546723193782017,
      "grad_norm": 2.0999183654785156,
      "learning_rate": 4.4455043278572695e-05,
      "loss": 6.5706,
      "step": 3140
    },
    {
      "epoch": 0.5582052640876171,
      "grad_norm": 2.773245334625244,
      "learning_rate": 4.4419713831478535e-05,
      "loss": 6.5894,
      "step": 3160
    },
    {
      "epoch": 0.5617382087970323,
      "grad_norm": 3.809401512145996,
      "learning_rate": 4.438438438438439e-05,
      "loss": 6.4921,
      "step": 3180
    },
    {
      "epoch": 0.5652711535064476,
      "grad_norm": 3.2208609580993652,
      "learning_rate": 4.4349054937290236e-05,
      "loss": 6.5769,
      "step": 3200
    },
    {
      "epoch": 0.568804098215863,
      "grad_norm": 3.0859720706939697,
      "learning_rate": 4.431372549019608e-05,
      "loss": 6.4885,
      "step": 3220
    },
    {
      "epoch": 0.5723370429252782,
      "grad_norm": 2.33025860786438,
      "learning_rate": 4.427839604310193e-05,
      "loss": 6.6266,
      "step": 3240
    },
    {
      "epoch": 0.5758699876346935,
      "grad_norm": 3.342343807220459,
      "learning_rate": 4.424306659600778e-05,
      "loss": 6.5257,
      "step": 3260
    },
    {
      "epoch": 0.5794029323441088,
      "grad_norm": 2.975060224533081,
      "learning_rate": 4.420773714891362e-05,
      "loss": 6.5213,
      "step": 3280
    },
    {
      "epoch": 0.5829358770535241,
      "grad_norm": 3.6094179153442383,
      "learning_rate": 4.4172407701819465e-05,
      "loss": 6.5084,
      "step": 3300
    },
    {
      "epoch": 0.5864688217629395,
      "grad_norm": 2.774113655090332,
      "learning_rate": 4.413707825472532e-05,
      "loss": 6.5478,
      "step": 3320
    },
    {
      "epoch": 0.5900017664723547,
      "grad_norm": 2.981888771057129,
      "learning_rate": 4.410174880763116e-05,
      "loss": 6.4848,
      "step": 3340
    },
    {
      "epoch": 0.59353471118177,
      "grad_norm": 2.376561403274536,
      "learning_rate": 4.4066419360537007e-05,
      "loss": 6.4902,
      "step": 3360
    },
    {
      "epoch": 0.5970676558911853,
      "grad_norm": 2.970175266265869,
      "learning_rate": 4.403108991344286e-05,
      "loss": 6.4908,
      "step": 3380
    },
    {
      "epoch": 0.6006006006006006,
      "grad_norm": 2.6904244422912598,
      "learning_rate": 4.39957604663487e-05,
      "loss": 6.5148,
      "step": 3400
    },
    {
      "epoch": 0.604133545310016,
      "grad_norm": 4.1967549324035645,
      "learning_rate": 4.396043101925455e-05,
      "loss": 6.5978,
      "step": 3420
    },
    {
      "epoch": 0.6076664900194312,
      "grad_norm": 3.263247489929199,
      "learning_rate": 4.39251015721604e-05,
      "loss": 6.4145,
      "step": 3440
    },
    {
      "epoch": 0.6111994347288465,
      "grad_norm": 3.399101734161377,
      "learning_rate": 4.388977212506625e-05,
      "loss": 6.5634,
      "step": 3460
    },
    {
      "epoch": 0.6147323794382618,
      "grad_norm": 3.2378203868865967,
      "learning_rate": 4.385444267797209e-05,
      "loss": 6.5135,
      "step": 3480
    },
    {
      "epoch": 0.6182653241476771,
      "grad_norm": 3.4927520751953125,
      "learning_rate": 4.3819113230877936e-05,
      "loss": 6.5818,
      "step": 3500
    },
    {
      "epoch": 0.6217982688570924,
      "grad_norm": 2.7787160873413086,
      "learning_rate": 4.378378378378379e-05,
      "loss": 6.6007,
      "step": 3520
    },
    {
      "epoch": 0.6253312135665077,
      "grad_norm": 2.934175491333008,
      "learning_rate": 4.374845433668963e-05,
      "loss": 6.4752,
      "step": 3540
    },
    {
      "epoch": 0.628864158275923,
      "grad_norm": 2.555938482284546,
      "learning_rate": 4.371312488959548e-05,
      "loss": 6.5905,
      "step": 3560
    },
    {
      "epoch": 0.6323971029853382,
      "grad_norm": 2.9567513465881348,
      "learning_rate": 4.367779544250133e-05,
      "loss": 6.5616,
      "step": 3580
    },
    {
      "epoch": 0.6359300476947536,
      "grad_norm": 3.3884756565093994,
      "learning_rate": 4.364246599540717e-05,
      "loss": 6.5861,
      "step": 3600
    },
    {
      "epoch": 0.6394629924041689,
      "grad_norm": 2.3398733139038086,
      "learning_rate": 4.360713654831302e-05,
      "loss": 6.3415,
      "step": 3620
    },
    {
      "epoch": 0.6429959371135842,
      "grad_norm": 3.154568910598755,
      "learning_rate": 4.357180710121887e-05,
      "loss": 6.524,
      "step": 3640
    },
    {
      "epoch": 0.6465288818229995,
      "grad_norm": 3.0510473251342773,
      "learning_rate": 4.353647765412471e-05,
      "loss": 6.6177,
      "step": 3660
    },
    {
      "epoch": 0.6500618265324147,
      "grad_norm": 2.8640739917755127,
      "learning_rate": 4.350114820703056e-05,
      "loss": 6.5893,
      "step": 3680
    },
    {
      "epoch": 0.6535947712418301,
      "grad_norm": 2.6784534454345703,
      "learning_rate": 4.3465818759936414e-05,
      "loss": 6.4706,
      "step": 3700
    },
    {
      "epoch": 0.6571277159512454,
      "grad_norm": 4.334136009216309,
      "learning_rate": 4.3430489312842255e-05,
      "loss": 6.497,
      "step": 3720
    },
    {
      "epoch": 0.6606606606606606,
      "grad_norm": 3.0087568759918213,
      "learning_rate": 4.33951598657481e-05,
      "loss": 6.5435,
      "step": 3740
    },
    {
      "epoch": 0.664193605370076,
      "grad_norm": 2.9341373443603516,
      "learning_rate": 4.335983041865395e-05,
      "loss": 6.5148,
      "step": 3760
    },
    {
      "epoch": 0.6677265500794912,
      "grad_norm": 2.8642160892486572,
      "learning_rate": 4.3324500971559796e-05,
      "loss": 6.4558,
      "step": 3780
    },
    {
      "epoch": 0.6712594947889066,
      "grad_norm": 2.8537890911102295,
      "learning_rate": 4.328917152446564e-05,
      "loss": 6.4763,
      "step": 3800
    },
    {
      "epoch": 0.6747924394983219,
      "grad_norm": 2.8616104125976562,
      "learning_rate": 4.325384207737149e-05,
      "loss": 6.522,
      "step": 3820
    },
    {
      "epoch": 0.6783253842077371,
      "grad_norm": 4.21375036239624,
      "learning_rate": 4.321851263027734e-05,
      "loss": 6.5128,
      "step": 3840
    },
    {
      "epoch": 0.6818583289171525,
      "grad_norm": 3.674926996231079,
      "learning_rate": 4.3183183183183184e-05,
      "loss": 6.5165,
      "step": 3860
    },
    {
      "epoch": 0.6853912736265677,
      "grad_norm": 3.2876007556915283,
      "learning_rate": 4.314785373608903e-05,
      "loss": 6.6175,
      "step": 3880
    },
    {
      "epoch": 0.688924218335983,
      "grad_norm": 4.857275009155273,
      "learning_rate": 4.311252428899488e-05,
      "loss": 6.5913,
      "step": 3900
    },
    {
      "epoch": 0.6924571630453984,
      "grad_norm": 2.5502407550811768,
      "learning_rate": 4.3077194841900726e-05,
      "loss": 6.5405,
      "step": 3920
    },
    {
      "epoch": 0.6959901077548136,
      "grad_norm": 2.3610458374023438,
      "learning_rate": 4.304186539480657e-05,
      "loss": 6.4762,
      "step": 3940
    },
    {
      "epoch": 0.699523052464229,
      "grad_norm": 3.0819509029388428,
      "learning_rate": 4.300653594771242e-05,
      "loss": 6.6074,
      "step": 3960
    },
    {
      "epoch": 0.7030559971736442,
      "grad_norm": 3.6429295539855957,
      "learning_rate": 4.297120650061827e-05,
      "loss": 6.5466,
      "step": 3980
    },
    {
      "epoch": 0.7065889418830595,
      "grad_norm": 4.064307689666748,
      "learning_rate": 4.2935877053524114e-05,
      "loss": 6.4735,
      "step": 4000
    },
    {
      "epoch": 0.7101218865924749,
      "grad_norm": 4.286893367767334,
      "learning_rate": 4.290054760642996e-05,
      "loss": 6.4809,
      "step": 4020
    },
    {
      "epoch": 0.7136548313018901,
      "grad_norm": 3.8291451930999756,
      "learning_rate": 4.286521815933581e-05,
      "loss": 6.4875,
      "step": 4040
    },
    {
      "epoch": 0.7171877760113055,
      "grad_norm": 3.9280924797058105,
      "learning_rate": 4.2829888712241655e-05,
      "loss": 6.405,
      "step": 4060
    },
    {
      "epoch": 0.7207207207207207,
      "grad_norm": 2.8444526195526123,
      "learning_rate": 4.27945592651475e-05,
      "loss": 6.5497,
      "step": 4080
    },
    {
      "epoch": 0.724253665430136,
      "grad_norm": 4.162561893463135,
      "learning_rate": 4.275922981805335e-05,
      "loss": 6.4152,
      "step": 4100
    },
    {
      "epoch": 0.7277866101395514,
      "grad_norm": 3.030226945877075,
      "learning_rate": 4.27239003709592e-05,
      "loss": 6.4996,
      "step": 4120
    },
    {
      "epoch": 0.7313195548489666,
      "grad_norm": 2.862948179244995,
      "learning_rate": 4.2688570923865044e-05,
      "loss": 6.4309,
      "step": 4140
    },
    {
      "epoch": 0.7348524995583819,
      "grad_norm": 3.3181357383728027,
      "learning_rate": 4.265324147677089e-05,
      "loss": 6.3885,
      "step": 4160
    },
    {
      "epoch": 0.7383854442677972,
      "grad_norm": 3.3237266540527344,
      "learning_rate": 4.261791202967674e-05,
      "loss": 6.6076,
      "step": 4180
    },
    {
      "epoch": 0.7419183889772125,
      "grad_norm": 4.714998722076416,
      "learning_rate": 4.2582582582582585e-05,
      "loss": 6.4897,
      "step": 4200
    },
    {
      "epoch": 0.7454513336866279,
      "grad_norm": 3.0537607669830322,
      "learning_rate": 4.254725313548843e-05,
      "loss": 6.4726,
      "step": 4220
    },
    {
      "epoch": 0.7489842783960431,
      "grad_norm": 3.08565616607666,
      "learning_rate": 4.251192368839428e-05,
      "loss": 6.5595,
      "step": 4240
    },
    {
      "epoch": 0.7525172231054584,
      "grad_norm": 2.921062707901001,
      "learning_rate": 4.2476594241300126e-05,
      "loss": 6.4846,
      "step": 4260
    },
    {
      "epoch": 0.7560501678148737,
      "grad_norm": 5.1150736808776855,
      "learning_rate": 4.2441264794205974e-05,
      "loss": 6.3493,
      "step": 4280
    },
    {
      "epoch": 0.759583112524289,
      "grad_norm": 2.5190114974975586,
      "learning_rate": 4.240593534711182e-05,
      "loss": 6.442,
      "step": 4300
    },
    {
      "epoch": 0.7631160572337043,
      "grad_norm": 3.412081718444824,
      "learning_rate": 4.237060590001767e-05,
      "loss": 6.5884,
      "step": 4320
    },
    {
      "epoch": 0.7666490019431196,
      "grad_norm": 3.6102046966552734,
      "learning_rate": 4.233527645292351e-05,
      "loss": 6.4394,
      "step": 4340
    },
    {
      "epoch": 0.7701819466525349,
      "grad_norm": 3.0779623985290527,
      "learning_rate": 4.229994700582936e-05,
      "loss": 6.5151,
      "step": 4360
    },
    {
      "epoch": 0.7737148913619502,
      "grad_norm": 2.9720101356506348,
      "learning_rate": 4.226461755873521e-05,
      "loss": 6.4716,
      "step": 4380
    },
    {
      "epoch": 0.7772478360713655,
      "grad_norm": 3.7973170280456543,
      "learning_rate": 4.222928811164105e-05,
      "loss": 6.5299,
      "step": 4400
    },
    {
      "epoch": 0.7807807807807807,
      "grad_norm": 3.837419271469116,
      "learning_rate": 4.21939586645469e-05,
      "loss": 6.5306,
      "step": 4420
    },
    {
      "epoch": 0.7843137254901961,
      "grad_norm": 3.2415614128112793,
      "learning_rate": 4.215862921745275e-05,
      "loss": 6.48,
      "step": 4440
    },
    {
      "epoch": 0.7878466701996114,
      "grad_norm": 3.608278751373291,
      "learning_rate": 4.21232997703586e-05,
      "loss": 6.5021,
      "step": 4460
    },
    {
      "epoch": 0.7913796149090266,
      "grad_norm": 3.4930176734924316,
      "learning_rate": 4.2087970323264445e-05,
      "loss": 6.5106,
      "step": 4480
    },
    {
      "epoch": 0.794912559618442,
      "grad_norm": 2.8488478660583496,
      "learning_rate": 4.205264087617029e-05,
      "loss": 6.4462,
      "step": 4500
    },
    {
      "epoch": 0.7984455043278572,
      "grad_norm": 2.9487850666046143,
      "learning_rate": 4.201731142907614e-05,
      "loss": 6.3977,
      "step": 4520
    },
    {
      "epoch": 0.8019784490372726,
      "grad_norm": 3.9536190032958984,
      "learning_rate": 4.1981981981981986e-05,
      "loss": 6.4786,
      "step": 4540
    },
    {
      "epoch": 0.8055113937466879,
      "grad_norm": 7.987180233001709,
      "learning_rate": 4.194665253488783e-05,
      "loss": 6.448,
      "step": 4560
    },
    {
      "epoch": 0.8090443384561031,
      "grad_norm": 3.5943732261657715,
      "learning_rate": 4.191132308779368e-05,
      "loss": 6.6197,
      "step": 4580
    },
    {
      "epoch": 0.8125772831655185,
      "grad_norm": 4.613889217376709,
      "learning_rate": 4.187599364069952e-05,
      "loss": 6.5381,
      "step": 4600
    },
    {
      "epoch": 0.8161102278749337,
      "grad_norm": 2.730438470840454,
      "learning_rate": 4.1840664193605374e-05,
      "loss": 6.3574,
      "step": 4620
    },
    {
      "epoch": 0.819643172584349,
      "grad_norm": 3.4870707988739014,
      "learning_rate": 4.180533474651122e-05,
      "loss": 6.3988,
      "step": 4640
    },
    {
      "epoch": 0.8231761172937644,
      "grad_norm": 4.14669942855835,
      "learning_rate": 4.177000529941706e-05,
      "loss": 6.3835,
      "step": 4660
    },
    {
      "epoch": 0.8267090620031796,
      "grad_norm": 3.7034811973571777,
      "learning_rate": 4.1734675852322916e-05,
      "loss": 6.4999,
      "step": 4680
    },
    {
      "epoch": 0.830242006712595,
      "grad_norm": 3.4465444087982178,
      "learning_rate": 4.169934640522876e-05,
      "loss": 6.4173,
      "step": 4700
    },
    {
      "epoch": 0.8337749514220102,
      "grad_norm": 3.3313257694244385,
      "learning_rate": 4.16640169581346e-05,
      "loss": 6.4274,
      "step": 4720
    },
    {
      "epoch": 0.8373078961314255,
      "grad_norm": 2.9852707386016846,
      "learning_rate": 4.162868751104046e-05,
      "loss": 6.4782,
      "step": 4740
    },
    {
      "epoch": 0.8408408408408409,
      "grad_norm": 3.9689464569091797,
      "learning_rate": 4.1593358063946304e-05,
      "loss": 6.5851,
      "step": 4760
    },
    {
      "epoch": 0.8443737855502561,
      "grad_norm": 4.855855941772461,
      "learning_rate": 4.1558028616852144e-05,
      "loss": 6.4707,
      "step": 4780
    },
    {
      "epoch": 0.8479067302596714,
      "grad_norm": 3.021798849105835,
      "learning_rate": 4.152269916975799e-05,
      "loss": 6.4257,
      "step": 4800
    },
    {
      "epoch": 0.8514396749690867,
      "grad_norm": 3.7082157135009766,
      "learning_rate": 4.1487369722663845e-05,
      "loss": 6.4139,
      "step": 4820
    },
    {
      "epoch": 0.854972619678502,
      "grad_norm": 4.220890998840332,
      "learning_rate": 4.1452040275569686e-05,
      "loss": 6.4493,
      "step": 4840
    },
    {
      "epoch": 0.8585055643879174,
      "grad_norm": 3.349959373474121,
      "learning_rate": 4.141671082847553e-05,
      "loss": 6.4315,
      "step": 4860
    },
    {
      "epoch": 0.8620385090973326,
      "grad_norm": 3.6458559036254883,
      "learning_rate": 4.138138138138139e-05,
      "loss": 6.4611,
      "step": 4880
    },
    {
      "epoch": 0.8655714538067479,
      "grad_norm": 5.063289642333984,
      "learning_rate": 4.134605193428723e-05,
      "loss": 6.3005,
      "step": 4900
    },
    {
      "epoch": 0.8691043985161632,
      "grad_norm": 3.8073856830596924,
      "learning_rate": 4.1310722487193074e-05,
      "loss": 6.2775,
      "step": 4920
    },
    {
      "epoch": 0.8726373432255785,
      "grad_norm": 2.8843183517456055,
      "learning_rate": 4.127539304009893e-05,
      "loss": 6.4524,
      "step": 4940
    },
    {
      "epoch": 0.8761702879349939,
      "grad_norm": 3.2890615463256836,
      "learning_rate": 4.1240063593004775e-05,
      "loss": 6.3934,
      "step": 4960
    },
    {
      "epoch": 0.8797032326444091,
      "grad_norm": 4.359289169311523,
      "learning_rate": 4.1204734145910616e-05,
      "loss": 6.3113,
      "step": 4980
    },
    {
      "epoch": 0.8832361773538244,
      "grad_norm": 5.378597259521484,
      "learning_rate": 4.116940469881647e-05,
      "loss": 6.3879,
      "step": 5000
    },
    {
      "epoch": 0.8867691220632397,
      "grad_norm": 2.879483461380005,
      "learning_rate": 4.1134075251722317e-05,
      "loss": 6.3229,
      "step": 5020
    },
    {
      "epoch": 0.890302066772655,
      "grad_norm": 3.4469945430755615,
      "learning_rate": 4.109874580462816e-05,
      "loss": 6.3971,
      "step": 5040
    },
    {
      "epoch": 0.8938350114820703,
      "grad_norm": 3.561084508895874,
      "learning_rate": 4.1063416357534004e-05,
      "loss": 6.518,
      "step": 5060
    },
    {
      "epoch": 0.8973679561914856,
      "grad_norm": 3.917222023010254,
      "learning_rate": 4.102808691043986e-05,
      "loss": 6.3242,
      "step": 5080
    },
    {
      "epoch": 0.9009009009009009,
      "grad_norm": 3.0433337688446045,
      "learning_rate": 4.09927574633457e-05,
      "loss": 6.335,
      "step": 5100
    },
    {
      "epoch": 0.9044338456103161,
      "grad_norm": 3.661789894104004,
      "learning_rate": 4.0957428016251545e-05,
      "loss": 6.3658,
      "step": 5120
    },
    {
      "epoch": 0.9079667903197315,
      "grad_norm": 3.185473680496216,
      "learning_rate": 4.09220985691574e-05,
      "loss": 6.4428,
      "step": 5140
    },
    {
      "epoch": 0.9114997350291468,
      "grad_norm": 3.483764171600342,
      "learning_rate": 4.088676912206324e-05,
      "loss": 6.4746,
      "step": 5160
    },
    {
      "epoch": 0.9150326797385621,
      "grad_norm": 2.998976469039917,
      "learning_rate": 4.085143967496909e-05,
      "loss": 6.3894,
      "step": 5180
    },
    {
      "epoch": 0.9185656244479774,
      "grad_norm": 3.1719954013824463,
      "learning_rate": 4.081611022787494e-05,
      "loss": 6.4212,
      "step": 5200
    },
    {
      "epoch": 0.9220985691573926,
      "grad_norm": 3.6603944301605225,
      "learning_rate": 4.078078078078078e-05,
      "loss": 6.3023,
      "step": 5220
    },
    {
      "epoch": 0.925631513866808,
      "grad_norm": 3.398745536804199,
      "learning_rate": 4.074545133368663e-05,
      "loss": 6.4625,
      "step": 5240
    },
    {
      "epoch": 0.9291644585762233,
      "grad_norm": 4.192269802093506,
      "learning_rate": 4.0710121886592475e-05,
      "loss": 6.3724,
      "step": 5260
    },
    {
      "epoch": 0.9326974032856385,
      "grad_norm": 2.9892640113830566,
      "learning_rate": 4.067479243949832e-05,
      "loss": 6.4462,
      "step": 5280
    },
    {
      "epoch": 0.9362303479950539,
      "grad_norm": 3.8443477153778076,
      "learning_rate": 4.063946299240417e-05,
      "loss": 6.4656,
      "step": 5300
    },
    {
      "epoch": 0.9397632927044691,
      "grad_norm": 3.430056095123291,
      "learning_rate": 4.0604133545310016e-05,
      "loss": 6.4829,
      "step": 5320
    },
    {
      "epoch": 0.9432962374138845,
      "grad_norm": 2.7033493518829346,
      "learning_rate": 4.0568804098215863e-05,
      "loss": 6.4955,
      "step": 5340
    },
    {
      "epoch": 0.9468291821232998,
      "grad_norm": 3.7083613872528076,
      "learning_rate": 4.053347465112171e-05,
      "loss": 6.4592,
      "step": 5360
    },
    {
      "epoch": 0.950362126832715,
      "grad_norm": 4.003290176391602,
      "learning_rate": 4.049814520402756e-05,
      "loss": 6.4301,
      "step": 5380
    },
    {
      "epoch": 0.9538950715421304,
      "grad_norm": 4.617523193359375,
      "learning_rate": 4.0462815756933405e-05,
      "loss": 6.3743,
      "step": 5400
    },
    {
      "epoch": 0.9574280162515456,
      "grad_norm": 4.271944999694824,
      "learning_rate": 4.042748630983925e-05,
      "loss": 6.4202,
      "step": 5420
    },
    {
      "epoch": 0.960960960960961,
      "grad_norm": 2.8666749000549316,
      "learning_rate": 4.03921568627451e-05,
      "loss": 6.4166,
      "step": 5440
    },
    {
      "epoch": 0.9644939056703763,
      "grad_norm": 3.4331252574920654,
      "learning_rate": 4.035682741565095e-05,
      "loss": 6.3762,
      "step": 5460
    },
    {
      "epoch": 0.9680268503797915,
      "grad_norm": 4.148056983947754,
      "learning_rate": 4.032149796855679e-05,
      "loss": 6.3438,
      "step": 5480
    },
    {
      "epoch": 0.9715597950892069,
      "grad_norm": 4.143345355987549,
      "learning_rate": 4.028616852146264e-05,
      "loss": 6.4077,
      "step": 5500
    },
    {
      "epoch": 0.9750927397986221,
      "grad_norm": 5.898537635803223,
      "learning_rate": 4.025083907436849e-05,
      "loss": 6.4388,
      "step": 5520
    },
    {
      "epoch": 0.9786256845080374,
      "grad_norm": 3.3279764652252197,
      "learning_rate": 4.0215509627274335e-05,
      "loss": 6.466,
      "step": 5540
    },
    {
      "epoch": 0.9821586292174528,
      "grad_norm": 4.788580417633057,
      "learning_rate": 4.018018018018018e-05,
      "loss": 6.2675,
      "step": 5560
    },
    {
      "epoch": 0.985691573926868,
      "grad_norm": 3.249243974685669,
      "learning_rate": 4.014485073308603e-05,
      "loss": 6.4165,
      "step": 5580
    },
    {
      "epoch": 0.9892245186362834,
      "grad_norm": 9.554505348205566,
      "learning_rate": 4.0109521285991876e-05,
      "loss": 6.4551,
      "step": 5600
    },
    {
      "epoch": 0.9927574633456986,
      "grad_norm": 4.862668037414551,
      "learning_rate": 4.007419183889772e-05,
      "loss": 6.3572,
      "step": 5620
    },
    {
      "epoch": 0.9962904080551139,
      "grad_norm": 4.236467361450195,
      "learning_rate": 4.003886239180357e-05,
      "loss": 6.3864,
      "step": 5640
    },
    {
      "epoch": 0.9998233527645293,
      "grad_norm": 3.4328513145446777,
      "learning_rate": 4.000353294470942e-05,
      "loss": 6.4494,
      "step": 5660
    },
    {
      "epoch": 1.0033562974739445,
      "grad_norm": 3.836534261703491,
      "learning_rate": 3.9968203497615264e-05,
      "loss": 6.3694,
      "step": 5680
    },
    {
      "epoch": 1.0068892421833597,
      "grad_norm": 5.8186564445495605,
      "learning_rate": 3.993287405052111e-05,
      "loss": 6.38,
      "step": 5700
    },
    {
      "epoch": 1.0104221868927752,
      "grad_norm": 5.537466526031494,
      "learning_rate": 3.989754460342696e-05,
      "loss": 6.3975,
      "step": 5720
    },
    {
      "epoch": 1.0139551316021904,
      "grad_norm": 5.3966217041015625,
      "learning_rate": 3.9862215156332806e-05,
      "loss": 6.4566,
      "step": 5740
    },
    {
      "epoch": 1.0174880763116056,
      "grad_norm": 5.08579158782959,
      "learning_rate": 3.982688570923865e-05,
      "loss": 6.2198,
      "step": 5760
    },
    {
      "epoch": 1.021021021021021,
      "grad_norm": 5.076887607574463,
      "learning_rate": 3.97915562621445e-05,
      "loss": 6.2976,
      "step": 5780
    },
    {
      "epoch": 1.0245539657304363,
      "grad_norm": 3.361863374710083,
      "learning_rate": 3.975622681505035e-05,
      "loss": 6.3435,
      "step": 5800
    },
    {
      "epoch": 1.0280869104398516,
      "grad_norm": 3.6135759353637695,
      "learning_rate": 3.9720897367956194e-05,
      "loss": 6.399,
      "step": 5820
    },
    {
      "epoch": 1.031619855149267,
      "grad_norm": 4.072734832763672,
      "learning_rate": 3.968556792086204e-05,
      "loss": 6.4672,
      "step": 5840
    },
    {
      "epoch": 1.0351527998586822,
      "grad_norm": 4.983141899108887,
      "learning_rate": 3.965023847376789e-05,
      "loss": 6.3104,
      "step": 5860
    },
    {
      "epoch": 1.0386857445680975,
      "grad_norm": 4.365246772766113,
      "learning_rate": 3.9614909026673735e-05,
      "loss": 6.3671,
      "step": 5880
    },
    {
      "epoch": 1.0422186892775127,
      "grad_norm": 3.3041017055511475,
      "learning_rate": 3.9579579579579576e-05,
      "loss": 6.4187,
      "step": 5900
    },
    {
      "epoch": 1.0457516339869282,
      "grad_norm": 3.2295382022857666,
      "learning_rate": 3.954425013248543e-05,
      "loss": 6.4137,
      "step": 5920
    },
    {
      "epoch": 1.0492845786963434,
      "grad_norm": 4.35078239440918,
      "learning_rate": 3.950892068539128e-05,
      "loss": 6.3996,
      "step": 5940
    },
    {
      "epoch": 1.0528175234057586,
      "grad_norm": 3.6705515384674072,
      "learning_rate": 3.9473591238297124e-05,
      "loss": 6.4428,
      "step": 5960
    },
    {
      "epoch": 1.056350468115174,
      "grad_norm": 3.165915012359619,
      "learning_rate": 3.943826179120297e-05,
      "loss": 6.3842,
      "step": 5980
    },
    {
      "epoch": 1.0598834128245893,
      "grad_norm": 3.511340379714966,
      "learning_rate": 3.940293234410882e-05,
      "loss": 6.3819,
      "step": 6000
    },
    {
      "epoch": 1.0634163575340045,
      "grad_norm": 3.3639183044433594,
      "learning_rate": 3.9367602897014665e-05,
      "loss": 6.261,
      "step": 6020
    },
    {
      "epoch": 1.0669493022434198,
      "grad_norm": 3.544039249420166,
      "learning_rate": 3.933227344992051e-05,
      "loss": 6.3067,
      "step": 6040
    },
    {
      "epoch": 1.0704822469528352,
      "grad_norm": 5.107892990112305,
      "learning_rate": 3.929694400282636e-05,
      "loss": 6.3645,
      "step": 6060
    },
    {
      "epoch": 1.0740151916622505,
      "grad_norm": 3.1150619983673096,
      "learning_rate": 3.9261614555732206e-05,
      "loss": 6.4013,
      "step": 6080
    },
    {
      "epoch": 1.0775481363716657,
      "grad_norm": 4.009983062744141,
      "learning_rate": 3.922628510863805e-05,
      "loss": 6.5146,
      "step": 6100
    },
    {
      "epoch": 1.0810810810810811,
      "grad_norm": 3.8714215755462646,
      "learning_rate": 3.91909556615439e-05,
      "loss": 6.4358,
      "step": 6120
    },
    {
      "epoch": 1.0846140257904964,
      "grad_norm": 4.679163932800293,
      "learning_rate": 3.915562621444975e-05,
      "loss": 6.3289,
      "step": 6140
    },
    {
      "epoch": 1.0881469704999116,
      "grad_norm": 3.581385612487793,
      "learning_rate": 3.912029676735559e-05,
      "loss": 6.3383,
      "step": 6160
    },
    {
      "epoch": 1.091679915209327,
      "grad_norm": 3.613422155380249,
      "learning_rate": 3.908496732026144e-05,
      "loss": 6.4457,
      "step": 6180
    },
    {
      "epoch": 1.0952128599187423,
      "grad_norm": 5.383790493011475,
      "learning_rate": 3.904963787316729e-05,
      "loss": 6.2431,
      "step": 6200
    },
    {
      "epoch": 1.0987458046281575,
      "grad_norm": 6.367252349853516,
      "learning_rate": 3.901430842607313e-05,
      "loss": 6.324,
      "step": 6220
    },
    {
      "epoch": 1.102278749337573,
      "grad_norm": 3.302290201187134,
      "learning_rate": 3.897897897897898e-05,
      "loss": 6.2124,
      "step": 6240
    },
    {
      "epoch": 1.1058116940469882,
      "grad_norm": 2.9749653339385986,
      "learning_rate": 3.894364953188483e-05,
      "loss": 6.304,
      "step": 6260
    },
    {
      "epoch": 1.1093446387564034,
      "grad_norm": 3.0383639335632324,
      "learning_rate": 3.890832008479067e-05,
      "loss": 6.4079,
      "step": 6280
    },
    {
      "epoch": 1.1128775834658187,
      "grad_norm": 3.889382839202881,
      "learning_rate": 3.8872990637696525e-05,
      "loss": 6.393,
      "step": 6300
    },
    {
      "epoch": 1.1164105281752341,
      "grad_norm": 5.0545454025268555,
      "learning_rate": 3.883766119060237e-05,
      "loss": 6.3824,
      "step": 6320
    },
    {
      "epoch": 1.1199434728846493,
      "grad_norm": 3.298832416534424,
      "learning_rate": 3.880233174350821e-05,
      "loss": 6.4858,
      "step": 6340
    },
    {
      "epoch": 1.1234764175940646,
      "grad_norm": 3.0826449394226074,
      "learning_rate": 3.876700229641406e-05,
      "loss": 6.2228,
      "step": 6360
    },
    {
      "epoch": 1.12700936230348,
      "grad_norm": 4.170497417449951,
      "learning_rate": 3.873167284931991e-05,
      "loss": 6.3152,
      "step": 6380
    },
    {
      "epoch": 1.1305423070128953,
      "grad_norm": 3.5747134685516357,
      "learning_rate": 3.8696343402225753e-05,
      "loss": 6.3825,
      "step": 6400
    },
    {
      "epoch": 1.1340752517223105,
      "grad_norm": 4.336843013763428,
      "learning_rate": 3.86610139551316e-05,
      "loss": 6.4036,
      "step": 6420
    },
    {
      "epoch": 1.1376081964317257,
      "grad_norm": 4.171016216278076,
      "learning_rate": 3.8625684508037454e-05,
      "loss": 6.2702,
      "step": 6440
    },
    {
      "epoch": 1.1411411411411412,
      "grad_norm": 4.588955402374268,
      "learning_rate": 3.85903550609433e-05,
      "loss": 6.3558,
      "step": 6460
    },
    {
      "epoch": 1.1446740858505564,
      "grad_norm": 5.066800594329834,
      "learning_rate": 3.855502561384914e-05,
      "loss": 6.2828,
      "step": 6480
    },
    {
      "epoch": 1.1482070305599716,
      "grad_norm": 3.81943678855896,
      "learning_rate": 3.8519696166754996e-05,
      "loss": 6.5166,
      "step": 6500
    },
    {
      "epoch": 1.151739975269387,
      "grad_norm": 5.590307235717773,
      "learning_rate": 3.848436671966084e-05,
      "loss": 6.4438,
      "step": 6520
    },
    {
      "epoch": 1.1552729199788023,
      "grad_norm": 3.976414680480957,
      "learning_rate": 3.844903727256668e-05,
      "loss": 6.3526,
      "step": 6540
    },
    {
      "epoch": 1.1588058646882176,
      "grad_norm": 3.452045440673828,
      "learning_rate": 3.841370782547253e-05,
      "loss": 6.276,
      "step": 6560
    },
    {
      "epoch": 1.162338809397633,
      "grad_norm": 3.465064764022827,
      "learning_rate": 3.8378378378378384e-05,
      "loss": 6.3984,
      "step": 6580
    },
    {
      "epoch": 1.1658717541070482,
      "grad_norm": 4.051715850830078,
      "learning_rate": 3.8343048931284225e-05,
      "loss": 6.3111,
      "step": 6600
    },
    {
      "epoch": 1.1694046988164635,
      "grad_norm": 3.7922792434692383,
      "learning_rate": 3.830771948419007e-05,
      "loss": 6.186,
      "step": 6620
    },
    {
      "epoch": 1.172937643525879,
      "grad_norm": 4.306680679321289,
      "learning_rate": 3.8272390037095925e-05,
      "loss": 6.3161,
      "step": 6640
    },
    {
      "epoch": 1.1764705882352942,
      "grad_norm": 4.329588413238525,
      "learning_rate": 3.8237060590001766e-05,
      "loss": 6.3169,
      "step": 6660
    },
    {
      "epoch": 1.1800035329447094,
      "grad_norm": 5.385590076446533,
      "learning_rate": 3.820173114290761e-05,
      "loss": 6.3155,
      "step": 6680
    },
    {
      "epoch": 1.1835364776541246,
      "grad_norm": 4.738332271575928,
      "learning_rate": 3.816640169581347e-05,
      "loss": 6.3548,
      "step": 6700
    },
    {
      "epoch": 1.18706942236354,
      "grad_norm": 3.9316627979278564,
      "learning_rate": 3.813107224871931e-05,
      "loss": 6.3914,
      "step": 6720
    },
    {
      "epoch": 1.1906023670729553,
      "grad_norm": 5.154364585876465,
      "learning_rate": 3.8095742801625154e-05,
      "loss": 6.388,
      "step": 6740
    },
    {
      "epoch": 1.1941353117823705,
      "grad_norm": 5.173872470855713,
      "learning_rate": 3.806041335453101e-05,
      "loss": 6.3632,
      "step": 6760
    },
    {
      "epoch": 1.197668256491786,
      "grad_norm": 4.353388786315918,
      "learning_rate": 3.802508390743685e-05,
      "loss": 6.3477,
      "step": 6780
    },
    {
      "epoch": 1.2012012012012012,
      "grad_norm": 3.6183359622955322,
      "learning_rate": 3.7989754460342696e-05,
      "loss": 6.4488,
      "step": 6800
    },
    {
      "epoch": 1.2047341459106164,
      "grad_norm": 6.082581520080566,
      "learning_rate": 3.795442501324854e-05,
      "loss": 6.3298,
      "step": 6820
    },
    {
      "epoch": 1.2082670906200317,
      "grad_norm": 3.998884677886963,
      "learning_rate": 3.791909556615439e-05,
      "loss": 6.3393,
      "step": 6840
    },
    {
      "epoch": 1.2118000353294471,
      "grad_norm": 4.770720481872559,
      "learning_rate": 3.788376611906024e-05,
      "loss": 6.2711,
      "step": 6860
    },
    {
      "epoch": 1.2153329800388624,
      "grad_norm": 3.926170825958252,
      "learning_rate": 3.7848436671966084e-05,
      "loss": 6.4354,
      "step": 6880
    },
    {
      "epoch": 1.2188659247482776,
      "grad_norm": 4.271915912628174,
      "learning_rate": 3.781310722487193e-05,
      "loss": 6.411,
      "step": 6900
    },
    {
      "epoch": 1.222398869457693,
      "grad_norm": 4.395112037658691,
      "learning_rate": 3.777777777777778e-05,
      "loss": 6.4081,
      "step": 6920
    },
    {
      "epoch": 1.2259318141671083,
      "grad_norm": 4.992167949676514,
      "learning_rate": 3.7742448330683625e-05,
      "loss": 6.2629,
      "step": 6940
    },
    {
      "epoch": 1.2294647588765235,
      "grad_norm": 4.1270904541015625,
      "learning_rate": 3.770711888358948e-05,
      "loss": 6.2961,
      "step": 6960
    },
    {
      "epoch": 1.232997703585939,
      "grad_norm": 4.171128749847412,
      "learning_rate": 3.767178943649532e-05,
      "loss": 6.3113,
      "step": 6980
    },
    {
      "epoch": 1.2365306482953542,
      "grad_norm": 3.5658841133117676,
      "learning_rate": 3.763645998940117e-05,
      "loss": 6.3784,
      "step": 7000
    },
    {
      "epoch": 1.2400635930047694,
      "grad_norm": 5.047974109649658,
      "learning_rate": 3.7601130542307014e-05,
      "loss": 6.2625,
      "step": 7020
    },
    {
      "epoch": 1.2435965377141849,
      "grad_norm": 4.070808410644531,
      "learning_rate": 3.756580109521286e-05,
      "loss": 6.2395,
      "step": 7040
    },
    {
      "epoch": 1.2471294824236,
      "grad_norm": 4.036970615386963,
      "learning_rate": 3.753047164811871e-05,
      "loss": 6.3784,
      "step": 7060
    },
    {
      "epoch": 1.2506624271330153,
      "grad_norm": 2.95727801322937,
      "learning_rate": 3.7495142201024555e-05,
      "loss": 6.21,
      "step": 7080
    },
    {
      "epoch": 1.2541953718424308,
      "grad_norm": 4.65451717376709,
      "learning_rate": 3.74598127539304e-05,
      "loss": 6.2826,
      "step": 7100
    },
    {
      "epoch": 1.257728316551846,
      "grad_norm": 4.432318210601807,
      "learning_rate": 3.742448330683625e-05,
      "loss": 6.3069,
      "step": 7120
    },
    {
      "epoch": 1.2612612612612613,
      "grad_norm": 7.537336349487305,
      "learning_rate": 3.7389153859742096e-05,
      "loss": 6.3749,
      "step": 7140
    },
    {
      "epoch": 1.2647942059706765,
      "grad_norm": 6.164339542388916,
      "learning_rate": 3.7353824412647944e-05,
      "loss": 6.2912,
      "step": 7160
    },
    {
      "epoch": 1.2683271506800917,
      "grad_norm": 5.775334358215332,
      "learning_rate": 3.731849496555379e-05,
      "loss": 6.2355,
      "step": 7180
    },
    {
      "epoch": 1.2718600953895072,
      "grad_norm": 4.303321838378906,
      "learning_rate": 3.728316551845964e-05,
      "loss": 6.3612,
      "step": 7200
    },
    {
      "epoch": 1.2753930400989224,
      "grad_norm": 5.383946895599365,
      "learning_rate": 3.7247836071365485e-05,
      "loss": 6.2851,
      "step": 7220
    },
    {
      "epoch": 1.2789259848083376,
      "grad_norm": 4.298056125640869,
      "learning_rate": 3.721250662427133e-05,
      "loss": 6.3921,
      "step": 7240
    },
    {
      "epoch": 1.282458929517753,
      "grad_norm": 4.410927772521973,
      "learning_rate": 3.717717717717718e-05,
      "loss": 6.3124,
      "step": 7260
    },
    {
      "epoch": 1.2859918742271683,
      "grad_norm": 3.493422508239746,
      "learning_rate": 3.7141847730083026e-05,
      "loss": 6.2087,
      "step": 7280
    },
    {
      "epoch": 1.2895248189365836,
      "grad_norm": 4.084041118621826,
      "learning_rate": 3.710651828298887e-05,
      "loss": 6.2979,
      "step": 7300
    },
    {
      "epoch": 1.293057763645999,
      "grad_norm": 5.422825336456299,
      "learning_rate": 3.707118883589472e-05,
      "loss": 6.3977,
      "step": 7320
    },
    {
      "epoch": 1.2965907083554142,
      "grad_norm": 3.544994831085205,
      "learning_rate": 3.703585938880057e-05,
      "loss": 6.3368,
      "step": 7340
    },
    {
      "epoch": 1.3001236530648295,
      "grad_norm": 4.727290630340576,
      "learning_rate": 3.7000529941706415e-05,
      "loss": 6.2681,
      "step": 7360
    },
    {
      "epoch": 1.303656597774245,
      "grad_norm": 3.7608642578125,
      "learning_rate": 3.696520049461226e-05,
      "loss": 6.4612,
      "step": 7380
    },
    {
      "epoch": 1.3071895424836601,
      "grad_norm": 7.618465900421143,
      "learning_rate": 3.69298710475181e-05,
      "loss": 6.3247,
      "step": 7400
    },
    {
      "epoch": 1.3107224871930754,
      "grad_norm": 8.4298095703125,
      "learning_rate": 3.6894541600423956e-05,
      "loss": 6.2552,
      "step": 7420
    },
    {
      "epoch": 1.3142554319024908,
      "grad_norm": 5.088587760925293,
      "learning_rate": 3.68592121533298e-05,
      "loss": 6.2283,
      "step": 7440
    },
    {
      "epoch": 1.317788376611906,
      "grad_norm": 4.759389400482178,
      "learning_rate": 3.682388270623565e-05,
      "loss": 6.2725,
      "step": 7460
    },
    {
      "epoch": 1.3213213213213213,
      "grad_norm": 3.7007734775543213,
      "learning_rate": 3.67885532591415e-05,
      "loss": 6.2717,
      "step": 7480
    },
    {
      "epoch": 1.3248542660307367,
      "grad_norm": 3.4482884407043457,
      "learning_rate": 3.6753223812047344e-05,
      "loss": 6.3157,
      "step": 7500
    },
    {
      "epoch": 1.328387210740152,
      "grad_norm": 4.700312614440918,
      "learning_rate": 3.671789436495319e-05,
      "loss": 6.1854,
      "step": 7520
    },
    {
      "epoch": 1.3319201554495672,
      "grad_norm": 4.784731864929199,
      "learning_rate": 3.668256491785904e-05,
      "loss": 6.2618,
      "step": 7540
    },
    {
      "epoch": 1.3354531001589824,
      "grad_norm": 4.115416526794434,
      "learning_rate": 3.6647235470764886e-05,
      "loss": 6.3145,
      "step": 7560
    },
    {
      "epoch": 1.3389860448683977,
      "grad_norm": 4.396495342254639,
      "learning_rate": 3.661190602367073e-05,
      "loss": 6.2479,
      "step": 7580
    },
    {
      "epoch": 1.3425189895778131,
      "grad_norm": 4.917762756347656,
      "learning_rate": 3.657657657657658e-05,
      "loss": 6.307,
      "step": 7600
    },
    {
      "epoch": 1.3460519342872284,
      "grad_norm": 3.5154755115509033,
      "learning_rate": 3.654124712948243e-05,
      "loss": 6.2646,
      "step": 7620
    },
    {
      "epoch": 1.3495848789966436,
      "grad_norm": 3.8172357082366943,
      "learning_rate": 3.6505917682388274e-05,
      "loss": 6.2823,
      "step": 7640
    },
    {
      "epoch": 1.353117823706059,
      "grad_norm": 3.5996503829956055,
      "learning_rate": 3.6470588235294114e-05,
      "loss": 6.2736,
      "step": 7660
    },
    {
      "epoch": 1.3566507684154743,
      "grad_norm": 3.419260263442993,
      "learning_rate": 3.643525878819997e-05,
      "loss": 6.3089,
      "step": 7680
    },
    {
      "epoch": 1.3601837131248895,
      "grad_norm": 4.129175662994385,
      "learning_rate": 3.6399929341105815e-05,
      "loss": 6.2924,
      "step": 7700
    },
    {
      "epoch": 1.363716657834305,
      "grad_norm": 5.964575290679932,
      "learning_rate": 3.6364599894011656e-05,
      "loss": 6.2226,
      "step": 7720
    },
    {
      "epoch": 1.3672496025437202,
      "grad_norm": 5.6501688957214355,
      "learning_rate": 3.632927044691751e-05,
      "loss": 6.323,
      "step": 7740
    },
    {
      "epoch": 1.3707825472531354,
      "grad_norm": 3.8760764598846436,
      "learning_rate": 3.629394099982336e-05,
      "loss": 6.2982,
      "step": 7760
    },
    {
      "epoch": 1.3743154919625509,
      "grad_norm": 3.3824141025543213,
      "learning_rate": 3.62586115527292e-05,
      "loss": 6.3208,
      "step": 7780
    },
    {
      "epoch": 1.377848436671966,
      "grad_norm": 3.418440341949463,
      "learning_rate": 3.622328210563505e-05,
      "loss": 6.3992,
      "step": 7800
    },
    {
      "epoch": 1.3813813813813813,
      "grad_norm": 3.9110217094421387,
      "learning_rate": 3.61879526585409e-05,
      "loss": 6.2646,
      "step": 7820
    },
    {
      "epoch": 1.3849143260907968,
      "grad_norm": 4.368138790130615,
      "learning_rate": 3.615262321144674e-05,
      "loss": 6.2258,
      "step": 7840
    },
    {
      "epoch": 1.388447270800212,
      "grad_norm": 4.155664920806885,
      "learning_rate": 3.6117293764352586e-05,
      "loss": 6.2972,
      "step": 7860
    },
    {
      "epoch": 1.3919802155096273,
      "grad_norm": 5.071377277374268,
      "learning_rate": 3.608196431725844e-05,
      "loss": 6.3806,
      "step": 7880
    },
    {
      "epoch": 1.3955131602190427,
      "grad_norm": 6.906059265136719,
      "learning_rate": 3.6046634870164287e-05,
      "loss": 6.275,
      "step": 7900
    },
    {
      "epoch": 1.399046104928458,
      "grad_norm": 4.443281173706055,
      "learning_rate": 3.601130542307013e-05,
      "loss": 6.3242,
      "step": 7920
    },
    {
      "epoch": 1.4025790496378732,
      "grad_norm": 3.8937783241271973,
      "learning_rate": 3.597597597597598e-05,
      "loss": 6.2486,
      "step": 7940
    },
    {
      "epoch": 1.4061119943472884,
      "grad_norm": 4.304034233093262,
      "learning_rate": 3.594064652888183e-05,
      "loss": 6.2248,
      "step": 7960
    },
    {
      "epoch": 1.4096449390567036,
      "grad_norm": 4.513320446014404,
      "learning_rate": 3.590531708178767e-05,
      "loss": 6.4,
      "step": 7980
    },
    {
      "epoch": 1.413177883766119,
      "grad_norm": 3.302543878555298,
      "learning_rate": 3.586998763469352e-05,
      "loss": 6.3391,
      "step": 8000
    },
    {
      "epoch": 1.4167108284755343,
      "grad_norm": 4.177956581115723,
      "learning_rate": 3.583465818759937e-05,
      "loss": 6.1967,
      "step": 8020
    },
    {
      "epoch": 1.4202437731849495,
      "grad_norm": 4.114602088928223,
      "learning_rate": 3.579932874050521e-05,
      "loss": 6.2394,
      "step": 8040
    },
    {
      "epoch": 1.423776717894365,
      "grad_norm": 4.144913196563721,
      "learning_rate": 3.576399929341106e-05,
      "loss": 6.4194,
      "step": 8060
    },
    {
      "epoch": 1.4273096626037802,
      "grad_norm": 3.3678677082061768,
      "learning_rate": 3.572866984631691e-05,
      "loss": 6.3138,
      "step": 8080
    },
    {
      "epoch": 1.4308426073131955,
      "grad_norm": 3.8457224369049072,
      "learning_rate": 3.569334039922275e-05,
      "loss": 6.3807,
      "step": 8100
    },
    {
      "epoch": 1.434375552022611,
      "grad_norm": 4.03484582901001,
      "learning_rate": 3.56580109521286e-05,
      "loss": 6.3291,
      "step": 8120
    },
    {
      "epoch": 1.4379084967320261,
      "grad_norm": 3.5263898372650146,
      "learning_rate": 3.562268150503445e-05,
      "loss": 6.218,
      "step": 8140
    },
    {
      "epoch": 1.4414414414414414,
      "grad_norm": 5.566504955291748,
      "learning_rate": 3.558735205794029e-05,
      "loss": 6.2822,
      "step": 8160
    },
    {
      "epoch": 1.4449743861508568,
      "grad_norm": 4.128684043884277,
      "learning_rate": 3.555202261084614e-05,
      "loss": 6.3453,
      "step": 8180
    },
    {
      "epoch": 1.448507330860272,
      "grad_norm": 4.292187690734863,
      "learning_rate": 3.551669316375199e-05,
      "loss": 6.2972,
      "step": 8200
    },
    {
      "epoch": 1.4520402755696873,
      "grad_norm": 5.460926532745361,
      "learning_rate": 3.5481363716657833e-05,
      "loss": 6.3356,
      "step": 8220
    },
    {
      "epoch": 1.4555732202791027,
      "grad_norm": 6.212639331817627,
      "learning_rate": 3.544603426956368e-05,
      "loss": 6.1659,
      "step": 8240
    },
    {
      "epoch": 1.459106164988518,
      "grad_norm": 7.891108989715576,
      "learning_rate": 3.5410704822469534e-05,
      "loss": 6.3118,
      "step": 8260
    },
    {
      "epoch": 1.4626391096979332,
      "grad_norm": 7.693518161773682,
      "learning_rate": 3.5375375375375375e-05,
      "loss": 6.2116,
      "step": 8280
    },
    {
      "epoch": 1.4661720544073487,
      "grad_norm": 5.5785932540893555,
      "learning_rate": 3.534004592828122e-05,
      "loss": 6.3817,
      "step": 8300
    },
    {
      "epoch": 1.4697049991167639,
      "grad_norm": 5.366420745849609,
      "learning_rate": 3.530471648118707e-05,
      "loss": 6.2835,
      "step": 8320
    },
    {
      "epoch": 1.4732379438261791,
      "grad_norm": 4.074183940887451,
      "learning_rate": 3.5269387034092916e-05,
      "loss": 6.4119,
      "step": 8340
    },
    {
      "epoch": 1.4767708885355944,
      "grad_norm": 4.574335098266602,
      "learning_rate": 3.523405758699876e-05,
      "loss": 6.1492,
      "step": 8360
    },
    {
      "epoch": 1.4803038332450096,
      "grad_norm": 5.422031402587891,
      "learning_rate": 3.519872813990461e-05,
      "loss": 6.3266,
      "step": 8380
    },
    {
      "epoch": 1.483836777954425,
      "grad_norm": 5.017958164215088,
      "learning_rate": 3.5163398692810464e-05,
      "loss": 6.3284,
      "step": 8400
    },
    {
      "epoch": 1.4873697226638403,
      "grad_norm": 3.5584137439727783,
      "learning_rate": 3.5128069245716305e-05,
      "loss": 6.2386,
      "step": 8420
    },
    {
      "epoch": 1.4909026673732555,
      "grad_norm": 4.574039459228516,
      "learning_rate": 3.509273979862215e-05,
      "loss": 6.328,
      "step": 8440
    },
    {
      "epoch": 1.494435612082671,
      "grad_norm": 3.9543652534484863,
      "learning_rate": 3.5057410351528006e-05,
      "loss": 6.3037,
      "step": 8460
    },
    {
      "epoch": 1.4979685567920862,
      "grad_norm": 3.86936616897583,
      "learning_rate": 3.5022080904433846e-05,
      "loss": 6.3223,
      "step": 8480
    },
    {
      "epoch": 1.5015015015015014,
      "grad_norm": 6.22467565536499,
      "learning_rate": 3.498675145733969e-05,
      "loss": 6.2614,
      "step": 8500
    },
    {
      "epoch": 1.5050344462109169,
      "grad_norm": 5.076934337615967,
      "learning_rate": 3.495142201024555e-05,
      "loss": 6.289,
      "step": 8520
    },
    {
      "epoch": 1.508567390920332,
      "grad_norm": 4.382534503936768,
      "learning_rate": 3.491609256315139e-05,
      "loss": 6.3662,
      "step": 8540
    },
    {
      "epoch": 1.5121003356297473,
      "grad_norm": 8.00806999206543,
      "learning_rate": 3.4880763116057234e-05,
      "loss": 6.2985,
      "step": 8560
    },
    {
      "epoch": 1.5156332803391628,
      "grad_norm": 4.718300819396973,
      "learning_rate": 3.484543366896308e-05,
      "loss": 6.4049,
      "step": 8580
    },
    {
      "epoch": 1.519166225048578,
      "grad_norm": 4.809146881103516,
      "learning_rate": 3.481010422186893e-05,
      "loss": 6.1699,
      "step": 8600
    },
    {
      "epoch": 1.5226991697579932,
      "grad_norm": 3.5759758949279785,
      "learning_rate": 3.4774774774774776e-05,
      "loss": 6.3421,
      "step": 8620
    },
    {
      "epoch": 1.5262321144674087,
      "grad_norm": 3.851015090942383,
      "learning_rate": 3.473944532768062e-05,
      "loss": 6.1232,
      "step": 8640
    },
    {
      "epoch": 1.529765059176824,
      "grad_norm": 4.511651039123535,
      "learning_rate": 3.470411588058647e-05,
      "loss": 6.2715,
      "step": 8660
    },
    {
      "epoch": 1.5332980038862392,
      "grad_norm": 4.304897308349609,
      "learning_rate": 3.466878643349232e-05,
      "loss": 6.1455,
      "step": 8680
    },
    {
      "epoch": 1.5368309485956546,
      "grad_norm": 4.315720081329346,
      "learning_rate": 3.4633456986398164e-05,
      "loss": 6.3403,
      "step": 8700
    },
    {
      "epoch": 1.5403638933050696,
      "grad_norm": 5.3416218757629395,
      "learning_rate": 3.459812753930401e-05,
      "loss": 6.1973,
      "step": 8720
    },
    {
      "epoch": 1.543896838014485,
      "grad_norm": 4.0190277099609375,
      "learning_rate": 3.456279809220986e-05,
      "loss": 6.2449,
      "step": 8740
    },
    {
      "epoch": 1.5474297827239005,
      "grad_norm": 4.851346015930176,
      "learning_rate": 3.4527468645115705e-05,
      "loss": 6.361,
      "step": 8760
    },
    {
      "epoch": 1.5509627274333155,
      "grad_norm": 4.383399963378906,
      "learning_rate": 3.449213919802155e-05,
      "loss": 6.2927,
      "step": 8780
    },
    {
      "epoch": 1.554495672142731,
      "grad_norm": 5.282801151275635,
      "learning_rate": 3.44568097509274e-05,
      "loss": 6.2097,
      "step": 8800
    },
    {
      "epoch": 1.5580286168521462,
      "grad_norm": 3.987938165664673,
      "learning_rate": 3.442148030383325e-05,
      "loss": 6.339,
      "step": 8820
    },
    {
      "epoch": 1.5615615615615615,
      "grad_norm": 5.351476192474365,
      "learning_rate": 3.4386150856739094e-05,
      "loss": 6.2504,
      "step": 8840
    },
    {
      "epoch": 1.565094506270977,
      "grad_norm": 3.7649178504943848,
      "learning_rate": 3.435082140964494e-05,
      "loss": 6.1247,
      "step": 8860
    },
    {
      "epoch": 1.5686274509803921,
      "grad_norm": 4.936562538146973,
      "learning_rate": 3.431549196255079e-05,
      "loss": 6.1657,
      "step": 8880
    },
    {
      "epoch": 1.5721603956898074,
      "grad_norm": 4.144001007080078,
      "learning_rate": 3.4280162515456635e-05,
      "loss": 6.2591,
      "step": 8900
    },
    {
      "epoch": 1.5756933403992228,
      "grad_norm": 4.293639659881592,
      "learning_rate": 3.424483306836248e-05,
      "loss": 6.3359,
      "step": 8920
    },
    {
      "epoch": 1.579226285108638,
      "grad_norm": 4.130398273468018,
      "learning_rate": 3.420950362126833e-05,
      "loss": 6.4138,
      "step": 8940
    },
    {
      "epoch": 1.5827592298180533,
      "grad_norm": 5.415055274963379,
      "learning_rate": 3.4174174174174176e-05,
      "loss": 6.2572,
      "step": 8960
    },
    {
      "epoch": 1.5862921745274687,
      "grad_norm": 3.8479251861572266,
      "learning_rate": 3.4138844727080024e-05,
      "loss": 6.2011,
      "step": 8980
    },
    {
      "epoch": 1.589825119236884,
      "grad_norm": 4.4061479568481445,
      "learning_rate": 3.410351527998587e-05,
      "loss": 6.2954,
      "step": 9000
    },
    {
      "epoch": 1.5933580639462992,
      "grad_norm": 4.464443683624268,
      "learning_rate": 3.406818583289172e-05,
      "loss": 6.3944,
      "step": 9020
    },
    {
      "epoch": 1.5968910086557146,
      "grad_norm": 4.354587078094482,
      "learning_rate": 3.4032856385797565e-05,
      "loss": 6.2674,
      "step": 9040
    },
    {
      "epoch": 1.6004239533651299,
      "grad_norm": 4.3760294914245605,
      "learning_rate": 3.399752693870341e-05,
      "loss": 6.2714,
      "step": 9060
    },
    {
      "epoch": 1.6039568980745451,
      "grad_norm": 6.825485706329346,
      "learning_rate": 3.396219749160926e-05,
      "loss": 6.1352,
      "step": 9080
    },
    {
      "epoch": 1.6074898427839606,
      "grad_norm": 4.855443477630615,
      "learning_rate": 3.3926868044515106e-05,
      "loss": 6.3451,
      "step": 9100
    },
    {
      "epoch": 1.6110227874933756,
      "grad_norm": 6.4343767166137695,
      "learning_rate": 3.389153859742095e-05,
      "loss": 6.1051,
      "step": 9120
    },
    {
      "epoch": 1.614555732202791,
      "grad_norm": 5.768179416656494,
      "learning_rate": 3.38562091503268e-05,
      "loss": 6.2811,
      "step": 9140
    },
    {
      "epoch": 1.6180886769122065,
      "grad_norm": 4.541106224060059,
      "learning_rate": 3.382087970323264e-05,
      "loss": 6.2998,
      "step": 9160
    },
    {
      "epoch": 1.6216216216216215,
      "grad_norm": 5.232997894287109,
      "learning_rate": 3.3785550256138495e-05,
      "loss": 6.3058,
      "step": 9180
    },
    {
      "epoch": 1.625154566331037,
      "grad_norm": 5.483921527862549,
      "learning_rate": 3.375022080904434e-05,
      "loss": 6.1768,
      "step": 9200
    },
    {
      "epoch": 1.6286875110404522,
      "grad_norm": 4.196094989776611,
      "learning_rate": 3.371489136195018e-05,
      "loss": 6.2874,
      "step": 9220
    },
    {
      "epoch": 1.6322204557498674,
      "grad_norm": 4.813497066497803,
      "learning_rate": 3.3679561914856036e-05,
      "loss": 6.2751,
      "step": 9240
    },
    {
      "epoch": 1.6357534004592829,
      "grad_norm": 4.396677494049072,
      "learning_rate": 3.364423246776188e-05,
      "loss": 6.2325,
      "step": 9260
    },
    {
      "epoch": 1.639286345168698,
      "grad_norm": 4.456203937530518,
      "learning_rate": 3.3608903020667723e-05,
      "loss": 6.2511,
      "step": 9280
    },
    {
      "epoch": 1.6428192898781133,
      "grad_norm": 4.21030330657959,
      "learning_rate": 3.357357357357358e-05,
      "loss": 6.2833,
      "step": 9300
    },
    {
      "epoch": 1.6463522345875288,
      "grad_norm": 4.522396087646484,
      "learning_rate": 3.3538244126479424e-05,
      "loss": 6.2894,
      "step": 9320
    },
    {
      "epoch": 1.649885179296944,
      "grad_norm": 4.143383026123047,
      "learning_rate": 3.3502914679385265e-05,
      "loss": 6.2984,
      "step": 9340
    },
    {
      "epoch": 1.6534181240063592,
      "grad_norm": 3.8840465545654297,
      "learning_rate": 3.346758523229112e-05,
      "loss": 6.1813,
      "step": 9360
    },
    {
      "epoch": 1.6569510687157747,
      "grad_norm": 3.3119056224823,
      "learning_rate": 3.3432255785196966e-05,
      "loss": 6.2737,
      "step": 9380
    },
    {
      "epoch": 1.66048401342519,
      "grad_norm": 6.978363037109375,
      "learning_rate": 3.339692633810281e-05,
      "loss": 6.2157,
      "step": 9400
    },
    {
      "epoch": 1.6640169581346052,
      "grad_norm": 4.828327655792236,
      "learning_rate": 3.336159689100865e-05,
      "loss": 6.309,
      "step": 9420
    },
    {
      "epoch": 1.6675499028440206,
      "grad_norm": 5.033078670501709,
      "learning_rate": 3.332626744391451e-05,
      "loss": 6.2526,
      "step": 9440
    },
    {
      "epoch": 1.6710828475534358,
      "grad_norm": 4.495326519012451,
      "learning_rate": 3.3290937996820354e-05,
      "loss": 6.2116,
      "step": 9460
    },
    {
      "epoch": 1.674615792262851,
      "grad_norm": 5.727184772491455,
      "learning_rate": 3.3255608549726195e-05,
      "loss": 6.2182,
      "step": 9480
    },
    {
      "epoch": 1.6781487369722665,
      "grad_norm": 5.229281902313232,
      "learning_rate": 3.322027910263205e-05,
      "loss": 6.3432,
      "step": 9500
    },
    {
      "epoch": 1.6816816816816815,
      "grad_norm": 4.976431846618652,
      "learning_rate": 3.3184949655537895e-05,
      "loss": 6.0496,
      "step": 9520
    },
    {
      "epoch": 1.685214626391097,
      "grad_norm": 11.44308853149414,
      "learning_rate": 3.3149620208443736e-05,
      "loss": 6.1736,
      "step": 9540
    },
    {
      "epoch": 1.6887475711005124,
      "grad_norm": 5.133945941925049,
      "learning_rate": 3.311429076134959e-05,
      "loss": 6.0922,
      "step": 9560
    },
    {
      "epoch": 1.6922805158099274,
      "grad_norm": 5.487083435058594,
      "learning_rate": 3.307896131425544e-05,
      "loss": 6.3215,
      "step": 9580
    },
    {
      "epoch": 1.695813460519343,
      "grad_norm": 8.63055419921875,
      "learning_rate": 3.304363186716128e-05,
      "loss": 6.2482,
      "step": 9600
    },
    {
      "epoch": 1.6993464052287581,
      "grad_norm": 5.122991561889648,
      "learning_rate": 3.3008302420067124e-05,
      "loss": 6.1698,
      "step": 9620
    },
    {
      "epoch": 1.7028793499381734,
      "grad_norm": 6.0753560066223145,
      "learning_rate": 3.297297297297298e-05,
      "loss": 6.3078,
      "step": 9640
    },
    {
      "epoch": 1.7064122946475888,
      "grad_norm": 3.7454264163970947,
      "learning_rate": 3.293764352587882e-05,
      "loss": 6.2477,
      "step": 9660
    },
    {
      "epoch": 1.709945239357004,
      "grad_norm": 4.791019439697266,
      "learning_rate": 3.2902314078784666e-05,
      "loss": 6.2074,
      "step": 9680
    },
    {
      "epoch": 1.7134781840664193,
      "grad_norm": 4.1393303871154785,
      "learning_rate": 3.286698463169052e-05,
      "loss": 6.239,
      "step": 9700
    },
    {
      "epoch": 1.7170111287758347,
      "grad_norm": 4.475949287414551,
      "learning_rate": 3.283165518459636e-05,
      "loss": 6.2797,
      "step": 9720
    },
    {
      "epoch": 1.72054407348525,
      "grad_norm": 4.532350540161133,
      "learning_rate": 3.279632573750221e-05,
      "loss": 6.2308,
      "step": 9740
    },
    {
      "epoch": 1.7240770181946652,
      "grad_norm": 7.7697625160217285,
      "learning_rate": 3.276099629040806e-05,
      "loss": 6.179,
      "step": 9760
    },
    {
      "epoch": 1.7276099629040806,
      "grad_norm": 5.576974391937256,
      "learning_rate": 3.27256668433139e-05,
      "loss": 6.3851,
      "step": 9780
    },
    {
      "epoch": 1.7311429076134959,
      "grad_norm": 4.836179733276367,
      "learning_rate": 3.269033739621975e-05,
      "loss": 6.1153,
      "step": 9800
    },
    {
      "epoch": 1.734675852322911,
      "grad_norm": 3.856200695037842,
      "learning_rate": 3.26550079491256e-05,
      "loss": 6.1764,
      "step": 9820
    },
    {
      "epoch": 1.7382087970323266,
      "grad_norm": 3.293546676635742,
      "learning_rate": 3.261967850203144e-05,
      "loss": 6.3334,
      "step": 9840
    },
    {
      "epoch": 1.7417417417417418,
      "grad_norm": 4.05333948135376,
      "learning_rate": 3.258434905493729e-05,
      "loss": 6.1331,
      "step": 9860
    },
    {
      "epoch": 1.745274686451157,
      "grad_norm": 4.609803676605225,
      "learning_rate": 3.254901960784314e-05,
      "loss": 6.1799,
      "step": 9880
    },
    {
      "epoch": 1.7488076311605725,
      "grad_norm": 5.241206169128418,
      "learning_rate": 3.251369016074899e-05,
      "loss": 6.155,
      "step": 9900
    },
    {
      "epoch": 1.7523405758699875,
      "grad_norm": 4.675573825836182,
      "learning_rate": 3.247836071365483e-05,
      "loss": 6.2531,
      "step": 9920
    },
    {
      "epoch": 1.755873520579403,
      "grad_norm": 6.323975086212158,
      "learning_rate": 3.244303126656068e-05,
      "loss": 6.2981,
      "step": 9940
    },
    {
      "epoch": 1.7594064652888184,
      "grad_norm": 3.6454074382781982,
      "learning_rate": 3.240770181946653e-05,
      "loss": 6.2896,
      "step": 9960
    },
    {
      "epoch": 1.7629394099982334,
      "grad_norm": 7.746918678283691,
      "learning_rate": 3.237237237237237e-05,
      "loss": 6.0831,
      "step": 9980
    },
    {
      "epoch": 1.7664723547076489,
      "grad_norm": 4.483269214630127,
      "learning_rate": 3.233704292527822e-05,
      "loss": 6.2454,
      "step": 10000
    },
    {
      "epoch": 1.770005299417064,
      "grad_norm": 5.180083751678467,
      "learning_rate": 3.230171347818407e-05,
      "loss": 6.2287,
      "step": 10020
    },
    {
      "epoch": 1.7735382441264793,
      "grad_norm": 4.509536266326904,
      "learning_rate": 3.2266384031089914e-05,
      "loss": 6.2954,
      "step": 10040
    },
    {
      "epoch": 1.7770711888358948,
      "grad_norm": 4.936921119689941,
      "learning_rate": 3.223105458399576e-05,
      "loss": 6.2081,
      "step": 10060
    },
    {
      "epoch": 1.78060413354531,
      "grad_norm": 4.651065349578857,
      "learning_rate": 3.219572513690161e-05,
      "loss": 6.2232,
      "step": 10080
    },
    {
      "epoch": 1.7841370782547252,
      "grad_norm": 3.814239501953125,
      "learning_rate": 3.2160395689807455e-05,
      "loss": 6.1886,
      "step": 10100
    },
    {
      "epoch": 1.7876700229641407,
      "grad_norm": 5.514261722564697,
      "learning_rate": 3.21250662427133e-05,
      "loss": 6.1236,
      "step": 10120
    },
    {
      "epoch": 1.791202967673556,
      "grad_norm": 5.029142379760742,
      "learning_rate": 3.208973679561915e-05,
      "loss": 6.0981,
      "step": 10140
    },
    {
      "epoch": 1.7947359123829711,
      "grad_norm": 4.9806389808654785,
      "learning_rate": 3.2054407348524996e-05,
      "loss": 6.091,
      "step": 10160
    },
    {
      "epoch": 1.7982688570923866,
      "grad_norm": 4.336585998535156,
      "learning_rate": 3.201907790143084e-05,
      "loss": 6.2085,
      "step": 10180
    },
    {
      "epoch": 1.8018018018018018,
      "grad_norm": 4.906468868255615,
      "learning_rate": 3.198374845433669e-05,
      "loss": 6.2998,
      "step": 10200
    },
    {
      "epoch": 1.805334746511217,
      "grad_norm": 4.808584690093994,
      "learning_rate": 3.194841900724254e-05,
      "loss": 6.1001,
      "step": 10220
    },
    {
      "epoch": 1.8088676912206325,
      "grad_norm": 5.354442596435547,
      "learning_rate": 3.1913089560148385e-05,
      "loss": 6.3138,
      "step": 10240
    },
    {
      "epoch": 1.8124006359300477,
      "grad_norm": 3.910701036453247,
      "learning_rate": 3.187776011305423e-05,
      "loss": 6.2659,
      "step": 10260
    },
    {
      "epoch": 1.815933580639463,
      "grad_norm": 5.744259834289551,
      "learning_rate": 3.184243066596008e-05,
      "loss": 6.2779,
      "step": 10280
    },
    {
      "epoch": 1.8194665253488784,
      "grad_norm": 4.942541122436523,
      "learning_rate": 3.1807101218865926e-05,
      "loss": 6.2687,
      "step": 10300
    },
    {
      "epoch": 1.8229994700582934,
      "grad_norm": 5.384326934814453,
      "learning_rate": 3.177177177177177e-05,
      "loss": 6.1234,
      "step": 10320
    },
    {
      "epoch": 1.826532414767709,
      "grad_norm": 4.414157867431641,
      "learning_rate": 3.173644232467762e-05,
      "loss": 6.3036,
      "step": 10340
    },
    {
      "epoch": 1.8300653594771243,
      "grad_norm": 5.059570789337158,
      "learning_rate": 3.170111287758347e-05,
      "loss": 6.2077,
      "step": 10360
    },
    {
      "epoch": 1.8335983041865394,
      "grad_norm": 5.2548723220825195,
      "learning_rate": 3.1665783430489314e-05,
      "loss": 6.1981,
      "step": 10380
    },
    {
      "epoch": 1.8371312488959548,
      "grad_norm": 3.999846935272217,
      "learning_rate": 3.163045398339516e-05,
      "loss": 6.1417,
      "step": 10400
    },
    {
      "epoch": 1.84066419360537,
      "grad_norm": 4.315743923187256,
      "learning_rate": 3.159512453630101e-05,
      "loss": 6.3005,
      "step": 10420
    },
    {
      "epoch": 1.8441971383147853,
      "grad_norm": 5.569835186004639,
      "learning_rate": 3.1559795089206856e-05,
      "loss": 6.2693,
      "step": 10440
    },
    {
      "epoch": 1.8477300830242007,
      "grad_norm": 4.483007431030273,
      "learning_rate": 3.15244656421127e-05,
      "loss": 6.3012,
      "step": 10460
    },
    {
      "epoch": 1.851263027733616,
      "grad_norm": 5.5342230796813965,
      "learning_rate": 3.148913619501855e-05,
      "loss": 6.2403,
      "step": 10480
    },
    {
      "epoch": 1.8547959724430312,
      "grad_norm": 5.961752414703369,
      "learning_rate": 3.14538067479244e-05,
      "loss": 6.0974,
      "step": 10500
    },
    {
      "epoch": 1.8583289171524466,
      "grad_norm": 5.547461032867432,
      "learning_rate": 3.1418477300830244e-05,
      "loss": 6.3115,
      "step": 10520
    },
    {
      "epoch": 1.8618618618618619,
      "grad_norm": 5.512816905975342,
      "learning_rate": 3.138314785373609e-05,
      "loss": 6.1593,
      "step": 10540
    },
    {
      "epoch": 1.865394806571277,
      "grad_norm": 4.852603435516357,
      "learning_rate": 3.134781840664194e-05,
      "loss": 6.302,
      "step": 10560
    },
    {
      "epoch": 1.8689277512806926,
      "grad_norm": 5.117591857910156,
      "learning_rate": 3.1312488959547785e-05,
      "loss": 6.2197,
      "step": 10580
    },
    {
      "epoch": 1.8724606959901078,
      "grad_norm": 7.9850544929504395,
      "learning_rate": 3.127715951245363e-05,
      "loss": 6.1641,
      "step": 10600
    },
    {
      "epoch": 1.875993640699523,
      "grad_norm": 5.233881950378418,
      "learning_rate": 3.124183006535948e-05,
      "loss": 6.2535,
      "step": 10620
    },
    {
      "epoch": 1.8795265854089385,
      "grad_norm": 5.588960647583008,
      "learning_rate": 3.120650061826533e-05,
      "loss": 6.3072,
      "step": 10640
    },
    {
      "epoch": 1.8830595301183537,
      "grad_norm": 5.721825122833252,
      "learning_rate": 3.1171171171171174e-05,
      "loss": 6.2268,
      "step": 10660
    },
    {
      "epoch": 1.886592474827769,
      "grad_norm": 6.667246341705322,
      "learning_rate": 3.113584172407702e-05,
      "loss": 6.3386,
      "step": 10680
    },
    {
      "epoch": 1.8901254195371844,
      "grad_norm": 5.166438579559326,
      "learning_rate": 3.110051227698287e-05,
      "loss": 6.2705,
      "step": 10700
    },
    {
      "epoch": 1.8936583642465994,
      "grad_norm": 4.930816173553467,
      "learning_rate": 3.106518282988871e-05,
      "loss": 6.2058,
      "step": 10720
    },
    {
      "epoch": 1.8971913089560148,
      "grad_norm": 4.448920726776123,
      "learning_rate": 3.102985338279456e-05,
      "loss": 6.3036,
      "step": 10740
    },
    {
      "epoch": 1.9007242536654303,
      "grad_norm": 4.4406890869140625,
      "learning_rate": 3.099452393570041e-05,
      "loss": 6.1537,
      "step": 10760
    },
    {
      "epoch": 1.9042571983748453,
      "grad_norm": 4.810821056365967,
      "learning_rate": 3.095919448860625e-05,
      "loss": 6.3541,
      "step": 10780
    },
    {
      "epoch": 1.9077901430842608,
      "grad_norm": 3.6091244220733643,
      "learning_rate": 3.0923865041512104e-05,
      "loss": 6.1812,
      "step": 10800
    },
    {
      "epoch": 1.911323087793676,
      "grad_norm": 4.219336986541748,
      "learning_rate": 3.088853559441795e-05,
      "loss": 6.1711,
      "step": 10820
    },
    {
      "epoch": 1.9148560325030912,
      "grad_norm": 5.232601165771484,
      "learning_rate": 3.085320614732379e-05,
      "loss": 6.2149,
      "step": 10840
    },
    {
      "epoch": 1.9183889772125067,
      "grad_norm": 4.498899936676025,
      "learning_rate": 3.0817876700229645e-05,
      "loss": 6.1914,
      "step": 10860
    },
    {
      "epoch": 1.921921921921922,
      "grad_norm": 5.865389823913574,
      "learning_rate": 3.078254725313549e-05,
      "loss": 6.2765,
      "step": 10880
    },
    {
      "epoch": 1.9254548666313371,
      "grad_norm": 4.936125755310059,
      "learning_rate": 3.074721780604134e-05,
      "loss": 6.2025,
      "step": 10900
    },
    {
      "epoch": 1.9289878113407526,
      "grad_norm": 6.685454845428467,
      "learning_rate": 3.071188835894718e-05,
      "loss": 6.0462,
      "step": 10920
    },
    {
      "epoch": 1.9325207560501678,
      "grad_norm": 4.856143951416016,
      "learning_rate": 3.067655891185303e-05,
      "loss": 6.298,
      "step": 10940
    },
    {
      "epoch": 1.936053700759583,
      "grad_norm": 3.6789445877075195,
      "learning_rate": 3.064122946475888e-05,
      "loss": 6.2729,
      "step": 10960
    },
    {
      "epoch": 1.9395866454689985,
      "grad_norm": 6.040359020233154,
      "learning_rate": 3.060590001766472e-05,
      "loss": 6.2595,
      "step": 10980
    },
    {
      "epoch": 1.9431195901784137,
      "grad_norm": 8.657103538513184,
      "learning_rate": 3.0570570570570575e-05,
      "loss": 6.1731,
      "step": 11000
    },
    {
      "epoch": 1.946652534887829,
      "grad_norm": 6.031929969787598,
      "learning_rate": 3.053524112347642e-05,
      "loss": 6.3188,
      "step": 11020
    },
    {
      "epoch": 1.9501854795972444,
      "grad_norm": 4.648370742797852,
      "learning_rate": 3.0499911676382266e-05,
      "loss": 6.1393,
      "step": 11040
    },
    {
      "epoch": 1.9537184243066597,
      "grad_norm": 6.995303153991699,
      "learning_rate": 3.0464582229288113e-05,
      "loss": 6.3045,
      "step": 11060
    },
    {
      "epoch": 1.9572513690160749,
      "grad_norm": 4.604763031005859,
      "learning_rate": 3.0429252782193963e-05,
      "loss": 6.1809,
      "step": 11080
    },
    {
      "epoch": 1.9607843137254903,
      "grad_norm": 6.981404781341553,
      "learning_rate": 3.0393923335099807e-05,
      "loss": 6.157,
      "step": 11100
    },
    {
      "epoch": 1.9643172584349053,
      "grad_norm": 7.0014495849609375,
      "learning_rate": 3.0358593888005654e-05,
      "loss": 6.1129,
      "step": 11120
    },
    {
      "epoch": 1.9678502031443208,
      "grad_norm": 5.006554126739502,
      "learning_rate": 3.0323264440911504e-05,
      "loss": 6.1747,
      "step": 11140
    },
    {
      "epoch": 1.9713831478537363,
      "grad_norm": 4.381042957305908,
      "learning_rate": 3.0287934993817345e-05,
      "loss": 6.1385,
      "step": 11160
    },
    {
      "epoch": 1.9749160925631513,
      "grad_norm": 4.245527267456055,
      "learning_rate": 3.0252605546723195e-05,
      "loss": 6.1998,
      "step": 11180
    },
    {
      "epoch": 1.9784490372725667,
      "grad_norm": 5.735147476196289,
      "learning_rate": 3.0217276099629042e-05,
      "loss": 6.2584,
      "step": 11200
    },
    {
      "epoch": 1.981981981981982,
      "grad_norm": 5.81923770904541,
      "learning_rate": 3.0181946652534886e-05,
      "loss": 6.2942,
      "step": 11220
    },
    {
      "epoch": 1.9855149266913972,
      "grad_norm": 4.441771984100342,
      "learning_rate": 3.0146617205440737e-05,
      "loss": 6.1893,
      "step": 11240
    },
    {
      "epoch": 1.9890478714008126,
      "grad_norm": 4.42030143737793,
      "learning_rate": 3.0111287758346584e-05,
      "loss": 6.2987,
      "step": 11260
    },
    {
      "epoch": 1.9925808161102279,
      "grad_norm": 4.095874786376953,
      "learning_rate": 3.0075958311252427e-05,
      "loss": 6.2697,
      "step": 11280
    },
    {
      "epoch": 1.996113760819643,
      "grad_norm": 5.69060754776001,
      "learning_rate": 3.0040628864158278e-05,
      "loss": 6.144,
      "step": 11300
    },
    {
      "epoch": 1.9996467055290585,
      "grad_norm": 5.164163112640381,
      "learning_rate": 3.0005299417064125e-05,
      "loss": 6.2425,
      "step": 11320
    },
    {
      "epoch": 2.0031796502384736,
      "grad_norm": 4.849437713623047,
      "learning_rate": 2.996996996996997e-05,
      "loss": 6.0215,
      "step": 11340
    },
    {
      "epoch": 2.006712594947889,
      "grad_norm": 4.2916579246521,
      "learning_rate": 2.9934640522875816e-05,
      "loss": 6.2118,
      "step": 11360
    },
    {
      "epoch": 2.0102455396573045,
      "grad_norm": 4.9102325439453125,
      "learning_rate": 2.9899311075781666e-05,
      "loss": 6.1621,
      "step": 11380
    },
    {
      "epoch": 2.0137784843667195,
      "grad_norm": 4.042200565338135,
      "learning_rate": 2.9863981628687513e-05,
      "loss": 6.088,
      "step": 11400
    },
    {
      "epoch": 2.017311429076135,
      "grad_norm": 8.44846248626709,
      "learning_rate": 2.9828652181593357e-05,
      "loss": 6.1813,
      "step": 11420
    },
    {
      "epoch": 2.0208443737855504,
      "grad_norm": 5.373264312744141,
      "learning_rate": 2.9793322734499208e-05,
      "loss": 6.0764,
      "step": 11440
    },
    {
      "epoch": 2.0243773184949654,
      "grad_norm": 3.9963080883026123,
      "learning_rate": 2.9757993287405055e-05,
      "loss": 6.098,
      "step": 11460
    },
    {
      "epoch": 2.027910263204381,
      "grad_norm": 5.417449474334717,
      "learning_rate": 2.97226638403109e-05,
      "loss": 6.0788,
      "step": 11480
    },
    {
      "epoch": 2.0314432079137963,
      "grad_norm": 4.6870317459106445,
      "learning_rate": 2.968733439321675e-05,
      "loss": 6.1762,
      "step": 11500
    },
    {
      "epoch": 2.0349761526232113,
      "grad_norm": 4.665510654449463,
      "learning_rate": 2.9652004946122596e-05,
      "loss": 6.1504,
      "step": 11520
    },
    {
      "epoch": 2.0385090973326268,
      "grad_norm": 4.563864707946777,
      "learning_rate": 2.961667549902844e-05,
      "loss": 6.2167,
      "step": 11540
    },
    {
      "epoch": 2.042042042042042,
      "grad_norm": 4.0399675369262695,
      "learning_rate": 2.958134605193429e-05,
      "loss": 6.2667,
      "step": 11560
    },
    {
      "epoch": 2.045574986751457,
      "grad_norm": 5.6391682624816895,
      "learning_rate": 2.9546016604840137e-05,
      "loss": 6.162,
      "step": 11580
    },
    {
      "epoch": 2.0491079314608727,
      "grad_norm": 6.287148952484131,
      "learning_rate": 2.951068715774598e-05,
      "loss": 6.2683,
      "step": 11600
    },
    {
      "epoch": 2.052640876170288,
      "grad_norm": 4.511873722076416,
      "learning_rate": 2.9475357710651828e-05,
      "loss": 6.174,
      "step": 11620
    },
    {
      "epoch": 2.056173820879703,
      "grad_norm": 5.3795695304870605,
      "learning_rate": 2.944002826355768e-05,
      "loss": 6.2656,
      "step": 11640
    },
    {
      "epoch": 2.0597067655891186,
      "grad_norm": 5.2112016677856445,
      "learning_rate": 2.9404698816463522e-05,
      "loss": 6.1855,
      "step": 11660
    },
    {
      "epoch": 2.063239710298534,
      "grad_norm": 4.669471740722656,
      "learning_rate": 2.936936936936937e-05,
      "loss": 6.2666,
      "step": 11680
    },
    {
      "epoch": 2.066772655007949,
      "grad_norm": 4.894561767578125,
      "learning_rate": 2.933403992227522e-05,
      "loss": 6.3001,
      "step": 11700
    },
    {
      "epoch": 2.0703055997173645,
      "grad_norm": 4.899141788482666,
      "learning_rate": 2.9298710475181064e-05,
      "loss": 6.124,
      "step": 11720
    },
    {
      "epoch": 2.0738385444267795,
      "grad_norm": 5.168580532073975,
      "learning_rate": 2.926338102808691e-05,
      "loss": 6.3603,
      "step": 11740
    },
    {
      "epoch": 2.077371489136195,
      "grad_norm": 4.3867506980896,
      "learning_rate": 2.922805158099276e-05,
      "loss": 6.1399,
      "step": 11760
    },
    {
      "epoch": 2.0809044338456104,
      "grad_norm": 4.922942161560059,
      "learning_rate": 2.9192722133898602e-05,
      "loss": 6.1682,
      "step": 11780
    },
    {
      "epoch": 2.0844373785550254,
      "grad_norm": 5.224905490875244,
      "learning_rate": 2.9157392686804452e-05,
      "loss": 6.1376,
      "step": 11800
    },
    {
      "epoch": 2.087970323264441,
      "grad_norm": 5.5642476081848145,
      "learning_rate": 2.91220632397103e-05,
      "loss": 6.1906,
      "step": 11820
    },
    {
      "epoch": 2.0915032679738563,
      "grad_norm": 3.372044324874878,
      "learning_rate": 2.9086733792616143e-05,
      "loss": 6.1673,
      "step": 11840
    },
    {
      "epoch": 2.0950362126832713,
      "grad_norm": 6.141493797302246,
      "learning_rate": 2.9051404345521994e-05,
      "loss": 6.1175,
      "step": 11860
    },
    {
      "epoch": 2.098569157392687,
      "grad_norm": 5.616699695587158,
      "learning_rate": 2.901607489842784e-05,
      "loss": 6.2037,
      "step": 11880
    },
    {
      "epoch": 2.1021021021021022,
      "grad_norm": 7.650850772857666,
      "learning_rate": 2.898074545133369e-05,
      "loss": 6.0289,
      "step": 11900
    },
    {
      "epoch": 2.1056350468115173,
      "grad_norm": 4.054670810699463,
      "learning_rate": 2.8945416004239535e-05,
      "loss": 6.0531,
      "step": 11920
    },
    {
      "epoch": 2.1091679915209327,
      "grad_norm": 4.401232719421387,
      "learning_rate": 2.8910086557145382e-05,
      "loss": 6.0866,
      "step": 11940
    },
    {
      "epoch": 2.112700936230348,
      "grad_norm": 6.295513153076172,
      "learning_rate": 2.8874757110051233e-05,
      "loss": 6.3223,
      "step": 11960
    },
    {
      "epoch": 2.116233880939763,
      "grad_norm": 4.053289413452148,
      "learning_rate": 2.8839427662957076e-05,
      "loss": 6.2624,
      "step": 11980
    },
    {
      "epoch": 2.1197668256491786,
      "grad_norm": 4.423572540283203,
      "learning_rate": 2.8804098215862923e-05,
      "loss": 6.193,
      "step": 12000
    },
    {
      "epoch": 2.123299770358594,
      "grad_norm": 3.9142396450042725,
      "learning_rate": 2.8768768768768774e-05,
      "loss": 6.2469,
      "step": 12020
    },
    {
      "epoch": 2.126832715068009,
      "grad_norm": 4.137810707092285,
      "learning_rate": 2.8733439321674614e-05,
      "loss": 6.0821,
      "step": 12040
    },
    {
      "epoch": 2.1303656597774245,
      "grad_norm": 10.996370315551758,
      "learning_rate": 2.8698109874580465e-05,
      "loss": 6.1597,
      "step": 12060
    },
    {
      "epoch": 2.1338986044868395,
      "grad_norm": 4.9008660316467285,
      "learning_rate": 2.8662780427486312e-05,
      "loss": 6.2354,
      "step": 12080
    },
    {
      "epoch": 2.137431549196255,
      "grad_norm": 4.833057880401611,
      "learning_rate": 2.8627450980392155e-05,
      "loss": 6.2427,
      "step": 12100
    },
    {
      "epoch": 2.1409644939056705,
      "grad_norm": 5.345411777496338,
      "learning_rate": 2.8592121533298006e-05,
      "loss": 6.2486,
      "step": 12120
    },
    {
      "epoch": 2.144497438615086,
      "grad_norm": 7.466638088226318,
      "learning_rate": 2.8556792086203853e-05,
      "loss": 6.0812,
      "step": 12140
    },
    {
      "epoch": 2.148030383324501,
      "grad_norm": 5.629303455352783,
      "learning_rate": 2.8521462639109697e-05,
      "loss": 6.1723,
      "step": 12160
    },
    {
      "epoch": 2.1515633280339164,
      "grad_norm": 4.264660835266113,
      "learning_rate": 2.8486133192015547e-05,
      "loss": 6.1787,
      "step": 12180
    },
    {
      "epoch": 2.1550962727433314,
      "grad_norm": 4.547660827636719,
      "learning_rate": 2.8450803744921394e-05,
      "loss": 6.1038,
      "step": 12200
    },
    {
      "epoch": 2.158629217452747,
      "grad_norm": 8.322354316711426,
      "learning_rate": 2.8415474297827238e-05,
      "loss": 6.0399,
      "step": 12220
    },
    {
      "epoch": 2.1621621621621623,
      "grad_norm": 6.276821136474609,
      "learning_rate": 2.8380144850733085e-05,
      "loss": 6.2082,
      "step": 12240
    },
    {
      "epoch": 2.1656951068715773,
      "grad_norm": 3.801814556121826,
      "learning_rate": 2.8344815403638936e-05,
      "loss": 6.1983,
      "step": 12260
    },
    {
      "epoch": 2.1692280515809927,
      "grad_norm": 5.774707794189453,
      "learning_rate": 2.830948595654478e-05,
      "loss": 6.1685,
      "step": 12280
    },
    {
      "epoch": 2.172760996290408,
      "grad_norm": 3.682412624359131,
      "learning_rate": 2.8274156509450627e-05,
      "loss": 6.074,
      "step": 12300
    },
    {
      "epoch": 2.176293940999823,
      "grad_norm": 6.19378662109375,
      "learning_rate": 2.8238827062356477e-05,
      "loss": 6.2718,
      "step": 12320
    },
    {
      "epoch": 2.1798268857092387,
      "grad_norm": 6.89476203918457,
      "learning_rate": 2.820349761526232e-05,
      "loss": 6.1324,
      "step": 12340
    },
    {
      "epoch": 2.183359830418654,
      "grad_norm": 4.078179836273193,
      "learning_rate": 2.8168168168168168e-05,
      "loss": 6.1857,
      "step": 12360
    },
    {
      "epoch": 2.186892775128069,
      "grad_norm": 6.877439498901367,
      "learning_rate": 2.813283872107402e-05,
      "loss": 6.2517,
      "step": 12380
    },
    {
      "epoch": 2.1904257198374846,
      "grad_norm": 4.913537502288818,
      "learning_rate": 2.8097509273979865e-05,
      "loss": 6.0818,
      "step": 12400
    },
    {
      "epoch": 2.1939586645469,
      "grad_norm": 5.973616600036621,
      "learning_rate": 2.806217982688571e-05,
      "loss": 6.1691,
      "step": 12420
    },
    {
      "epoch": 2.197491609256315,
      "grad_norm": 5.16467809677124,
      "learning_rate": 2.802685037979156e-05,
      "loss": 6.2488,
      "step": 12440
    },
    {
      "epoch": 2.2010245539657305,
      "grad_norm": 5.088154315948486,
      "learning_rate": 2.7991520932697407e-05,
      "loss": 6.2164,
      "step": 12460
    },
    {
      "epoch": 2.204557498675146,
      "grad_norm": 10.48298454284668,
      "learning_rate": 2.795619148560325e-05,
      "loss": 6.1996,
      "step": 12480
    },
    {
      "epoch": 2.208090443384561,
      "grad_norm": 6.613230228424072,
      "learning_rate": 2.7920862038509098e-05,
      "loss": 6.1503,
      "step": 12500
    },
    {
      "epoch": 2.2116233880939764,
      "grad_norm": 5.991113185882568,
      "learning_rate": 2.7885532591414948e-05,
      "loss": 6.2904,
      "step": 12520
    },
    {
      "epoch": 2.2151563328033914,
      "grad_norm": 5.921806812286377,
      "learning_rate": 2.7850203144320792e-05,
      "loss": 6.1066,
      "step": 12540
    },
    {
      "epoch": 2.218689277512807,
      "grad_norm": 5.493920803070068,
      "learning_rate": 2.781487369722664e-05,
      "loss": 6.1761,
      "step": 12560
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 5.949894905090332,
      "learning_rate": 2.777954425013249e-05,
      "loss": 6.0176,
      "step": 12580
    },
    {
      "epoch": 2.2257551669316373,
      "grad_norm": 7.247734546661377,
      "learning_rate": 2.7744214803038333e-05,
      "loss": 6.1725,
      "step": 12600
    },
    {
      "epoch": 2.229288111641053,
      "grad_norm": 4.43519926071167,
      "learning_rate": 2.770888535594418e-05,
      "loss": 6.1538,
      "step": 12620
    },
    {
      "epoch": 2.2328210563504682,
      "grad_norm": 5.901267051696777,
      "learning_rate": 2.767355590885003e-05,
      "loss": 6.2097,
      "step": 12640
    },
    {
      "epoch": 2.2363540010598832,
      "grad_norm": 5.355630397796631,
      "learning_rate": 2.7638226461755875e-05,
      "loss": 6.2382,
      "step": 12660
    },
    {
      "epoch": 2.2398869457692987,
      "grad_norm": 5.784236431121826,
      "learning_rate": 2.760289701466172e-05,
      "loss": 6.1077,
      "step": 12680
    },
    {
      "epoch": 2.243419890478714,
      "grad_norm": 4.422774314880371,
      "learning_rate": 2.7567567567567572e-05,
      "loss": 6.293,
      "step": 12700
    },
    {
      "epoch": 2.246952835188129,
      "grad_norm": 5.514664173126221,
      "learning_rate": 2.7532238120473412e-05,
      "loss": 6.2721,
      "step": 12720
    },
    {
      "epoch": 2.2504857798975446,
      "grad_norm": 5.262328624725342,
      "learning_rate": 2.7496908673379263e-05,
      "loss": 6.159,
      "step": 12740
    },
    {
      "epoch": 2.25401872460696,
      "grad_norm": 5.179184436798096,
      "learning_rate": 2.746157922628511e-05,
      "loss": 6.1807,
      "step": 12760
    },
    {
      "epoch": 2.257551669316375,
      "grad_norm": 5.384062767028809,
      "learning_rate": 2.7426249779190954e-05,
      "loss": 6.2196,
      "step": 12780
    },
    {
      "epoch": 2.2610846140257905,
      "grad_norm": 5.246307849884033,
      "learning_rate": 2.7390920332096804e-05,
      "loss": 6.182,
      "step": 12800
    },
    {
      "epoch": 2.264617558735206,
      "grad_norm": 4.566552639007568,
      "learning_rate": 2.735559088500265e-05,
      "loss": 6.1442,
      "step": 12820
    },
    {
      "epoch": 2.268150503444621,
      "grad_norm": 4.799476146697998,
      "learning_rate": 2.7320261437908495e-05,
      "loss": 6.1403,
      "step": 12840
    },
    {
      "epoch": 2.2716834481540364,
      "grad_norm": 3.398071527481079,
      "learning_rate": 2.7284931990814346e-05,
      "loss": 6.2216,
      "step": 12860
    },
    {
      "epoch": 2.2752163928634515,
      "grad_norm": 4.882523059844971,
      "learning_rate": 2.7249602543720193e-05,
      "loss": 6.1872,
      "step": 12880
    },
    {
      "epoch": 2.278749337572867,
      "grad_norm": 5.747114181518555,
      "learning_rate": 2.7214273096626043e-05,
      "loss": 5.9978,
      "step": 12900
    },
    {
      "epoch": 2.2822822822822824,
      "grad_norm": 4.4921417236328125,
      "learning_rate": 2.7178943649531884e-05,
      "loss": 6.1446,
      "step": 12920
    },
    {
      "epoch": 2.285815226991698,
      "grad_norm": 5.514984607696533,
      "learning_rate": 2.7143614202437734e-05,
      "loss": 6.1829,
      "step": 12940
    },
    {
      "epoch": 2.289348171701113,
      "grad_norm": 5.039333820343018,
      "learning_rate": 2.710828475534358e-05,
      "loss": 6.1593,
      "step": 12960
    },
    {
      "epoch": 2.2928811164105283,
      "grad_norm": 5.333925247192383,
      "learning_rate": 2.7072955308249425e-05,
      "loss": 6.083,
      "step": 12980
    },
    {
      "epoch": 2.2964140611199433,
      "grad_norm": 6.583813667297363,
      "learning_rate": 2.7037625861155275e-05,
      "loss": 6.1938,
      "step": 13000
    },
    {
      "epoch": 2.2999470058293587,
      "grad_norm": 4.219202995300293,
      "learning_rate": 2.7002296414061122e-05,
      "loss": 6.2903,
      "step": 13020
    },
    {
      "epoch": 2.303479950538774,
      "grad_norm": 4.97269344329834,
      "learning_rate": 2.6966966966966966e-05,
      "loss": 6.2572,
      "step": 13040
    },
    {
      "epoch": 2.307012895248189,
      "grad_norm": 7.727025985717773,
      "learning_rate": 2.6931637519872817e-05,
      "loss": 6.1686,
      "step": 13060
    },
    {
      "epoch": 2.3105458399576047,
      "grad_norm": 4.683537006378174,
      "learning_rate": 2.6896308072778664e-05,
      "loss": 6.1729,
      "step": 13080
    },
    {
      "epoch": 2.31407878466702,
      "grad_norm": 4.940415382385254,
      "learning_rate": 2.6860978625684507e-05,
      "loss": 6.2485,
      "step": 13100
    },
    {
      "epoch": 2.317611729376435,
      "grad_norm": 4.605518817901611,
      "learning_rate": 2.6825649178590358e-05,
      "loss": 6.1771,
      "step": 13120
    },
    {
      "epoch": 2.3211446740858506,
      "grad_norm": 5.331225395202637,
      "learning_rate": 2.6790319731496205e-05,
      "loss": 6.0853,
      "step": 13140
    },
    {
      "epoch": 2.324677618795266,
      "grad_norm": 8.275592803955078,
      "learning_rate": 2.675499028440205e-05,
      "loss": 6.298,
      "step": 13160
    },
    {
      "epoch": 2.328210563504681,
      "grad_norm": 6.56551456451416,
      "learning_rate": 2.6719660837307896e-05,
      "loss": 6.1201,
      "step": 13180
    },
    {
      "epoch": 2.3317435082140965,
      "grad_norm": 5.125573635101318,
      "learning_rate": 2.6684331390213746e-05,
      "loss": 6.0869,
      "step": 13200
    },
    {
      "epoch": 2.335276452923512,
      "grad_norm": 4.683995246887207,
      "learning_rate": 2.664900194311959e-05,
      "loss": 6.1353,
      "step": 13220
    },
    {
      "epoch": 2.338809397632927,
      "grad_norm": 5.0016889572143555,
      "learning_rate": 2.6613672496025437e-05,
      "loss": 6.103,
      "step": 13240
    },
    {
      "epoch": 2.3423423423423424,
      "grad_norm": 6.332942485809326,
      "learning_rate": 2.6578343048931288e-05,
      "loss": 6.2297,
      "step": 13260
    },
    {
      "epoch": 2.345875287051758,
      "grad_norm": 4.372300148010254,
      "learning_rate": 2.654301360183713e-05,
      "loss": 6.2958,
      "step": 13280
    },
    {
      "epoch": 2.349408231761173,
      "grad_norm": 6.5925140380859375,
      "learning_rate": 2.650768415474298e-05,
      "loss": 6.1659,
      "step": 13300
    },
    {
      "epoch": 2.3529411764705883,
      "grad_norm": 6.816028118133545,
      "learning_rate": 2.647235470764883e-05,
      "loss": 6.1751,
      "step": 13320
    },
    {
      "epoch": 2.3564741211800033,
      "grad_norm": 5.427158832550049,
      "learning_rate": 2.643702526055467e-05,
      "loss": 6.25,
      "step": 13340
    },
    {
      "epoch": 2.3600070658894188,
      "grad_norm": 5.415367126464844,
      "learning_rate": 2.640169581346052e-05,
      "loss": 6.1793,
      "step": 13360
    },
    {
      "epoch": 2.3635400105988342,
      "grad_norm": 4.068799018859863,
      "learning_rate": 2.6366366366366367e-05,
      "loss": 6.0955,
      "step": 13380
    },
    {
      "epoch": 2.3670729553082492,
      "grad_norm": 4.840312480926514,
      "learning_rate": 2.6331036919272218e-05,
      "loss": 6.2437,
      "step": 13400
    },
    {
      "epoch": 2.3706059000176647,
      "grad_norm": 6.6365814208984375,
      "learning_rate": 2.629570747217806e-05,
      "loss": 6.2847,
      "step": 13420
    },
    {
      "epoch": 2.37413884472708,
      "grad_norm": 6.233641147613525,
      "learning_rate": 2.626037802508391e-05,
      "loss": 6.1629,
      "step": 13440
    },
    {
      "epoch": 2.377671789436495,
      "grad_norm": 5.433926582336426,
      "learning_rate": 2.622504857798976e-05,
      "loss": 6.062,
      "step": 13460
    },
    {
      "epoch": 2.3812047341459106,
      "grad_norm": 5.920274257659912,
      "learning_rate": 2.6189719130895603e-05,
      "loss": 6.2278,
      "step": 13480
    },
    {
      "epoch": 2.384737678855326,
      "grad_norm": 5.483188629150391,
      "learning_rate": 2.615438968380145e-05,
      "loss": 6.1486,
      "step": 13500
    },
    {
      "epoch": 2.388270623564741,
      "grad_norm": 5.924935817718506,
      "learning_rate": 2.61190602367073e-05,
      "loss": 6.2088,
      "step": 13520
    },
    {
      "epoch": 2.3918035682741565,
      "grad_norm": 4.884492874145508,
      "learning_rate": 2.6083730789613144e-05,
      "loss": 6.1132,
      "step": 13540
    },
    {
      "epoch": 2.395336512983572,
      "grad_norm": 5.239272117614746,
      "learning_rate": 2.604840134251899e-05,
      "loss": 6.1762,
      "step": 13560
    },
    {
      "epoch": 2.398869457692987,
      "grad_norm": 3.9309425354003906,
      "learning_rate": 2.601307189542484e-05,
      "loss": 6.2729,
      "step": 13580
    },
    {
      "epoch": 2.4024024024024024,
      "grad_norm": 5.996307373046875,
      "learning_rate": 2.5977742448330682e-05,
      "loss": 6.1623,
      "step": 13600
    },
    {
      "epoch": 2.405935347111818,
      "grad_norm": 6.818655967712402,
      "learning_rate": 2.5942413001236532e-05,
      "loss": 6.1251,
      "step": 13620
    },
    {
      "epoch": 2.409468291821233,
      "grad_norm": 5.3649516105651855,
      "learning_rate": 2.590708355414238e-05,
      "loss": 6.1277,
      "step": 13640
    },
    {
      "epoch": 2.4130012365306484,
      "grad_norm": 4.343100070953369,
      "learning_rate": 2.5871754107048223e-05,
      "loss": 6.2301,
      "step": 13660
    },
    {
      "epoch": 2.4165341812400634,
      "grad_norm": 6.2974467277526855,
      "learning_rate": 2.5836424659954074e-05,
      "loss": 6.0999,
      "step": 13680
    },
    {
      "epoch": 2.420067125949479,
      "grad_norm": 8.358311653137207,
      "learning_rate": 2.580109521285992e-05,
      "loss": 6.1329,
      "step": 13700
    },
    {
      "epoch": 2.4236000706588943,
      "grad_norm": 5.737406253814697,
      "learning_rate": 2.5765765765765764e-05,
      "loss": 6.1173,
      "step": 13720
    },
    {
      "epoch": 2.4271330153683097,
      "grad_norm": 7.010124683380127,
      "learning_rate": 2.5730436318671615e-05,
      "loss": 6.1377,
      "step": 13740
    },
    {
      "epoch": 2.4306659600777247,
      "grad_norm": 6.823358058929443,
      "learning_rate": 2.5695106871577462e-05,
      "loss": 6.0114,
      "step": 13760
    },
    {
      "epoch": 2.43419890478714,
      "grad_norm": 3.682305335998535,
      "learning_rate": 2.5659777424483306e-05,
      "loss": 6.2791,
      "step": 13780
    },
    {
      "epoch": 2.437731849496555,
      "grad_norm": 6.030653476715088,
      "learning_rate": 2.5624447977389153e-05,
      "loss": 6.1814,
      "step": 13800
    },
    {
      "epoch": 2.4412647942059706,
      "grad_norm": 7.157156944274902,
      "learning_rate": 2.5589118530295003e-05,
      "loss": 6.1471,
      "step": 13820
    },
    {
      "epoch": 2.444797738915386,
      "grad_norm": 5.3048787117004395,
      "learning_rate": 2.5553789083200847e-05,
      "loss": 6.1592,
      "step": 13840
    },
    {
      "epoch": 2.448330683624801,
      "grad_norm": 5.533438205718994,
      "learning_rate": 2.5518459636106694e-05,
      "loss": 6.1407,
      "step": 13860
    },
    {
      "epoch": 2.4518636283342166,
      "grad_norm": 5.826267719268799,
      "learning_rate": 2.5483130189012545e-05,
      "loss": 6.1009,
      "step": 13880
    },
    {
      "epoch": 2.455396573043632,
      "grad_norm": 4.8638105392456055,
      "learning_rate": 2.5447800741918392e-05,
      "loss": 6.1634,
      "step": 13900
    },
    {
      "epoch": 2.458929517753047,
      "grad_norm": 4.744060039520264,
      "learning_rate": 2.5412471294824236e-05,
      "loss": 6.1288,
      "step": 13920
    },
    {
      "epoch": 2.4624624624624625,
      "grad_norm": 8.163169860839844,
      "learning_rate": 2.5377141847730086e-05,
      "loss": 6.1209,
      "step": 13940
    },
    {
      "epoch": 2.465995407171878,
      "grad_norm": 5.491811752319336,
      "learning_rate": 2.5341812400635933e-05,
      "loss": 6.046,
      "step": 13960
    },
    {
      "epoch": 2.469528351881293,
      "grad_norm": 3.9538936614990234,
      "learning_rate": 2.5306482953541777e-05,
      "loss": 6.0686,
      "step": 13980
    },
    {
      "epoch": 2.4730612965907084,
      "grad_norm": 5.631532192230225,
      "learning_rate": 2.5271153506447627e-05,
      "loss": 6.2842,
      "step": 14000
    },
    {
      "epoch": 2.476594241300124,
      "grad_norm": 6.722609519958496,
      "learning_rate": 2.5235824059353474e-05,
      "loss": 6.1439,
      "step": 14020
    },
    {
      "epoch": 2.480127186009539,
      "grad_norm": 4.920430660247803,
      "learning_rate": 2.5200494612259318e-05,
      "loss": 6.0901,
      "step": 14040
    },
    {
      "epoch": 2.4836601307189543,
      "grad_norm": 4.504127502441406,
      "learning_rate": 2.5165165165165165e-05,
      "loss": 6.1153,
      "step": 14060
    },
    {
      "epoch": 2.4871930754283698,
      "grad_norm": 6.955075263977051,
      "learning_rate": 2.5129835718071016e-05,
      "loss": 6.1404,
      "step": 14080
    },
    {
      "epoch": 2.4907260201377848,
      "grad_norm": 5.396081447601318,
      "learning_rate": 2.509450627097686e-05,
      "loss": 6.223,
      "step": 14100
    },
    {
      "epoch": 2.4942589648472,
      "grad_norm": 5.7868499755859375,
      "learning_rate": 2.5059176823882707e-05,
      "loss": 6.1837,
      "step": 14120
    },
    {
      "epoch": 2.4977919095566152,
      "grad_norm": 5.931972980499268,
      "learning_rate": 2.5023847376788557e-05,
      "loss": 6.0824,
      "step": 14140
    },
    {
      "epoch": 2.5013248542660307,
      "grad_norm": 3.988041877746582,
      "learning_rate": 2.49885179296944e-05,
      "loss": 6.1002,
      "step": 14160
    },
    {
      "epoch": 2.504857798975446,
      "grad_norm": 4.974977493286133,
      "learning_rate": 2.4953188482600248e-05,
      "loss": 6.1179,
      "step": 14180
    },
    {
      "epoch": 2.5083907436848616,
      "grad_norm": 5.097127437591553,
      "learning_rate": 2.4917859035506095e-05,
      "loss": 6.2435,
      "step": 14200
    },
    {
      "epoch": 2.5119236883942766,
      "grad_norm": 4.39295768737793,
      "learning_rate": 2.4882529588411942e-05,
      "loss": 6.1191,
      "step": 14220
    },
    {
      "epoch": 2.515456633103692,
      "grad_norm": 5.49833345413208,
      "learning_rate": 2.484720014131779e-05,
      "loss": 6.1321,
      "step": 14240
    },
    {
      "epoch": 2.518989577813107,
      "grad_norm": 4.729291915893555,
      "learning_rate": 2.4811870694223636e-05,
      "loss": 6.0664,
      "step": 14260
    },
    {
      "epoch": 2.5225225225225225,
      "grad_norm": 4.5757012367248535,
      "learning_rate": 2.4776541247129483e-05,
      "loss": 6.1525,
      "step": 14280
    },
    {
      "epoch": 2.526055467231938,
      "grad_norm": 5.917512893676758,
      "learning_rate": 2.474121180003533e-05,
      "loss": 6.3345,
      "step": 14300
    },
    {
      "epoch": 2.529588411941353,
      "grad_norm": 5.1633992195129395,
      "learning_rate": 2.4705882352941178e-05,
      "loss": 6.3114,
      "step": 14320
    },
    {
      "epoch": 2.5331213566507684,
      "grad_norm": 4.844723701477051,
      "learning_rate": 2.4670552905847025e-05,
      "loss": 6.1456,
      "step": 14340
    },
    {
      "epoch": 2.5366543013601834,
      "grad_norm": 4.4274373054504395,
      "learning_rate": 2.4635223458752872e-05,
      "loss": 6.0649,
      "step": 14360
    },
    {
      "epoch": 2.540187246069599,
      "grad_norm": 7.264862060546875,
      "learning_rate": 2.459989401165872e-05,
      "loss": 6.2234,
      "step": 14380
    },
    {
      "epoch": 2.5437201907790143,
      "grad_norm": 10.593957901000977,
      "learning_rate": 2.4564564564564566e-05,
      "loss": 5.9935,
      "step": 14400
    },
    {
      "epoch": 2.54725313548843,
      "grad_norm": 6.2905120849609375,
      "learning_rate": 2.4529235117470413e-05,
      "loss": 6.1579,
      "step": 14420
    },
    {
      "epoch": 2.550786080197845,
      "grad_norm": 5.1602559089660645,
      "learning_rate": 2.449390567037626e-05,
      "loss": 6.1213,
      "step": 14440
    },
    {
      "epoch": 2.5543190249072603,
      "grad_norm": 6.190672874450684,
      "learning_rate": 2.4458576223282107e-05,
      "loss": 6.1612,
      "step": 14460
    },
    {
      "epoch": 2.5578519696166753,
      "grad_norm": 7.1961669921875,
      "learning_rate": 2.4423246776187955e-05,
      "loss": 6.0943,
      "step": 14480
    },
    {
      "epoch": 2.5613849143260907,
      "grad_norm": 5.161622524261475,
      "learning_rate": 2.43879173290938e-05,
      "loss": 6.0603,
      "step": 14500
    },
    {
      "epoch": 2.564917859035506,
      "grad_norm": 4.088913440704346,
      "learning_rate": 2.435258788199965e-05,
      "loss": 6.0691,
      "step": 14520
    },
    {
      "epoch": 2.5684508037449216,
      "grad_norm": 5.302951812744141,
      "learning_rate": 2.4317258434905496e-05,
      "loss": 6.0256,
      "step": 14540
    },
    {
      "epoch": 2.5719837484543366,
      "grad_norm": 5.227699279785156,
      "learning_rate": 2.4281928987811343e-05,
      "loss": 6.0913,
      "step": 14560
    },
    {
      "epoch": 2.575516693163752,
      "grad_norm": 6.201503753662109,
      "learning_rate": 2.4246599540717187e-05,
      "loss": 6.257,
      "step": 14580
    },
    {
      "epoch": 2.579049637873167,
      "grad_norm": 5.373772621154785,
      "learning_rate": 2.4211270093623037e-05,
      "loss": 6.1399,
      "step": 14600
    },
    {
      "epoch": 2.5825825825825826,
      "grad_norm": 6.580806732177734,
      "learning_rate": 2.4175940646528884e-05,
      "loss": 6.2116,
      "step": 14620
    },
    {
      "epoch": 2.586115527291998,
      "grad_norm": 5.180931568145752,
      "learning_rate": 2.4140611199434728e-05,
      "loss": 6.0612,
      "step": 14640
    },
    {
      "epoch": 2.589648472001413,
      "grad_norm": 5.411649227142334,
      "learning_rate": 2.410528175234058e-05,
      "loss": 6.1792,
      "step": 14660
    },
    {
      "epoch": 2.5931814167108285,
      "grad_norm": 4.257610321044922,
      "learning_rate": 2.4069952305246422e-05,
      "loss": 6.1053,
      "step": 14680
    },
    {
      "epoch": 2.596714361420244,
      "grad_norm": 5.478486061096191,
      "learning_rate": 2.403462285815227e-05,
      "loss": 6.0128,
      "step": 14700
    },
    {
      "epoch": 2.600247306129659,
      "grad_norm": 5.520956039428711,
      "learning_rate": 2.399929341105812e-05,
      "loss": 6.1604,
      "step": 14720
    },
    {
      "epoch": 2.6037802508390744,
      "grad_norm": 5.8276543617248535,
      "learning_rate": 2.3963963963963964e-05,
      "loss": 6.1367,
      "step": 14740
    },
    {
      "epoch": 2.60731319554849,
      "grad_norm": 4.670294761657715,
      "learning_rate": 2.3928634516869814e-05,
      "loss": 6.1055,
      "step": 14760
    },
    {
      "epoch": 2.610846140257905,
      "grad_norm": 4.907327651977539,
      "learning_rate": 2.3893305069775658e-05,
      "loss": 6.2965,
      "step": 14780
    },
    {
      "epoch": 2.6143790849673203,
      "grad_norm": 5.228297233581543,
      "learning_rate": 2.3857975622681505e-05,
      "loss": 6.0728,
      "step": 14800
    },
    {
      "epoch": 2.6179120296767353,
      "grad_norm": 4.596251487731934,
      "learning_rate": 2.3822646175587355e-05,
      "loss": 6.0588,
      "step": 14820
    },
    {
      "epoch": 2.6214449743861508,
      "grad_norm": 4.363655090332031,
      "learning_rate": 2.37873167284932e-05,
      "loss": 6.0733,
      "step": 14840
    },
    {
      "epoch": 2.624977919095566,
      "grad_norm": 5.951022148132324,
      "learning_rate": 2.3751987281399046e-05,
      "loss": 6.2178,
      "step": 14860
    },
    {
      "epoch": 2.6285108638049817,
      "grad_norm": 5.4007887840271,
      "learning_rate": 2.3716657834304897e-05,
      "loss": 6.2064,
      "step": 14880
    },
    {
      "epoch": 2.6320438085143967,
      "grad_norm": 5.824831008911133,
      "learning_rate": 2.368132838721074e-05,
      "loss": 6.189,
      "step": 14900
    },
    {
      "epoch": 2.635576753223812,
      "grad_norm": 4.107145309448242,
      "learning_rate": 2.3645998940116588e-05,
      "loss": 6.1091,
      "step": 14920
    },
    {
      "epoch": 2.639109697933227,
      "grad_norm": 7.477430820465088,
      "learning_rate": 2.3610669493022435e-05,
      "loss": 6.1123,
      "step": 14940
    },
    {
      "epoch": 2.6426426426426426,
      "grad_norm": 6.607240676879883,
      "learning_rate": 2.3575340045928282e-05,
      "loss": 6.0854,
      "step": 14960
    },
    {
      "epoch": 2.646175587352058,
      "grad_norm": 4.391608715057373,
      "learning_rate": 2.3540010598834132e-05,
      "loss": 6.0264,
      "step": 14980
    },
    {
      "epoch": 2.6497085320614735,
      "grad_norm": 6.6849751472473145,
      "learning_rate": 2.3504681151739976e-05,
      "loss": 6.1317,
      "step": 15000
    },
    {
      "epoch": 2.6532414767708885,
      "grad_norm": 6.333319187164307,
      "learning_rate": 2.3469351704645823e-05,
      "loss": 6.1526,
      "step": 15020
    },
    {
      "epoch": 2.656774421480304,
      "grad_norm": 6.031174659729004,
      "learning_rate": 2.343402225755167e-05,
      "loss": 6.1062,
      "step": 15040
    },
    {
      "epoch": 2.660307366189719,
      "grad_norm": 5.27201509475708,
      "learning_rate": 2.3398692810457517e-05,
      "loss": 6.0854,
      "step": 15060
    },
    {
      "epoch": 2.6638403108991344,
      "grad_norm": 6.1456122398376465,
      "learning_rate": 2.3363363363363364e-05,
      "loss": 6.1697,
      "step": 15080
    },
    {
      "epoch": 2.66737325560855,
      "grad_norm": 5.551096439361572,
      "learning_rate": 2.332803391626921e-05,
      "loss": 6.3085,
      "step": 15100
    },
    {
      "epoch": 2.670906200317965,
      "grad_norm": 6.639586925506592,
      "learning_rate": 2.329270446917506e-05,
      "loss": 6.0392,
      "step": 15120
    },
    {
      "epoch": 2.6744391450273803,
      "grad_norm": 5.4066996574401855,
      "learning_rate": 2.3257375022080906e-05,
      "loss": 6.1972,
      "step": 15140
    },
    {
      "epoch": 2.6779720897367953,
      "grad_norm": 4.5824785232543945,
      "learning_rate": 2.3222045574986753e-05,
      "loss": 6.0892,
      "step": 15160
    },
    {
      "epoch": 2.681505034446211,
      "grad_norm": 4.932438850402832,
      "learning_rate": 2.31867161278926e-05,
      "loss": 6.243,
      "step": 15180
    },
    {
      "epoch": 2.6850379791556263,
      "grad_norm": 7.491782188415527,
      "learning_rate": 2.3151386680798444e-05,
      "loss": 6.0311,
      "step": 15200
    },
    {
      "epoch": 2.6885709238650417,
      "grad_norm": 4.958659648895264,
      "learning_rate": 2.3116057233704294e-05,
      "loss": 6.1302,
      "step": 15220
    },
    {
      "epoch": 2.6921038685744567,
      "grad_norm": 4.3047051429748535,
      "learning_rate": 2.308072778661014e-05,
      "loss": 6.1691,
      "step": 15240
    },
    {
      "epoch": 2.695636813283872,
      "grad_norm": 5.789051532745361,
      "learning_rate": 2.304539833951599e-05,
      "loss": 6.2068,
      "step": 15260
    },
    {
      "epoch": 2.699169757993287,
      "grad_norm": 6.225069999694824,
      "learning_rate": 2.3010068892421835e-05,
      "loss": 6.1793,
      "step": 15280
    },
    {
      "epoch": 2.7027027027027026,
      "grad_norm": 5.300074577331543,
      "learning_rate": 2.2974739445327683e-05,
      "loss": 6.1862,
      "step": 15300
    },
    {
      "epoch": 2.706235647412118,
      "grad_norm": 3.908994436264038,
      "learning_rate": 2.293940999823353e-05,
      "loss": 6.2274,
      "step": 15320
    },
    {
      "epoch": 2.7097685921215335,
      "grad_norm": 7.167547702789307,
      "learning_rate": 2.2904080551139377e-05,
      "loss": 6.1052,
      "step": 15340
    },
    {
      "epoch": 2.7133015368309485,
      "grad_norm": 7.261615753173828,
      "learning_rate": 2.286875110404522e-05,
      "loss": 6.1007,
      "step": 15360
    },
    {
      "epoch": 2.716834481540364,
      "grad_norm": 4.940035343170166,
      "learning_rate": 2.283342165695107e-05,
      "loss": 6.1664,
      "step": 15380
    },
    {
      "epoch": 2.720367426249779,
      "grad_norm": 9.76021957397461,
      "learning_rate": 2.2798092209856918e-05,
      "loss": 6.1381,
      "step": 15400
    },
    {
      "epoch": 2.7239003709591945,
      "grad_norm": 6.35681676864624,
      "learning_rate": 2.2762762762762762e-05,
      "loss": 6.1045,
      "step": 15420
    },
    {
      "epoch": 2.72743331566861,
      "grad_norm": 7.188170433044434,
      "learning_rate": 2.2727433315668612e-05,
      "loss": 6.1282,
      "step": 15440
    },
    {
      "epoch": 2.730966260378025,
      "grad_norm": 6.067205905914307,
      "learning_rate": 2.2692103868574456e-05,
      "loss": 6.1187,
      "step": 15460
    },
    {
      "epoch": 2.7344992050874404,
      "grad_norm": 4.707911014556885,
      "learning_rate": 2.2656774421480307e-05,
      "loss": 6.0747,
      "step": 15480
    },
    {
      "epoch": 2.738032149796856,
      "grad_norm": 4.35614538192749,
      "learning_rate": 2.2621444974386154e-05,
      "loss": 6.1208,
      "step": 15500
    },
    {
      "epoch": 2.741565094506271,
      "grad_norm": 5.931572437286377,
      "learning_rate": 2.2586115527291997e-05,
      "loss": 6.1283,
      "step": 15520
    },
    {
      "epoch": 2.7450980392156863,
      "grad_norm": 6.55891752243042,
      "learning_rate": 2.2550786080197848e-05,
      "loss": 6.1485,
      "step": 15540
    },
    {
      "epoch": 2.7486309839251017,
      "grad_norm": 6.419869422912598,
      "learning_rate": 2.251545663310369e-05,
      "loss": 6.2145,
      "step": 15560
    },
    {
      "epoch": 2.7521639286345168,
      "grad_norm": 5.5562896728515625,
      "learning_rate": 2.248012718600954e-05,
      "loss": 6.1345,
      "step": 15580
    },
    {
      "epoch": 2.755696873343932,
      "grad_norm": 5.101718425750732,
      "learning_rate": 2.244479773891539e-05,
      "loss": 6.0178,
      "step": 15600
    },
    {
      "epoch": 2.759229818053347,
      "grad_norm": 7.471424102783203,
      "learning_rate": 2.2409468291821233e-05,
      "loss": 6.0522,
      "step": 15620
    },
    {
      "epoch": 2.7627627627627627,
      "grad_norm": 6.58192777633667,
      "learning_rate": 2.237413884472708e-05,
      "loss": 6.1664,
      "step": 15640
    },
    {
      "epoch": 2.766295707472178,
      "grad_norm": 5.2260661125183105,
      "learning_rate": 2.2338809397632927e-05,
      "loss": 6.2796,
      "step": 15660
    },
    {
      "epoch": 2.7698286521815936,
      "grad_norm": 5.011332035064697,
      "learning_rate": 2.2303479950538774e-05,
      "loss": 6.1408,
      "step": 15680
    },
    {
      "epoch": 2.7733615968910086,
      "grad_norm": 4.7545881271362305,
      "learning_rate": 2.226815050344462e-05,
      "loss": 6.2271,
      "step": 15700
    },
    {
      "epoch": 2.776894541600424,
      "grad_norm": 6.0674848556518555,
      "learning_rate": 2.223282105635047e-05,
      "loss": 6.046,
      "step": 15720
    },
    {
      "epoch": 2.780427486309839,
      "grad_norm": 6.225482940673828,
      "learning_rate": 2.2197491609256316e-05,
      "loss": 6.1247,
      "step": 15740
    },
    {
      "epoch": 2.7839604310192545,
      "grad_norm": 4.572603225708008,
      "learning_rate": 2.2162162162162166e-05,
      "loss": 6.0191,
      "step": 15760
    },
    {
      "epoch": 2.78749337572867,
      "grad_norm": 5.557419300079346,
      "learning_rate": 2.212683271506801e-05,
      "loss": 6.1461,
      "step": 15780
    },
    {
      "epoch": 2.7910263204380854,
      "grad_norm": 4.574228763580322,
      "learning_rate": 2.2091503267973857e-05,
      "loss": 6.2345,
      "step": 15800
    },
    {
      "epoch": 2.7945592651475004,
      "grad_norm": 12.4007568359375,
      "learning_rate": 2.2056173820879704e-05,
      "loss": 6.0827,
      "step": 15820
    },
    {
      "epoch": 2.798092209856916,
      "grad_norm": 7.3189826011657715,
      "learning_rate": 2.202084437378555e-05,
      "loss": 6.1832,
      "step": 15840
    },
    {
      "epoch": 2.801625154566331,
      "grad_norm": 4.525838375091553,
      "learning_rate": 2.1985514926691398e-05,
      "loss": 6.1946,
      "step": 15860
    },
    {
      "epoch": 2.8051580992757463,
      "grad_norm": 4.5169997215271,
      "learning_rate": 2.1950185479597245e-05,
      "loss": 6.0881,
      "step": 15880
    },
    {
      "epoch": 2.808691043985162,
      "grad_norm": 7.028967380523682,
      "learning_rate": 2.1914856032503092e-05,
      "loss": 6.1517,
      "step": 15900
    },
    {
      "epoch": 2.812223988694577,
      "grad_norm": 4.9041876792907715,
      "learning_rate": 2.187952658540894e-05,
      "loss": 6.0482,
      "step": 15920
    },
    {
      "epoch": 2.8157569334039922,
      "grad_norm": 5.162725925445557,
      "learning_rate": 2.1844197138314787e-05,
      "loss": 5.9861,
      "step": 15940
    },
    {
      "epoch": 2.8192898781134073,
      "grad_norm": 5.694614410400391,
      "learning_rate": 2.1808867691220634e-05,
      "loss": 6.1649,
      "step": 15960
    },
    {
      "epoch": 2.8228228228228227,
      "grad_norm": 4.932132720947266,
      "learning_rate": 2.177353824412648e-05,
      "loss": 6.3059,
      "step": 15980
    },
    {
      "epoch": 2.826355767532238,
      "grad_norm": 5.120681285858154,
      "learning_rate": 2.1738208797032328e-05,
      "loss": 5.9269,
      "step": 16000
    },
    {
      "epoch": 2.8298887122416536,
      "grad_norm": 4.667840480804443,
      "learning_rate": 2.1702879349938175e-05,
      "loss": 6.1063,
      "step": 16020
    },
    {
      "epoch": 2.8334216569510686,
      "grad_norm": 5.392625331878662,
      "learning_rate": 2.1667549902844022e-05,
      "loss": 5.9962,
      "step": 16040
    },
    {
      "epoch": 2.836954601660484,
      "grad_norm": 5.127820014953613,
      "learning_rate": 2.163222045574987e-05,
      "loss": 6.1559,
      "step": 16060
    },
    {
      "epoch": 2.840487546369899,
      "grad_norm": 4.680235862731934,
      "learning_rate": 2.1596891008655713e-05,
      "loss": 6.0959,
      "step": 16080
    },
    {
      "epoch": 2.8440204910793145,
      "grad_norm": 5.2880072593688965,
      "learning_rate": 2.1561561561561564e-05,
      "loss": 6.0197,
      "step": 16100
    },
    {
      "epoch": 2.84755343578873,
      "grad_norm": 5.666548728942871,
      "learning_rate": 2.152623211446741e-05,
      "loss": 6.0354,
      "step": 16120
    },
    {
      "epoch": 2.8510863804981454,
      "grad_norm": 3.7184932231903076,
      "learning_rate": 2.1490902667373254e-05,
      "loss": 6.1317,
      "step": 16140
    },
    {
      "epoch": 2.8546193252075605,
      "grad_norm": 5.979211330413818,
      "learning_rate": 2.1455573220279105e-05,
      "loss": 6.1474,
      "step": 16160
    },
    {
      "epoch": 2.858152269916976,
      "grad_norm": 4.752920150756836,
      "learning_rate": 2.1420243773184952e-05,
      "loss": 6.0509,
      "step": 16180
    },
    {
      "epoch": 2.861685214626391,
      "grad_norm": 6.83397102355957,
      "learning_rate": 2.1384914326090796e-05,
      "loss": 6.1797,
      "step": 16200
    },
    {
      "epoch": 2.8652181593358064,
      "grad_norm": 5.491633892059326,
      "learning_rate": 2.1349584878996646e-05,
      "loss": 6.0437,
      "step": 16220
    },
    {
      "epoch": 2.868751104045222,
      "grad_norm": 4.847708225250244,
      "learning_rate": 2.131425543190249e-05,
      "loss": 6.2122,
      "step": 16240
    },
    {
      "epoch": 2.872284048754637,
      "grad_norm": 4.061422348022461,
      "learning_rate": 2.127892598480834e-05,
      "loss": 6.1051,
      "step": 16260
    },
    {
      "epoch": 2.8758169934640523,
      "grad_norm": 4.1381449699401855,
      "learning_rate": 2.1243596537714188e-05,
      "loss": 6.0833,
      "step": 16280
    },
    {
      "epoch": 2.8793499381734677,
      "grad_norm": 4.366675853729248,
      "learning_rate": 2.120826709062003e-05,
      "loss": 6.1131,
      "step": 16300
    },
    {
      "epoch": 2.8828828828828827,
      "grad_norm": 5.532602787017822,
      "learning_rate": 2.1172937643525882e-05,
      "loss": 6.06,
      "step": 16320
    },
    {
      "epoch": 2.886415827592298,
      "grad_norm": 5.188580513000488,
      "learning_rate": 2.1137608196431725e-05,
      "loss": 6.1646,
      "step": 16340
    },
    {
      "epoch": 2.8899487723017137,
      "grad_norm": 5.377060890197754,
      "learning_rate": 2.1102278749337573e-05,
      "loss": 6.2834,
      "step": 16360
    },
    {
      "epoch": 2.8934817170111287,
      "grad_norm": 5.773397445678711,
      "learning_rate": 2.1066949302243423e-05,
      "loss": 6.1353,
      "step": 16380
    },
    {
      "epoch": 2.897014661720544,
      "grad_norm": 6.11830472946167,
      "learning_rate": 2.1031619855149267e-05,
      "loss": 6.1804,
      "step": 16400
    },
    {
      "epoch": 2.900547606429959,
      "grad_norm": 4.299738883972168,
      "learning_rate": 2.0996290408055114e-05,
      "loss": 6.201,
      "step": 16420
    },
    {
      "epoch": 2.9040805511393746,
      "grad_norm": 5.409706115722656,
      "learning_rate": 2.096096096096096e-05,
      "loss": 6.1436,
      "step": 16440
    },
    {
      "epoch": 2.90761349584879,
      "grad_norm": 5.387650489807129,
      "learning_rate": 2.0925631513866808e-05,
      "loss": 6.1513,
      "step": 16460
    },
    {
      "epoch": 2.9111464405582055,
      "grad_norm": 5.461880207061768,
      "learning_rate": 2.089030206677266e-05,
      "loss": 6.0466,
      "step": 16480
    },
    {
      "epoch": 2.9146793852676205,
      "grad_norm": 4.5595173835754395,
      "learning_rate": 2.0854972619678502e-05,
      "loss": 6.1368,
      "step": 16500
    },
    {
      "epoch": 2.918212329977036,
      "grad_norm": 4.622173309326172,
      "learning_rate": 2.081964317258435e-05,
      "loss": 6.0311,
      "step": 16520
    },
    {
      "epoch": 2.921745274686451,
      "grad_norm": 6.528127193450928,
      "learning_rate": 2.0784313725490197e-05,
      "loss": 6.117,
      "step": 16540
    },
    {
      "epoch": 2.9252782193958664,
      "grad_norm": 4.878301620483398,
      "learning_rate": 2.0748984278396044e-05,
      "loss": 5.9804,
      "step": 16560
    },
    {
      "epoch": 2.928811164105282,
      "grad_norm": 7.757384300231934,
      "learning_rate": 2.071365483130189e-05,
      "loss": 6.1601,
      "step": 16580
    },
    {
      "epoch": 2.9323441088146973,
      "grad_norm": 4.7269415855407715,
      "learning_rate": 2.0678325384207738e-05,
      "loss": 6.266,
      "step": 16600
    },
    {
      "epoch": 2.9358770535241123,
      "grad_norm": 3.9312963485717773,
      "learning_rate": 2.0642995937113585e-05,
      "loss": 6.0693,
      "step": 16620
    },
    {
      "epoch": 2.9394099982335278,
      "grad_norm": 6.511004447937012,
      "learning_rate": 2.0607666490019432e-05,
      "loss": 6.2748,
      "step": 16640
    },
    {
      "epoch": 2.942942942942943,
      "grad_norm": 5.48725700378418,
      "learning_rate": 2.057233704292528e-05,
      "loss": 6.0249,
      "step": 16660
    },
    {
      "epoch": 2.9464758876523582,
      "grad_norm": 5.386373996734619,
      "learning_rate": 2.0537007595831126e-05,
      "loss": 6.0826,
      "step": 16680
    },
    {
      "epoch": 2.9500088323617737,
      "grad_norm": 4.378690242767334,
      "learning_rate": 2.0501678148736973e-05,
      "loss": 6.0925,
      "step": 16700
    },
    {
      "epoch": 2.9535417770711887,
      "grad_norm": 5.626668930053711,
      "learning_rate": 2.046634870164282e-05,
      "loss": 6.1502,
      "step": 16720
    },
    {
      "epoch": 2.957074721780604,
      "grad_norm": 4.927701473236084,
      "learning_rate": 2.0431019254548668e-05,
      "loss": 6.0279,
      "step": 16740
    },
    {
      "epoch": 2.960607666490019,
      "grad_norm": 5.322242736816406,
      "learning_rate": 2.0395689807454515e-05,
      "loss": 6.1432,
      "step": 16760
    },
    {
      "epoch": 2.9641406111994346,
      "grad_norm": 5.326430320739746,
      "learning_rate": 2.0360360360360362e-05,
      "loss": 6.0155,
      "step": 16780
    },
    {
      "epoch": 2.96767355590885,
      "grad_norm": 4.3326520919799805,
      "learning_rate": 2.032503091326621e-05,
      "loss": 6.1584,
      "step": 16800
    },
    {
      "epoch": 2.9712065006182655,
      "grad_norm": 8.922612190246582,
      "learning_rate": 2.0289701466172056e-05,
      "loss": 6.0998,
      "step": 16820
    },
    {
      "epoch": 2.9747394453276805,
      "grad_norm": 4.381181240081787,
      "learning_rate": 2.0254372019077903e-05,
      "loss": 5.9966,
      "step": 16840
    },
    {
      "epoch": 2.978272390037096,
      "grad_norm": 6.009340763092041,
      "learning_rate": 2.0219042571983747e-05,
      "loss": 6.2422,
      "step": 16860
    },
    {
      "epoch": 2.981805334746511,
      "grad_norm": 4.679842948913574,
      "learning_rate": 2.0183713124889597e-05,
      "loss": 6.0384,
      "step": 16880
    },
    {
      "epoch": 2.9853382794559264,
      "grad_norm": 5.230784893035889,
      "learning_rate": 2.0148383677795444e-05,
      "loss": 6.2148,
      "step": 16900
    },
    {
      "epoch": 2.988871224165342,
      "grad_norm": 6.21830415725708,
      "learning_rate": 2.0113054230701288e-05,
      "loss": 6.2012,
      "step": 16920
    },
    {
      "epoch": 2.9924041688747574,
      "grad_norm": 4.963735103607178,
      "learning_rate": 2.007772478360714e-05,
      "loss": 5.9918,
      "step": 16940
    },
    {
      "epoch": 2.9959371135841724,
      "grad_norm": 5.704095363616943,
      "learning_rate": 2.0042395336512982e-05,
      "loss": 6.2067,
      "step": 16960
    },
    {
      "epoch": 2.999470058293588,
      "grad_norm": 5.5307698249816895,
      "learning_rate": 2.0007065889418833e-05,
      "loss": 6.15,
      "step": 16980
    },
    {
      "epoch": 3.003003003003003,
      "grad_norm": 5.201543807983398,
      "learning_rate": 1.997173644232468e-05,
      "loss": 5.9976,
      "step": 17000
    },
    {
      "epoch": 3.0065359477124183,
      "grad_norm": 5.404130458831787,
      "learning_rate": 1.9936406995230524e-05,
      "loss": 6.0963,
      "step": 17020
    },
    {
      "epoch": 3.0100688924218337,
      "grad_norm": 5.389249801635742,
      "learning_rate": 1.9901077548136374e-05,
      "loss": 6.2908,
      "step": 17040
    },
    {
      "epoch": 3.0136018371312487,
      "grad_norm": 5.447967052459717,
      "learning_rate": 1.986574810104222e-05,
      "loss": 6.0989,
      "step": 17060
    },
    {
      "epoch": 3.017134781840664,
      "grad_norm": 4.4334187507629395,
      "learning_rate": 1.9830418653948065e-05,
      "loss": 6.0307,
      "step": 17080
    },
    {
      "epoch": 3.0206677265500796,
      "grad_norm": 5.917455673217773,
      "learning_rate": 1.9795089206853916e-05,
      "loss": 6.0627,
      "step": 17100
    },
    {
      "epoch": 3.0242006712594947,
      "grad_norm": 4.866710662841797,
      "learning_rate": 1.975975975975976e-05,
      "loss": 6.0589,
      "step": 17120
    },
    {
      "epoch": 3.02773361596891,
      "grad_norm": 5.117027282714844,
      "learning_rate": 1.9724430312665606e-05,
      "loss": 6.2275,
      "step": 17140
    },
    {
      "epoch": 3.0312665606783256,
      "grad_norm": 4.447414875030518,
      "learning_rate": 1.9689100865571457e-05,
      "loss": 6.0476,
      "step": 17160
    },
    {
      "epoch": 3.0347995053877406,
      "grad_norm": 4.383427143096924,
      "learning_rate": 1.96537714184773e-05,
      "loss": 6.0186,
      "step": 17180
    },
    {
      "epoch": 3.038332450097156,
      "grad_norm": 6.470145225524902,
      "learning_rate": 1.9618441971383148e-05,
      "loss": 6.0495,
      "step": 17200
    },
    {
      "epoch": 3.0418653948065715,
      "grad_norm": 5.666048049926758,
      "learning_rate": 1.9583112524288995e-05,
      "loss": 6.0708,
      "step": 17220
    },
    {
      "epoch": 3.0453983395159865,
      "grad_norm": 8.088298797607422,
      "learning_rate": 1.9547783077194842e-05,
      "loss": 6.0874,
      "step": 17240
    },
    {
      "epoch": 3.048931284225402,
      "grad_norm": 6.58474063873291,
      "learning_rate": 1.9512453630100692e-05,
      "loss": 6.0525,
      "step": 17260
    },
    {
      "epoch": 3.0524642289348174,
      "grad_norm": 5.458385944366455,
      "learning_rate": 1.9477124183006536e-05,
      "loss": 6.0683,
      "step": 17280
    },
    {
      "epoch": 3.0559971736442324,
      "grad_norm": 4.419260025024414,
      "learning_rate": 1.9441794735912383e-05,
      "loss": 6.1192,
      "step": 17300
    },
    {
      "epoch": 3.059530118353648,
      "grad_norm": 6.299439430236816,
      "learning_rate": 1.940646528881823e-05,
      "loss": 6.0735,
      "step": 17320
    },
    {
      "epoch": 3.063063063063063,
      "grad_norm": 4.568260192871094,
      "learning_rate": 1.9371135841724077e-05,
      "loss": 6.1551,
      "step": 17340
    },
    {
      "epoch": 3.0665960077724783,
      "grad_norm": 5.048155784606934,
      "learning_rate": 1.9335806394629925e-05,
      "loss": 5.9587,
      "step": 17360
    },
    {
      "epoch": 3.0701289524818938,
      "grad_norm": 5.088223457336426,
      "learning_rate": 1.930047694753577e-05,
      "loss": 6.0268,
      "step": 17380
    },
    {
      "epoch": 3.073661897191309,
      "grad_norm": 5.404823303222656,
      "learning_rate": 1.926514750044162e-05,
      "loss": 6.0715,
      "step": 17400
    },
    {
      "epoch": 3.0771948419007242,
      "grad_norm": 6.494258880615234,
      "learning_rate": 1.9229818053347466e-05,
      "loss": 5.9998,
      "step": 17420
    },
    {
      "epoch": 3.0807277866101397,
      "grad_norm": 6.194351673126221,
      "learning_rate": 1.9194488606253313e-05,
      "loss": 6.0245,
      "step": 17440
    },
    {
      "epoch": 3.0842607313195547,
      "grad_norm": 5.2278056144714355,
      "learning_rate": 1.915915915915916e-05,
      "loss": 6.0873,
      "step": 17460
    },
    {
      "epoch": 3.08779367602897,
      "grad_norm": 5.875067234039307,
      "learning_rate": 1.9123829712065007e-05,
      "loss": 5.9971,
      "step": 17480
    },
    {
      "epoch": 3.0913266207383856,
      "grad_norm": 6.29652738571167,
      "learning_rate": 1.9088500264970854e-05,
      "loss": 6.1348,
      "step": 17500
    },
    {
      "epoch": 3.0948595654478006,
      "grad_norm": 6.163087844848633,
      "learning_rate": 1.90531708178767e-05,
      "loss": 6.098,
      "step": 17520
    },
    {
      "epoch": 3.098392510157216,
      "grad_norm": 5.198293685913086,
      "learning_rate": 1.901784137078255e-05,
      "loss": 6.0168,
      "step": 17540
    },
    {
      "epoch": 3.1019254548666315,
      "grad_norm": 4.579934120178223,
      "learning_rate": 1.8982511923688396e-05,
      "loss": 6.2817,
      "step": 17560
    },
    {
      "epoch": 3.1054583995760465,
      "grad_norm": 7.034184455871582,
      "learning_rate": 1.8947182476594243e-05,
      "loss": 5.987,
      "step": 17580
    },
    {
      "epoch": 3.108991344285462,
      "grad_norm": 5.7693023681640625,
      "learning_rate": 1.891185302950009e-05,
      "loss": 6.1353,
      "step": 17600
    },
    {
      "epoch": 3.1125242889948774,
      "grad_norm": 7.053133010864258,
      "learning_rate": 1.8876523582405937e-05,
      "loss": 6.1362,
      "step": 17620
    },
    {
      "epoch": 3.1160572337042924,
      "grad_norm": 5.451486587524414,
      "learning_rate": 1.884119413531178e-05,
      "loss": 6.1062,
      "step": 17640
    },
    {
      "epoch": 3.119590178413708,
      "grad_norm": 6.39077091217041,
      "learning_rate": 1.880586468821763e-05,
      "loss": 6.0358,
      "step": 17660
    },
    {
      "epoch": 3.123123123123123,
      "grad_norm": 5.35858154296875,
      "learning_rate": 1.8770535241123478e-05,
      "loss": 6.1468,
      "step": 17680
    },
    {
      "epoch": 3.1266560678325384,
      "grad_norm": 6.168949127197266,
      "learning_rate": 1.8735205794029322e-05,
      "loss": 6.0189,
      "step": 17700
    },
    {
      "epoch": 3.130189012541954,
      "grad_norm": 9.054725646972656,
      "learning_rate": 1.8699876346935172e-05,
      "loss": 6.0137,
      "step": 17720
    },
    {
      "epoch": 3.133721957251369,
      "grad_norm": 5.14966344833374,
      "learning_rate": 1.8664546899841016e-05,
      "loss": 6.131,
      "step": 17740
    },
    {
      "epoch": 3.1372549019607843,
      "grad_norm": 5.467013359069824,
      "learning_rate": 1.8629217452746867e-05,
      "loss": 6.1514,
      "step": 17760
    },
    {
      "epoch": 3.1407878466701997,
      "grad_norm": 5.499118804931641,
      "learning_rate": 1.8593888005652714e-05,
      "loss": 6.088,
      "step": 17780
    },
    {
      "epoch": 3.1443207913796147,
      "grad_norm": 5.955964088439941,
      "learning_rate": 1.8558558558558558e-05,
      "loss": 6.0422,
      "step": 17800
    },
    {
      "epoch": 3.14785373608903,
      "grad_norm": 5.6904683113098145,
      "learning_rate": 1.8523229111464408e-05,
      "loss": 5.9781,
      "step": 17820
    },
    {
      "epoch": 3.1513866807984456,
      "grad_norm": 4.477741718292236,
      "learning_rate": 1.8487899664370252e-05,
      "loss": 5.953,
      "step": 17840
    },
    {
      "epoch": 3.1549196255078606,
      "grad_norm": 5.338586330413818,
      "learning_rate": 1.84525702172761e-05,
      "loss": 5.9939,
      "step": 17860
    },
    {
      "epoch": 3.158452570217276,
      "grad_norm": 5.917939186096191,
      "learning_rate": 1.841724077018195e-05,
      "loss": 6.0906,
      "step": 17880
    },
    {
      "epoch": 3.1619855149266916,
      "grad_norm": 4.597060680389404,
      "learning_rate": 1.8381911323087793e-05,
      "loss": 6.1558,
      "step": 17900
    },
    {
      "epoch": 3.1655184596361066,
      "grad_norm": 4.081207275390625,
      "learning_rate": 1.834658187599364e-05,
      "loss": 6.2018,
      "step": 17920
    },
    {
      "epoch": 3.169051404345522,
      "grad_norm": 5.583325386047363,
      "learning_rate": 1.831125242889949e-05,
      "loss": 6.015,
      "step": 17940
    },
    {
      "epoch": 3.1725843490549375,
      "grad_norm": 7.630334854125977,
      "learning_rate": 1.8275922981805334e-05,
      "loss": 6.0625,
      "step": 17960
    },
    {
      "epoch": 3.1761172937643525,
      "grad_norm": 6.453568458557129,
      "learning_rate": 1.8240593534711185e-05,
      "loss": 6.0041,
      "step": 17980
    },
    {
      "epoch": 3.179650238473768,
      "grad_norm": 4.524495601654053,
      "learning_rate": 1.820526408761703e-05,
      "loss": 6.0713,
      "step": 18000
    },
    {
      "epoch": 3.1831831831831834,
      "grad_norm": 8.772602081298828,
      "learning_rate": 1.8169934640522876e-05,
      "loss": 6.0189,
      "step": 18020
    },
    {
      "epoch": 3.1867161278925984,
      "grad_norm": 6.271427631378174,
      "learning_rate": 1.8134605193428726e-05,
      "loss": 5.9767,
      "step": 18040
    },
    {
      "epoch": 3.190249072602014,
      "grad_norm": 4.285387992858887,
      "learning_rate": 1.809927574633457e-05,
      "loss": 6.0686,
      "step": 18060
    },
    {
      "epoch": 3.1937820173114293,
      "grad_norm": 5.540162563323975,
      "learning_rate": 1.8063946299240417e-05,
      "loss": 6.0572,
      "step": 18080
    },
    {
      "epoch": 3.1973149620208443,
      "grad_norm": 5.543749809265137,
      "learning_rate": 1.8028616852146264e-05,
      "loss": 6.1529,
      "step": 18100
    },
    {
      "epoch": 3.2008479067302598,
      "grad_norm": 5.880597114562988,
      "learning_rate": 1.799328740505211e-05,
      "loss": 6.0533,
      "step": 18120
    },
    {
      "epoch": 3.2043808514396748,
      "grad_norm": 5.58390998840332,
      "learning_rate": 1.795795795795796e-05,
      "loss": 5.9592,
      "step": 18140
    },
    {
      "epoch": 3.2079137961490902,
      "grad_norm": 5.605345726013184,
      "learning_rate": 1.7922628510863805e-05,
      "loss": 6.078,
      "step": 18160
    },
    {
      "epoch": 3.2114467408585057,
      "grad_norm": 7.73813009262085,
      "learning_rate": 1.7887299063769653e-05,
      "loss": 6.2145,
      "step": 18180
    },
    {
      "epoch": 3.2149796855679207,
      "grad_norm": 4.969625949859619,
      "learning_rate": 1.78519696166755e-05,
      "loss": 6.1579,
      "step": 18200
    },
    {
      "epoch": 3.218512630277336,
      "grad_norm": 5.364051342010498,
      "learning_rate": 1.7816640169581347e-05,
      "loss": 5.9949,
      "step": 18220
    },
    {
      "epoch": 3.2220455749867516,
      "grad_norm": 5.086876392364502,
      "learning_rate": 1.7781310722487194e-05,
      "loss": 6.0439,
      "step": 18240
    },
    {
      "epoch": 3.2255785196961666,
      "grad_norm": 6.252832412719727,
      "learning_rate": 1.774598127539304e-05,
      "loss": 6.1713,
      "step": 18260
    },
    {
      "epoch": 3.229111464405582,
      "grad_norm": 5.14478874206543,
      "learning_rate": 1.7710651828298888e-05,
      "loss": 6.0565,
      "step": 18280
    },
    {
      "epoch": 3.2326444091149975,
      "grad_norm": 5.3999810218811035,
      "learning_rate": 1.7675322381204735e-05,
      "loss": 5.9522,
      "step": 18300
    },
    {
      "epoch": 3.2361773538244125,
      "grad_norm": 3.5628037452697754,
      "learning_rate": 1.7639992934110582e-05,
      "loss": 6.1965,
      "step": 18320
    },
    {
      "epoch": 3.239710298533828,
      "grad_norm": 5.220704078674316,
      "learning_rate": 1.760466348701643e-05,
      "loss": 6.0473,
      "step": 18340
    },
    {
      "epoch": 3.2432432432432434,
      "grad_norm": 6.631680965423584,
      "learning_rate": 1.7569334039922277e-05,
      "loss": 6.1425,
      "step": 18360
    },
    {
      "epoch": 3.2467761879526584,
      "grad_norm": 3.8572232723236084,
      "learning_rate": 1.7534004592828124e-05,
      "loss": 6.0373,
      "step": 18380
    },
    {
      "epoch": 3.250309132662074,
      "grad_norm": 5.870222568511963,
      "learning_rate": 1.749867514573397e-05,
      "loss": 6.1197,
      "step": 18400
    },
    {
      "epoch": 3.2538420773714893,
      "grad_norm": 5.573754787445068,
      "learning_rate": 1.7463345698639815e-05,
      "loss": 6.0707,
      "step": 18420
    },
    {
      "epoch": 3.2573750220809043,
      "grad_norm": 6.246384143829346,
      "learning_rate": 1.7428016251545665e-05,
      "loss": 6.0202,
      "step": 18440
    },
    {
      "epoch": 3.26090796679032,
      "grad_norm": 5.069446563720703,
      "learning_rate": 1.7392686804451512e-05,
      "loss": 6.0999,
      "step": 18460
    },
    {
      "epoch": 3.264440911499735,
      "grad_norm": 7.489522457122803,
      "learning_rate": 1.735735735735736e-05,
      "loss": 5.9228,
      "step": 18480
    },
    {
      "epoch": 3.2679738562091503,
      "grad_norm": 4.807502746582031,
      "learning_rate": 1.7322027910263206e-05,
      "loss": 6.1511,
      "step": 18500
    },
    {
      "epoch": 3.2715068009185657,
      "grad_norm": 5.343313694000244,
      "learning_rate": 1.728669846316905e-05,
      "loss": 6.0448,
      "step": 18520
    },
    {
      "epoch": 3.275039745627981,
      "grad_norm": 6.33812952041626,
      "learning_rate": 1.72513690160749e-05,
      "loss": 5.8627,
      "step": 18540
    },
    {
      "epoch": 3.278572690337396,
      "grad_norm": 5.411532878875732,
      "learning_rate": 1.7216039568980748e-05,
      "loss": 6.1195,
      "step": 18560
    },
    {
      "epoch": 3.2821056350468116,
      "grad_norm": 5.983673095703125,
      "learning_rate": 1.718071012188659e-05,
      "loss": 6.2354,
      "step": 18580
    },
    {
      "epoch": 3.2856385797562266,
      "grad_norm": 4.634200572967529,
      "learning_rate": 1.7145380674792442e-05,
      "loss": 6.1052,
      "step": 18600
    },
    {
      "epoch": 3.289171524465642,
      "grad_norm": 5.412171363830566,
      "learning_rate": 1.7110051227698286e-05,
      "loss": 6.0729,
      "step": 18620
    },
    {
      "epoch": 3.2927044691750575,
      "grad_norm": 5.994733810424805,
      "learning_rate": 1.7074721780604133e-05,
      "loss": 6.0998,
      "step": 18640
    },
    {
      "epoch": 3.2962374138844726,
      "grad_norm": 6.821359634399414,
      "learning_rate": 1.7039392333509983e-05,
      "loss": 5.9861,
      "step": 18660
    },
    {
      "epoch": 3.299770358593888,
      "grad_norm": 6.331686496734619,
      "learning_rate": 1.7004062886415827e-05,
      "loss": 6.1794,
      "step": 18680
    },
    {
      "epoch": 3.3033033033033035,
      "grad_norm": 5.663290500640869,
      "learning_rate": 1.6968733439321677e-05,
      "loss": 5.8961,
      "step": 18700
    },
    {
      "epoch": 3.3068362480127185,
      "grad_norm": 5.509191989898682,
      "learning_rate": 1.6933403992227525e-05,
      "loss": 6.1104,
      "step": 18720
    },
    {
      "epoch": 3.310369192722134,
      "grad_norm": 5.177720546722412,
      "learning_rate": 1.6898074545133368e-05,
      "loss": 6.0334,
      "step": 18740
    },
    {
      "epoch": 3.3139021374315494,
      "grad_norm": 5.063615322113037,
      "learning_rate": 1.686274509803922e-05,
      "loss": 6.1397,
      "step": 18760
    },
    {
      "epoch": 3.3174350821409644,
      "grad_norm": 7.45560359954834,
      "learning_rate": 1.6827415650945062e-05,
      "loss": 6.1163,
      "step": 18780
    },
    {
      "epoch": 3.32096802685038,
      "grad_norm": 4.159889221191406,
      "learning_rate": 1.679208620385091e-05,
      "loss": 5.9831,
      "step": 18800
    },
    {
      "epoch": 3.324500971559795,
      "grad_norm": 5.2165422439575195,
      "learning_rate": 1.675675675675676e-05,
      "loss": 6.1366,
      "step": 18820
    },
    {
      "epoch": 3.3280339162692103,
      "grad_norm": 5.1237287521362305,
      "learning_rate": 1.6721427309662604e-05,
      "loss": 6.1252,
      "step": 18840
    },
    {
      "epoch": 3.3315668609786258,
      "grad_norm": 5.7941155433654785,
      "learning_rate": 1.668609786256845e-05,
      "loss": 6.1324,
      "step": 18860
    },
    {
      "epoch": 3.335099805688041,
      "grad_norm": 6.674384117126465,
      "learning_rate": 1.6650768415474298e-05,
      "loss": 6.1211,
      "step": 18880
    },
    {
      "epoch": 3.338632750397456,
      "grad_norm": 5.91032075881958,
      "learning_rate": 1.6615438968380145e-05,
      "loss": 6.0851,
      "step": 18900
    },
    {
      "epoch": 3.3421656951068717,
      "grad_norm": 8.160966873168945,
      "learning_rate": 1.6580109521285992e-05,
      "loss": 5.9878,
      "step": 18920
    },
    {
      "epoch": 3.3456986398162867,
      "grad_norm": 5.614634990692139,
      "learning_rate": 1.654478007419184e-05,
      "loss": 6.1305,
      "step": 18940
    },
    {
      "epoch": 3.349231584525702,
      "grad_norm": 5.236543655395508,
      "learning_rate": 1.6509450627097686e-05,
      "loss": 6.0448,
      "step": 18960
    },
    {
      "epoch": 3.3527645292351176,
      "grad_norm": 4.5517659187316895,
      "learning_rate": 1.6474121180003534e-05,
      "loss": 6.0264,
      "step": 18980
    },
    {
      "epoch": 3.3562974739445326,
      "grad_norm": 5.597684860229492,
      "learning_rate": 1.643879173290938e-05,
      "loss": 6.053,
      "step": 19000
    },
    {
      "epoch": 3.359830418653948,
      "grad_norm": 12.762521743774414,
      "learning_rate": 1.6403462285815228e-05,
      "loss": 6.0508,
      "step": 19020
    },
    {
      "epoch": 3.3633633633633635,
      "grad_norm": 6.686087608337402,
      "learning_rate": 1.6368132838721075e-05,
      "loss": 6.0702,
      "step": 19040
    },
    {
      "epoch": 3.3668963080727785,
      "grad_norm": 8.15401840209961,
      "learning_rate": 1.6332803391626922e-05,
      "loss": 6.107,
      "step": 19060
    },
    {
      "epoch": 3.370429252782194,
      "grad_norm": 5.20070219039917,
      "learning_rate": 1.629747394453277e-05,
      "loss": 6.1266,
      "step": 19080
    },
    {
      "epoch": 3.3739621974916094,
      "grad_norm": 6.325072288513184,
      "learning_rate": 1.6262144497438616e-05,
      "loss": 6.1083,
      "step": 19100
    },
    {
      "epoch": 3.3774951422010244,
      "grad_norm": 5.463371276855469,
      "learning_rate": 1.6226815050344463e-05,
      "loss": 6.0843,
      "step": 19120
    },
    {
      "epoch": 3.38102808691044,
      "grad_norm": 7.066322326660156,
      "learning_rate": 1.619148560325031e-05,
      "loss": 6.0733,
      "step": 19140
    },
    {
      "epoch": 3.3845610316198553,
      "grad_norm": 5.152021408081055,
      "learning_rate": 1.6156156156156157e-05,
      "loss": 5.9664,
      "step": 19160
    },
    {
      "epoch": 3.3880939763292703,
      "grad_norm": 7.558361053466797,
      "learning_rate": 1.6120826709062005e-05,
      "loss": 6.1227,
      "step": 19180
    },
    {
      "epoch": 3.391626921038686,
      "grad_norm": 5.279222011566162,
      "learning_rate": 1.6085497261967852e-05,
      "loss": 6.2296,
      "step": 19200
    },
    {
      "epoch": 3.3951598657481012,
      "grad_norm": 4.7450032234191895,
      "learning_rate": 1.60501678148737e-05,
      "loss": 6.0418,
      "step": 19220
    },
    {
      "epoch": 3.3986928104575163,
      "grad_norm": 6.079216480255127,
      "learning_rate": 1.6014838367779546e-05,
      "loss": 6.1939,
      "step": 19240
    },
    {
      "epoch": 3.4022257551669317,
      "grad_norm": 4.402517795562744,
      "learning_rate": 1.5979508920685393e-05,
      "loss": 6.103,
      "step": 19260
    },
    {
      "epoch": 3.4057586998763467,
      "grad_norm": 4.884817600250244,
      "learning_rate": 1.594417947359124e-05,
      "loss": 6.0575,
      "step": 19280
    },
    {
      "epoch": 3.409291644585762,
      "grad_norm": 6.63920259475708,
      "learning_rate": 1.5908850026497084e-05,
      "loss": 6.107,
      "step": 19300
    },
    {
      "epoch": 3.4128245892951776,
      "grad_norm": 5.728780746459961,
      "learning_rate": 1.5873520579402934e-05,
      "loss": 6.1331,
      "step": 19320
    },
    {
      "epoch": 3.416357534004593,
      "grad_norm": 5.3147759437561035,
      "learning_rate": 1.583819113230878e-05,
      "loss": 6.0717,
      "step": 19340
    },
    {
      "epoch": 3.419890478714008,
      "grad_norm": 6.605123996734619,
      "learning_rate": 1.5802861685214625e-05,
      "loss": 6.1162,
      "step": 19360
    },
    {
      "epoch": 3.4234234234234235,
      "grad_norm": 6.179511070251465,
      "learning_rate": 1.5767532238120476e-05,
      "loss": 6.1289,
      "step": 19380
    },
    {
      "epoch": 3.4269563681328385,
      "grad_norm": 5.128077030181885,
      "learning_rate": 1.573220279102632e-05,
      "loss": 6.114,
      "step": 19400
    },
    {
      "epoch": 3.430489312842254,
      "grad_norm": 5.456145763397217,
      "learning_rate": 1.5696873343932167e-05,
      "loss": 6.0156,
      "step": 19420
    },
    {
      "epoch": 3.4340222575516695,
      "grad_norm": 5.147080898284912,
      "learning_rate": 1.5661543896838017e-05,
      "loss": 6.1335,
      "step": 19440
    },
    {
      "epoch": 3.4375552022610845,
      "grad_norm": 7.967092514038086,
      "learning_rate": 1.562621444974386e-05,
      "loss": 6.1626,
      "step": 19460
    },
    {
      "epoch": 3.4410881469705,
      "grad_norm": 6.575359344482422,
      "learning_rate": 1.559088500264971e-05,
      "loss": 6.0722,
      "step": 19480
    },
    {
      "epoch": 3.4446210916799154,
      "grad_norm": 8.332601547241211,
      "learning_rate": 1.5555555555555555e-05,
      "loss": 6.02,
      "step": 19500
    },
    {
      "epoch": 3.4481540363893304,
      "grad_norm": 4.975987434387207,
      "learning_rate": 1.5520226108461402e-05,
      "loss": 6.0586,
      "step": 19520
    },
    {
      "epoch": 3.451686981098746,
      "grad_norm": 4.5035319328308105,
      "learning_rate": 1.5484896661367253e-05,
      "loss": 5.9322,
      "step": 19540
    },
    {
      "epoch": 3.4552199258081613,
      "grad_norm": 5.02323055267334,
      "learning_rate": 1.5449567214273096e-05,
      "loss": 5.9901,
      "step": 19560
    },
    {
      "epoch": 3.4587528705175763,
      "grad_norm": 7.218209743499756,
      "learning_rate": 1.5414237767178943e-05,
      "loss": 6.084,
      "step": 19580
    },
    {
      "epoch": 3.4622858152269917,
      "grad_norm": 5.406628131866455,
      "learning_rate": 1.5378908320084794e-05,
      "loss": 6.0633,
      "step": 19600
    },
    {
      "epoch": 3.4658187599364068,
      "grad_norm": 5.3594441413879395,
      "learning_rate": 1.5343578872990638e-05,
      "loss": 6.0975,
      "step": 19620
    },
    {
      "epoch": 3.469351704645822,
      "grad_norm": 6.159677982330322,
      "learning_rate": 1.5308249425896485e-05,
      "loss": 5.9988,
      "step": 19640
    },
    {
      "epoch": 3.4728846493552377,
      "grad_norm": 11.375577926635742,
      "learning_rate": 1.5272919978802332e-05,
      "loss": 6.1064,
      "step": 19660
    },
    {
      "epoch": 3.476417594064653,
      "grad_norm": 5.2257771492004395,
      "learning_rate": 1.5237590531708179e-05,
      "loss": 6.1167,
      "step": 19680
    },
    {
      "epoch": 3.479950538774068,
      "grad_norm": 5.73963737487793,
      "learning_rate": 1.5202261084614028e-05,
      "loss": 6.1343,
      "step": 19700
    },
    {
      "epoch": 3.4834834834834836,
      "grad_norm": 6.25850248336792,
      "learning_rate": 1.5166931637519873e-05,
      "loss": 5.9554,
      "step": 19720
    },
    {
      "epoch": 3.4870164281928986,
      "grad_norm": 5.882789611816406,
      "learning_rate": 1.513160219042572e-05,
      "loss": 5.9549,
      "step": 19740
    },
    {
      "epoch": 3.490549372902314,
      "grad_norm": 3.9778051376342773,
      "learning_rate": 1.5096272743331569e-05,
      "loss": 6.0185,
      "step": 19760
    },
    {
      "epoch": 3.4940823176117295,
      "grad_norm": 5.561087608337402,
      "learning_rate": 1.5060943296237414e-05,
      "loss": 6.1853,
      "step": 19780
    },
    {
      "epoch": 3.4976152623211445,
      "grad_norm": 4.409276485443115,
      "learning_rate": 1.5025613849143262e-05,
      "loss": 6.0804,
      "step": 19800
    },
    {
      "epoch": 3.50114820703056,
      "grad_norm": 5.921575546264648,
      "learning_rate": 1.499028440204911e-05,
      "loss": 6.021,
      "step": 19820
    },
    {
      "epoch": 3.5046811517399754,
      "grad_norm": 5.947900295257568,
      "learning_rate": 1.4954954954954956e-05,
      "loss": 6.1246,
      "step": 19840
    },
    {
      "epoch": 3.5082140964493904,
      "grad_norm": 5.859098434448242,
      "learning_rate": 1.4919625507860801e-05,
      "loss": 6.0111,
      "step": 19860
    },
    {
      "epoch": 3.511747041158806,
      "grad_norm": 6.503411293029785,
      "learning_rate": 1.488429606076665e-05,
      "loss": 5.9485,
      "step": 19880
    },
    {
      "epoch": 3.5152799858682213,
      "grad_norm": 4.731358051300049,
      "learning_rate": 1.4848966613672497e-05,
      "loss": 5.8428,
      "step": 19900
    },
    {
      "epoch": 3.5188129305776363,
      "grad_norm": 4.932625770568848,
      "learning_rate": 1.4813637166578343e-05,
      "loss": 6.2485,
      "step": 19920
    },
    {
      "epoch": 3.522345875287052,
      "grad_norm": 4.39272403717041,
      "learning_rate": 1.4778307719484191e-05,
      "loss": 6.123,
      "step": 19940
    },
    {
      "epoch": 3.525878819996467,
      "grad_norm": 4.6024651527404785,
      "learning_rate": 1.4742978272390037e-05,
      "loss": 6.2341,
      "step": 19960
    },
    {
      "epoch": 3.5294117647058822,
      "grad_norm": 7.715967655181885,
      "learning_rate": 1.4707648825295886e-05,
      "loss": 6.0268,
      "step": 19980
    },
    {
      "epoch": 3.5329447094152977,
      "grad_norm": 4.743484973907471,
      "learning_rate": 1.4672319378201733e-05,
      "loss": 6.1385,
      "step": 20000
    },
    {
      "epoch": 3.536477654124713,
      "grad_norm": 7.130825519561768,
      "learning_rate": 1.4636989931107578e-05,
      "loss": 6.119,
      "step": 20020
    },
    {
      "epoch": 3.540010598834128,
      "grad_norm": 5.774817943572998,
      "learning_rate": 1.4601660484013427e-05,
      "loss": 6.1017,
      "step": 20040
    },
    {
      "epoch": 3.5435435435435436,
      "grad_norm": 6.6794610023498535,
      "learning_rate": 1.4566331036919272e-05,
      "loss": 6.1575,
      "step": 20060
    },
    {
      "epoch": 3.5470764882529586,
      "grad_norm": 6.163045406341553,
      "learning_rate": 1.453100158982512e-05,
      "loss": 6.2461,
      "step": 20080
    },
    {
      "epoch": 3.550609432962374,
      "grad_norm": 4.414893627166748,
      "learning_rate": 1.4495672142730968e-05,
      "loss": 6.0669,
      "step": 20100
    },
    {
      "epoch": 3.5541423776717895,
      "grad_norm": 5.499738693237305,
      "learning_rate": 1.4460342695636814e-05,
      "loss": 6.3238,
      "step": 20120
    },
    {
      "epoch": 3.557675322381205,
      "grad_norm": 5.915602207183838,
      "learning_rate": 1.4425013248542659e-05,
      "loss": 6.2018,
      "step": 20140
    },
    {
      "epoch": 3.56120826709062,
      "grad_norm": 6.708395481109619,
      "learning_rate": 1.4389683801448508e-05,
      "loss": 6.0547,
      "step": 20160
    },
    {
      "epoch": 3.5647412118000354,
      "grad_norm": 5.589298248291016,
      "learning_rate": 1.4354354354354355e-05,
      "loss": 6.1266,
      "step": 20180
    },
    {
      "epoch": 3.5682741565094505,
      "grad_norm": 4.545877456665039,
      "learning_rate": 1.4319024907260204e-05,
      "loss": 6.0444,
      "step": 20200
    },
    {
      "epoch": 3.571807101218866,
      "grad_norm": 5.297930717468262,
      "learning_rate": 1.4283695460166049e-05,
      "loss": 6.0484,
      "step": 20220
    },
    {
      "epoch": 3.5753400459282814,
      "grad_norm": 6.225391864776611,
      "learning_rate": 1.4248366013071896e-05,
      "loss": 6.1495,
      "step": 20240
    },
    {
      "epoch": 3.5788729906376964,
      "grad_norm": 5.561765193939209,
      "learning_rate": 1.4213036565977745e-05,
      "loss": 6.0231,
      "step": 20260
    },
    {
      "epoch": 3.582405935347112,
      "grad_norm": 5.8315629959106445,
      "learning_rate": 1.417770711888359e-05,
      "loss": 6.1048,
      "step": 20280
    },
    {
      "epoch": 3.5859388800565273,
      "grad_norm": 7.434295654296875,
      "learning_rate": 1.4142377671789436e-05,
      "loss": 6.063,
      "step": 20300
    },
    {
      "epoch": 3.5894718247659423,
      "grad_norm": 5.9139404296875,
      "learning_rate": 1.4107048224695285e-05,
      "loss": 6.1506,
      "step": 20320
    },
    {
      "epoch": 3.5930047694753577,
      "grad_norm": 6.735517978668213,
      "learning_rate": 1.4071718777601132e-05,
      "loss": 6.1325,
      "step": 20340
    },
    {
      "epoch": 3.596537714184773,
      "grad_norm": 3.8752949237823486,
      "learning_rate": 1.4036389330506977e-05,
      "loss": 6.1551,
      "step": 20360
    },
    {
      "epoch": 3.600070658894188,
      "grad_norm": 6.611447334289551,
      "learning_rate": 1.4001059883412826e-05,
      "loss": 6.1699,
      "step": 20380
    },
    {
      "epoch": 3.6036036036036037,
      "grad_norm": 8.285500526428223,
      "learning_rate": 1.3965730436318671e-05,
      "loss": 5.966,
      "step": 20400
    },
    {
      "epoch": 3.6071365483130187,
      "grad_norm": 6.177441596984863,
      "learning_rate": 1.3930400989224519e-05,
      "loss": 5.9531,
      "step": 20420
    },
    {
      "epoch": 3.610669493022434,
      "grad_norm": 4.32745885848999,
      "learning_rate": 1.3895071542130367e-05,
      "loss": 6.1447,
      "step": 20440
    },
    {
      "epoch": 3.6142024377318496,
      "grad_norm": 8.079983711242676,
      "learning_rate": 1.3859742095036213e-05,
      "loss": 5.8675,
      "step": 20460
    },
    {
      "epoch": 3.617735382441265,
      "grad_norm": 5.965993404388428,
      "learning_rate": 1.3824412647942062e-05,
      "loss": 6.0252,
      "step": 20480
    },
    {
      "epoch": 3.62126832715068,
      "grad_norm": 4.654601573944092,
      "learning_rate": 1.3789083200847907e-05,
      "loss": 6.0836,
      "step": 20500
    },
    {
      "epoch": 3.6248012718600955,
      "grad_norm": 5.169610500335693,
      "learning_rate": 1.3753753753753754e-05,
      "loss": 6.2266,
      "step": 20520
    },
    {
      "epoch": 3.6283342165695105,
      "grad_norm": 4.734626293182373,
      "learning_rate": 1.3718424306659603e-05,
      "loss": 6.0779,
      "step": 20540
    },
    {
      "epoch": 3.631867161278926,
      "grad_norm": 6.338656902313232,
      "learning_rate": 1.3683094859565448e-05,
      "loss": 5.8606,
      "step": 20560
    },
    {
      "epoch": 3.6354001059883414,
      "grad_norm": 6.830764293670654,
      "learning_rate": 1.3647765412471294e-05,
      "loss": 6.0859,
      "step": 20580
    },
    {
      "epoch": 3.638933050697757,
      "grad_norm": 7.994452953338623,
      "learning_rate": 1.3612435965377142e-05,
      "loss": 5.9102,
      "step": 20600
    },
    {
      "epoch": 3.642465995407172,
      "grad_norm": 5.206066131591797,
      "learning_rate": 1.357710651828299e-05,
      "loss": 6.0536,
      "step": 20620
    },
    {
      "epoch": 3.6459989401165873,
      "grad_norm": 8.916748046875,
      "learning_rate": 1.3541777071188835e-05,
      "loss": 5.9985,
      "step": 20640
    },
    {
      "epoch": 3.6495318848260023,
      "grad_norm": 6.182560920715332,
      "learning_rate": 1.3506447624094684e-05,
      "loss": 5.9068,
      "step": 20660
    },
    {
      "epoch": 3.653064829535418,
      "grad_norm": 4.830123424530029,
      "learning_rate": 1.3471118177000531e-05,
      "loss": 6.149,
      "step": 20680
    },
    {
      "epoch": 3.6565977742448332,
      "grad_norm": 5.777958869934082,
      "learning_rate": 1.343578872990638e-05,
      "loss": 6.0612,
      "step": 20700
    },
    {
      "epoch": 3.6601307189542482,
      "grad_norm": 5.512069225311279,
      "learning_rate": 1.3400459282812225e-05,
      "loss": 5.9694,
      "step": 20720
    },
    {
      "epoch": 3.6636636636636637,
      "grad_norm": 6.140931129455566,
      "learning_rate": 1.336512983571807e-05,
      "loss": 6.1251,
      "step": 20740
    },
    {
      "epoch": 3.6671966083730787,
      "grad_norm": 6.724717140197754,
      "learning_rate": 1.332980038862392e-05,
      "loss": 6.0956,
      "step": 20760
    },
    {
      "epoch": 3.670729553082494,
      "grad_norm": 5.39150857925415,
      "learning_rate": 1.3294470941529766e-05,
      "loss": 6.105,
      "step": 20780
    },
    {
      "epoch": 3.6742624977919096,
      "grad_norm": 6.6280388832092285,
      "learning_rate": 1.3259141494435612e-05,
      "loss": 6.0911,
      "step": 20800
    },
    {
      "epoch": 3.677795442501325,
      "grad_norm": 5.147246837615967,
      "learning_rate": 1.322381204734146e-05,
      "loss": 6.0943,
      "step": 20820
    },
    {
      "epoch": 3.68132838721074,
      "grad_norm": 6.591007232666016,
      "learning_rate": 1.3188482600247306e-05,
      "loss": 5.9787,
      "step": 20840
    },
    {
      "epoch": 3.6848613319201555,
      "grad_norm": 4.732812404632568,
      "learning_rate": 1.3153153153153153e-05,
      "loss": 5.9187,
      "step": 20860
    },
    {
      "epoch": 3.6883942766295705,
      "grad_norm": 6.012028694152832,
      "learning_rate": 1.3117823706059002e-05,
      "loss": 6.0849,
      "step": 20880
    },
    {
      "epoch": 3.691927221338986,
      "grad_norm": 4.15037202835083,
      "learning_rate": 1.3082494258964847e-05,
      "loss": 6.0636,
      "step": 20900
    },
    {
      "epoch": 3.6954601660484014,
      "grad_norm": 5.98935604095459,
      "learning_rate": 1.3047164811870693e-05,
      "loss": 5.9983,
      "step": 20920
    },
    {
      "epoch": 3.698993110757817,
      "grad_norm": 5.46106481552124,
      "learning_rate": 1.3011835364776542e-05,
      "loss": 6.0507,
      "step": 20940
    },
    {
      "epoch": 3.702526055467232,
      "grad_norm": 6.300302505493164,
      "learning_rate": 1.2976505917682389e-05,
      "loss": 6.1016,
      "step": 20960
    },
    {
      "epoch": 3.7060590001766474,
      "grad_norm": 4.798513412475586,
      "learning_rate": 1.2941176470588238e-05,
      "loss": 6.0718,
      "step": 20980
    },
    {
      "epoch": 3.7095919448860624,
      "grad_norm": 4.330473899841309,
      "learning_rate": 1.2905847023494083e-05,
      "loss": 6.1183,
      "step": 21000
    },
    {
      "epoch": 3.713124889595478,
      "grad_norm": 5.930501461029053,
      "learning_rate": 1.2870517576399928e-05,
      "loss": 6.1001,
      "step": 21020
    },
    {
      "epoch": 3.7166578343048933,
      "grad_norm": 4.858200550079346,
      "learning_rate": 1.2835188129305777e-05,
      "loss": 6.0504,
      "step": 21040
    },
    {
      "epoch": 3.7201907790143083,
      "grad_norm": 5.202653408050537,
      "learning_rate": 1.2799858682211624e-05,
      "loss": 5.9721,
      "step": 21060
    },
    {
      "epoch": 3.7237237237237237,
      "grad_norm": 4.272921562194824,
      "learning_rate": 1.276452923511747e-05,
      "loss": 5.95,
      "step": 21080
    },
    {
      "epoch": 3.727256668433139,
      "grad_norm": 5.418572425842285,
      "learning_rate": 1.2729199788023319e-05,
      "loss": 6.0042,
      "step": 21100
    },
    {
      "epoch": 3.730789613142554,
      "grad_norm": 6.2664690017700195,
      "learning_rate": 1.2693870340929166e-05,
      "loss": 6.0062,
      "step": 21120
    },
    {
      "epoch": 3.7343225578519696,
      "grad_norm": 5.660012245178223,
      "learning_rate": 1.2658540893835011e-05,
      "loss": 6.0452,
      "step": 21140
    },
    {
      "epoch": 3.737855502561385,
      "grad_norm": 5.826183319091797,
      "learning_rate": 1.262321144674086e-05,
      "loss": 6.0492,
      "step": 21160
    },
    {
      "epoch": 3.7413884472708,
      "grad_norm": 6.574284076690674,
      "learning_rate": 1.2587881999646705e-05,
      "loss": 6.0719,
      "step": 21180
    },
    {
      "epoch": 3.7449213919802156,
      "grad_norm": 7.5463080406188965,
      "learning_rate": 1.2552552552552554e-05,
      "loss": 6.1453,
      "step": 21200
    },
    {
      "epoch": 3.7484543366896306,
      "grad_norm": 6.523699760437012,
      "learning_rate": 1.2517223105458401e-05,
      "loss": 6.0968,
      "step": 21220
    },
    {
      "epoch": 3.751987281399046,
      "grad_norm": 5.484009742736816,
      "learning_rate": 1.2481893658364247e-05,
      "loss": 6.2221,
      "step": 21240
    },
    {
      "epoch": 3.7555202261084615,
      "grad_norm": 5.810328006744385,
      "learning_rate": 1.2446564211270094e-05,
      "loss": 5.9931,
      "step": 21260
    },
    {
      "epoch": 3.759053170817877,
      "grad_norm": 6.537298202514648,
      "learning_rate": 1.241123476417594e-05,
      "loss": 6.1567,
      "step": 21280
    },
    {
      "epoch": 3.762586115527292,
      "grad_norm": 5.6001877784729,
      "learning_rate": 1.237590531708179e-05,
      "loss": 6.176,
      "step": 21300
    },
    {
      "epoch": 3.7661190602367074,
      "grad_norm": 6.56154203414917,
      "learning_rate": 1.2340575869987635e-05,
      "loss": 5.9451,
      "step": 21320
    },
    {
      "epoch": 3.7696520049461224,
      "grad_norm": 5.972870349884033,
      "learning_rate": 1.2305246422893482e-05,
      "loss": 6.0069,
      "step": 21340
    },
    {
      "epoch": 3.773184949655538,
      "grad_norm": 5.233994960784912,
      "learning_rate": 1.226991697579933e-05,
      "loss": 6.1325,
      "step": 21360
    },
    {
      "epoch": 3.7767178943649533,
      "grad_norm": 5.531635284423828,
      "learning_rate": 1.2234587528705176e-05,
      "loss": 6.011,
      "step": 21380
    },
    {
      "epoch": 3.7802508390743688,
      "grad_norm": 6.167291641235352,
      "learning_rate": 1.2199258081611023e-05,
      "loss": 6.0892,
      "step": 21400
    },
    {
      "epoch": 3.7837837837837838,
      "grad_norm": 5.357098579406738,
      "learning_rate": 1.216392863451687e-05,
      "loss": 6.1807,
      "step": 21420
    },
    {
      "epoch": 3.7873167284931992,
      "grad_norm": 6.622135639190674,
      "learning_rate": 1.2128599187422718e-05,
      "loss": 5.9823,
      "step": 21440
    },
    {
      "epoch": 3.7908496732026142,
      "grad_norm": 7.550996780395508,
      "learning_rate": 1.2093269740328563e-05,
      "loss": 6.0619,
      "step": 21460
    },
    {
      "epoch": 3.7943826179120297,
      "grad_norm": 5.209625244140625,
      "learning_rate": 1.2057940293234412e-05,
      "loss": 6.0041,
      "step": 21480
    },
    {
      "epoch": 3.797915562621445,
      "grad_norm": 5.274588108062744,
      "learning_rate": 1.2022610846140259e-05,
      "loss": 6.0522,
      "step": 21500
    },
    {
      "epoch": 3.80144850733086,
      "grad_norm": 4.91967248916626,
      "learning_rate": 1.1987281399046106e-05,
      "loss": 6.0151,
      "step": 21520
    },
    {
      "epoch": 3.8049814520402756,
      "grad_norm": 7.8510541915893555,
      "learning_rate": 1.1951951951951951e-05,
      "loss": 6.1577,
      "step": 21540
    },
    {
      "epoch": 3.8085143967496906,
      "grad_norm": 4.977729797363281,
      "learning_rate": 1.19166225048578e-05,
      "loss": 6.0909,
      "step": 21560
    },
    {
      "epoch": 3.812047341459106,
      "grad_norm": 6.036916732788086,
      "learning_rate": 1.1881293057763647e-05,
      "loss": 6.111,
      "step": 21580
    },
    {
      "epoch": 3.8155802861685215,
      "grad_norm": 5.189669132232666,
      "learning_rate": 1.1845963610669493e-05,
      "loss": 6.1206,
      "step": 21600
    },
    {
      "epoch": 3.819113230877937,
      "grad_norm": 7.210293769836426,
      "learning_rate": 1.181063416357534e-05,
      "loss": 5.9385,
      "step": 21620
    },
    {
      "epoch": 3.822646175587352,
      "grad_norm": 5.784574508666992,
      "learning_rate": 1.1775304716481187e-05,
      "loss": 6.0104,
      "step": 21640
    },
    {
      "epoch": 3.8261791202967674,
      "grad_norm": 7.420589923858643,
      "learning_rate": 1.1739975269387036e-05,
      "loss": 5.9426,
      "step": 21660
    },
    {
      "epoch": 3.8297120650061824,
      "grad_norm": 7.711899280548096,
      "learning_rate": 1.1704645822292881e-05,
      "loss": 6.0626,
      "step": 21680
    },
    {
      "epoch": 3.833245009715598,
      "grad_norm": 5.215768337249756,
      "learning_rate": 1.1669316375198728e-05,
      "loss": 6.1701,
      "step": 21700
    },
    {
      "epoch": 3.8367779544250133,
      "grad_norm": 4.8021559715271,
      "learning_rate": 1.1633986928104575e-05,
      "loss": 6.0245,
      "step": 21720
    },
    {
      "epoch": 3.840310899134429,
      "grad_norm": 5.656996250152588,
      "learning_rate": 1.1598657481010424e-05,
      "loss": 5.9537,
      "step": 21740
    },
    {
      "epoch": 3.843843843843844,
      "grad_norm": 4.666405200958252,
      "learning_rate": 1.156332803391627e-05,
      "loss": 5.9449,
      "step": 21760
    },
    {
      "epoch": 3.8473767885532593,
      "grad_norm": 5.704372882843018,
      "learning_rate": 1.1527998586822117e-05,
      "loss": 6.0672,
      "step": 21780
    },
    {
      "epoch": 3.8509097332626743,
      "grad_norm": 6.336370468139648,
      "learning_rate": 1.1492669139727964e-05,
      "loss": 5.9467,
      "step": 21800
    },
    {
      "epoch": 3.8544426779720897,
      "grad_norm": 5.075393199920654,
      "learning_rate": 1.1457339692633811e-05,
      "loss": 5.9541,
      "step": 21820
    },
    {
      "epoch": 3.857975622681505,
      "grad_norm": 6.029540538787842,
      "learning_rate": 1.1422010245539658e-05,
      "loss": 6.1339,
      "step": 21840
    },
    {
      "epoch": 3.86150856739092,
      "grad_norm": 5.258704662322998,
      "learning_rate": 1.1386680798445505e-05,
      "loss": 6.0428,
      "step": 21860
    },
    {
      "epoch": 3.8650415121003356,
      "grad_norm": 7.183765888214111,
      "learning_rate": 1.1351351351351352e-05,
      "loss": 5.9202,
      "step": 21880
    },
    {
      "epoch": 3.8685744568097507,
      "grad_norm": 8.865389823913574,
      "learning_rate": 1.1316021904257198e-05,
      "loss": 5.9958,
      "step": 21900
    },
    {
      "epoch": 3.872107401519166,
      "grad_norm": 5.1108503341674805,
      "learning_rate": 1.1280692457163047e-05,
      "loss": 6.1744,
      "step": 21920
    },
    {
      "epoch": 3.8756403462285816,
      "grad_norm": 5.980868339538574,
      "learning_rate": 1.1245363010068894e-05,
      "loss": 6.0905,
      "step": 21940
    },
    {
      "epoch": 3.879173290937997,
      "grad_norm": 4.816411018371582,
      "learning_rate": 1.1210033562974739e-05,
      "loss": 6.0309,
      "step": 21960
    },
    {
      "epoch": 3.882706235647412,
      "grad_norm": 5.099974155426025,
      "learning_rate": 1.1174704115880586e-05,
      "loss": 6.0158,
      "step": 21980
    },
    {
      "epoch": 3.8862391803568275,
      "grad_norm": 5.361636161804199,
      "learning_rate": 1.1139374668786435e-05,
      "loss": 5.9587,
      "step": 22000
    },
    {
      "epoch": 3.8897721250662425,
      "grad_norm": 7.076598644256592,
      "learning_rate": 1.1104045221692282e-05,
      "loss": 6.0953,
      "step": 22020
    },
    {
      "epoch": 3.893305069775658,
      "grad_norm": 4.429396629333496,
      "learning_rate": 1.1068715774598127e-05,
      "loss": 6.0869,
      "step": 22040
    },
    {
      "epoch": 3.8968380144850734,
      "grad_norm": 4.44785213470459,
      "learning_rate": 1.1033386327503975e-05,
      "loss": 6.1526,
      "step": 22060
    },
    {
      "epoch": 3.900370959194489,
      "grad_norm": 9.924291610717773,
      "learning_rate": 1.0998056880409822e-05,
      "loss": 6.1401,
      "step": 22080
    },
    {
      "epoch": 3.903903903903904,
      "grad_norm": 6.018221378326416,
      "learning_rate": 1.0962727433315669e-05,
      "loss": 5.9858,
      "step": 22100
    },
    {
      "epoch": 3.9074368486133193,
      "grad_norm": 4.586276054382324,
      "learning_rate": 1.0927397986221516e-05,
      "loss": 6.1467,
      "step": 22120
    },
    {
      "epoch": 3.9109697933227343,
      "grad_norm": 6.147805690765381,
      "learning_rate": 1.0892068539127363e-05,
      "loss": 6.0173,
      "step": 22140
    },
    {
      "epoch": 3.9145027380321498,
      "grad_norm": 5.452001094818115,
      "learning_rate": 1.085673909203321e-05,
      "loss": 5.9357,
      "step": 22160
    },
    {
      "epoch": 3.918035682741565,
      "grad_norm": 5.626492977142334,
      "learning_rate": 1.0821409644939057e-05,
      "loss": 6.075,
      "step": 22180
    },
    {
      "epoch": 3.9215686274509802,
      "grad_norm": 6.758292198181152,
      "learning_rate": 1.0786080197844904e-05,
      "loss": 6.2425,
      "step": 22200
    },
    {
      "epoch": 3.9251015721603957,
      "grad_norm": 5.673685073852539,
      "learning_rate": 1.0750750750750751e-05,
      "loss": 6.0355,
      "step": 22220
    },
    {
      "epoch": 3.928634516869811,
      "grad_norm": 7.389337539672852,
      "learning_rate": 1.0715421303656599e-05,
      "loss": 6.1228,
      "step": 22240
    },
    {
      "epoch": 3.932167461579226,
      "grad_norm": 5.444482326507568,
      "learning_rate": 1.0680091856562446e-05,
      "loss": 6.1239,
      "step": 22260
    },
    {
      "epoch": 3.9357004062886416,
      "grad_norm": 5.439142227172852,
      "learning_rate": 1.0644762409468293e-05,
      "loss": 6.0829,
      "step": 22280
    },
    {
      "epoch": 3.939233350998057,
      "grad_norm": 5.311077117919922,
      "learning_rate": 1.060943296237414e-05,
      "loss": 6.0388,
      "step": 22300
    },
    {
      "epoch": 3.942766295707472,
      "grad_norm": 7.83790922164917,
      "learning_rate": 1.0574103515279985e-05,
      "loss": 5.9902,
      "step": 22320
    },
    {
      "epoch": 3.9462992404168875,
      "grad_norm": 6.628385066986084,
      "learning_rate": 1.0538774068185832e-05,
      "loss": 5.9788,
      "step": 22340
    },
    {
      "epoch": 3.9498321851263025,
      "grad_norm": 5.152470588684082,
      "learning_rate": 1.0503444621091681e-05,
      "loss": 6.0012,
      "step": 22360
    },
    {
      "epoch": 3.953365129835718,
      "grad_norm": 4.825316905975342,
      "learning_rate": 1.0468115173997528e-05,
      "loss": 5.9581,
      "step": 22380
    },
    {
      "epoch": 3.9568980745451334,
      "grad_norm": 5.195298194885254,
      "learning_rate": 1.0432785726903374e-05,
      "loss": 6.1332,
      "step": 22400
    },
    {
      "epoch": 3.960431019254549,
      "grad_norm": 8.24953842163086,
      "learning_rate": 1.0397456279809221e-05,
      "loss": 6.0418,
      "step": 22420
    },
    {
      "epoch": 3.963963963963964,
      "grad_norm": 6.779095649719238,
      "learning_rate": 1.036212683271507e-05,
      "loss": 6.0567,
      "step": 22440
    },
    {
      "epoch": 3.9674969086733793,
      "grad_norm": 6.224007606506348,
      "learning_rate": 1.0326797385620915e-05,
      "loss": 6.2112,
      "step": 22460
    },
    {
      "epoch": 3.9710298533827944,
      "grad_norm": 6.692031383514404,
      "learning_rate": 1.0291467938526762e-05,
      "loss": 6.0371,
      "step": 22480
    },
    {
      "epoch": 3.97456279809221,
      "grad_norm": 6.500868320465088,
      "learning_rate": 1.025613849143261e-05,
      "loss": 6.1838,
      "step": 22500
    },
    {
      "epoch": 3.9780957428016253,
      "grad_norm": 6.715456485748291,
      "learning_rate": 1.0220809044338456e-05,
      "loss": 6.0209,
      "step": 22520
    },
    {
      "epoch": 3.9816286875110407,
      "grad_norm": 5.301516056060791,
      "learning_rate": 1.0185479597244304e-05,
      "loss": 6.0066,
      "step": 22540
    },
    {
      "epoch": 3.9851616322204557,
      "grad_norm": 5.512056827545166,
      "learning_rate": 1.015015015015015e-05,
      "loss": 6.11,
      "step": 22560
    },
    {
      "epoch": 3.988694576929871,
      "grad_norm": 5.959190845489502,
      "learning_rate": 1.0114820703055998e-05,
      "loss": 6.1413,
      "step": 22580
    },
    {
      "epoch": 3.992227521639286,
      "grad_norm": 7.202030658721924,
      "learning_rate": 1.0079491255961845e-05,
      "loss": 5.8984,
      "step": 22600
    },
    {
      "epoch": 3.9957604663487016,
      "grad_norm": 7.77725076675415,
      "learning_rate": 1.0044161808867692e-05,
      "loss": 5.9719,
      "step": 22620
    },
    {
      "epoch": 3.999293411058117,
      "grad_norm": 6.500852584838867,
      "learning_rate": 1.0008832361773539e-05,
      "loss": 6.0739,
      "step": 22640
    },
    {
      "epoch": 4.0028263557675325,
      "grad_norm": 8.718032836914062,
      "learning_rate": 9.973502914679386e-06,
      "loss": 5.9379,
      "step": 22660
    },
    {
      "epoch": 4.006359300476947,
      "grad_norm": 5.573793411254883,
      "learning_rate": 9.938173467585232e-06,
      "loss": 6.062,
      "step": 22680
    },
    {
      "epoch": 4.009892245186363,
      "grad_norm": 6.1545257568359375,
      "learning_rate": 9.90284402049108e-06,
      "loss": 6.0323,
      "step": 22700
    },
    {
      "epoch": 4.013425189895778,
      "grad_norm": 5.6928629875183105,
      "learning_rate": 9.867514573396927e-06,
      "loss": 6.0031,
      "step": 22720
    },
    {
      "epoch": 4.0169581346051935,
      "grad_norm": 6.755130767822266,
      "learning_rate": 9.832185126302775e-06,
      "loss": 5.9479,
      "step": 22740
    },
    {
      "epoch": 4.020491079314609,
      "grad_norm": 6.221550464630127,
      "learning_rate": 9.79685567920862e-06,
      "loss": 6.0084,
      "step": 22760
    },
    {
      "epoch": 4.024024024024024,
      "grad_norm": 4.70980167388916,
      "learning_rate": 9.761526232114467e-06,
      "loss": 6.082,
      "step": 22780
    },
    {
      "epoch": 4.027556968733439,
      "grad_norm": 4.692917346954346,
      "learning_rate": 9.726196785020316e-06,
      "loss": 6.0161,
      "step": 22800
    },
    {
      "epoch": 4.031089913442854,
      "grad_norm": 6.436196804046631,
      "learning_rate": 9.690867337926161e-06,
      "loss": 6.149,
      "step": 22820
    },
    {
      "epoch": 4.03462285815227,
      "grad_norm": 5.428383827209473,
      "learning_rate": 9.655537890832008e-06,
      "loss": 5.9127,
      "step": 22840
    },
    {
      "epoch": 4.038155802861685,
      "grad_norm": 6.508072853088379,
      "learning_rate": 9.620208443737856e-06,
      "loss": 5.9942,
      "step": 22860
    },
    {
      "epoch": 4.041688747571101,
      "grad_norm": 5.369599342346191,
      "learning_rate": 9.584878996643704e-06,
      "loss": 6.0227,
      "step": 22880
    },
    {
      "epoch": 4.045221692280516,
      "grad_norm": 4.340018272399902,
      "learning_rate": 9.54954954954955e-06,
      "loss": 5.9734,
      "step": 22900
    },
    {
      "epoch": 4.048754636989931,
      "grad_norm": 5.744077205657959,
      "learning_rate": 9.514220102455397e-06,
      "loss": 6.0809,
      "step": 22920
    },
    {
      "epoch": 4.052287581699346,
      "grad_norm": 5.004657745361328,
      "learning_rate": 9.478890655361244e-06,
      "loss": 6.1084,
      "step": 22940
    },
    {
      "epoch": 4.055820526408762,
      "grad_norm": 4.859432697296143,
      "learning_rate": 9.443561208267091e-06,
      "loss": 5.855,
      "step": 22960
    },
    {
      "epoch": 4.059353471118177,
      "grad_norm": 7.075984477996826,
      "learning_rate": 9.408231761172938e-06,
      "loss": 6.1149,
      "step": 22980
    },
    {
      "epoch": 4.062886415827593,
      "grad_norm": 4.557109832763672,
      "learning_rate": 9.372902314078785e-06,
      "loss": 5.9769,
      "step": 23000
    },
    {
      "epoch": 4.066419360537008,
      "grad_norm": 4.764817237854004,
      "learning_rate": 9.337572866984632e-06,
      "loss": 5.9989,
      "step": 23020
    },
    {
      "epoch": 4.069952305246423,
      "grad_norm": 6.659342288970947,
      "learning_rate": 9.30224341989048e-06,
      "loss": 5.9753,
      "step": 23040
    },
    {
      "epoch": 4.073485249955838,
      "grad_norm": 6.400298595428467,
      "learning_rate": 9.266913972796327e-06,
      "loss": 5.9706,
      "step": 23060
    },
    {
      "epoch": 4.0770181946652535,
      "grad_norm": 6.875054836273193,
      "learning_rate": 9.231584525702174e-06,
      "loss": 5.851,
      "step": 23080
    },
    {
      "epoch": 4.080551139374669,
      "grad_norm": 6.16037654876709,
      "learning_rate": 9.196255078608019e-06,
      "loss": 5.9846,
      "step": 23100
    },
    {
      "epoch": 4.084084084084084,
      "grad_norm": 7.5934014320373535,
      "learning_rate": 9.160925631513866e-06,
      "loss": 5.9264,
      "step": 23120
    },
    {
      "epoch": 4.087617028793499,
      "grad_norm": 5.316821098327637,
      "learning_rate": 9.125596184419715e-06,
      "loss": 6.1893,
      "step": 23140
    },
    {
      "epoch": 4.091149973502914,
      "grad_norm": 7.71054744720459,
      "learning_rate": 9.090266737325562e-06,
      "loss": 6.0792,
      "step": 23160
    },
    {
      "epoch": 4.09468291821233,
      "grad_norm": 5.72298002243042,
      "learning_rate": 9.054937290231408e-06,
      "loss": 6.1504,
      "step": 23180
    },
    {
      "epoch": 4.098215862921745,
      "grad_norm": 6.42032527923584,
      "learning_rate": 9.019607843137255e-06,
      "loss": 6.0506,
      "step": 23200
    },
    {
      "epoch": 4.101748807631161,
      "grad_norm": 7.629870891571045,
      "learning_rate": 8.984278396043102e-06,
      "loss": 6.1745,
      "step": 23220
    },
    {
      "epoch": 4.105281752340576,
      "grad_norm": 4.893223285675049,
      "learning_rate": 8.94894894894895e-06,
      "loss": 6.0795,
      "step": 23240
    },
    {
      "epoch": 4.108814697049991,
      "grad_norm": 4.9797258377075195,
      "learning_rate": 8.913619501854796e-06,
      "loss": 6.1485,
      "step": 23260
    },
    {
      "epoch": 4.112347641759406,
      "grad_norm": 8.73027515411377,
      "learning_rate": 8.878290054760643e-06,
      "loss": 6.1213,
      "step": 23280
    },
    {
      "epoch": 4.115880586468822,
      "grad_norm": 6.313558578491211,
      "learning_rate": 8.84296060766649e-06,
      "loss": 6.142,
      "step": 23300
    },
    {
      "epoch": 4.119413531178237,
      "grad_norm": 5.607224464416504,
      "learning_rate": 8.807631160572337e-06,
      "loss": 6.0763,
      "step": 23320
    },
    {
      "epoch": 4.122946475887653,
      "grad_norm": 4.429167747497559,
      "learning_rate": 8.772301713478184e-06,
      "loss": 6.172,
      "step": 23340
    },
    {
      "epoch": 4.126479420597068,
      "grad_norm": 3.734477996826172,
      "learning_rate": 8.736972266384032e-06,
      "loss": 5.9888,
      "step": 23360
    },
    {
      "epoch": 4.130012365306483,
      "grad_norm": 6.6058878898620605,
      "learning_rate": 8.701642819289879e-06,
      "loss": 5.9776,
      "step": 23380
    },
    {
      "epoch": 4.133545310015898,
      "grad_norm": 4.614311218261719,
      "learning_rate": 8.666313372195726e-06,
      "loss": 6.0153,
      "step": 23400
    },
    {
      "epoch": 4.1370782547253135,
      "grad_norm": 4.909837245941162,
      "learning_rate": 8.630983925101573e-06,
      "loss": 6.118,
      "step": 23420
    },
    {
      "epoch": 4.140611199434729,
      "grad_norm": 5.930766582489014,
      "learning_rate": 8.59565447800742e-06,
      "loss": 5.946,
      "step": 23440
    },
    {
      "epoch": 4.1441441441441444,
      "grad_norm": 6.942525386810303,
      "learning_rate": 8.560325030913265e-06,
      "loss": 6.1775,
      "step": 23460
    },
    {
      "epoch": 4.147677088853559,
      "grad_norm": 6.413014888763428,
      "learning_rate": 8.524995583819114e-06,
      "loss": 5.9775,
      "step": 23480
    },
    {
      "epoch": 4.1512100335629745,
      "grad_norm": 5.595881462097168,
      "learning_rate": 8.489666136724961e-06,
      "loss": 6.0972,
      "step": 23500
    },
    {
      "epoch": 4.15474297827239,
      "grad_norm": 5.006204128265381,
      "learning_rate": 8.454336689630808e-06,
      "loss": 6.0776,
      "step": 23520
    },
    {
      "epoch": 4.158275922981805,
      "grad_norm": 5.168039321899414,
      "learning_rate": 8.419007242536654e-06,
      "loss": 5.9735,
      "step": 23540
    },
    {
      "epoch": 4.161808867691221,
      "grad_norm": 5.11462926864624,
      "learning_rate": 8.383677795442501e-06,
      "loss": 6.1377,
      "step": 23560
    },
    {
      "epoch": 4.165341812400636,
      "grad_norm": 6.313705921173096,
      "learning_rate": 8.34834834834835e-06,
      "loss": 5.9638,
      "step": 23580
    },
    {
      "epoch": 4.168874757110051,
      "grad_norm": 6.574459075927734,
      "learning_rate": 8.313018901254195e-06,
      "loss": 5.8702,
      "step": 23600
    },
    {
      "epoch": 4.172407701819466,
      "grad_norm": 7.475161075592041,
      "learning_rate": 8.277689454160042e-06,
      "loss": 6.0455,
      "step": 23620
    },
    {
      "epoch": 4.175940646528882,
      "grad_norm": 6.018412113189697,
      "learning_rate": 8.24236000706589e-06,
      "loss": 6.1365,
      "step": 23640
    },
    {
      "epoch": 4.179473591238297,
      "grad_norm": 5.459251880645752,
      "learning_rate": 8.207030559971736e-06,
      "loss": 6.1426,
      "step": 23660
    },
    {
      "epoch": 4.183006535947713,
      "grad_norm": 7.53600549697876,
      "learning_rate": 8.171701112877584e-06,
      "loss": 6.1137,
      "step": 23680
    },
    {
      "epoch": 4.186539480657128,
      "grad_norm": 4.171685218811035,
      "learning_rate": 8.13637166578343e-06,
      "loss": 6.1691,
      "step": 23700
    },
    {
      "epoch": 4.190072425366543,
      "grad_norm": 6.400804042816162,
      "learning_rate": 8.101042218689278e-06,
      "loss": 5.8536,
      "step": 23720
    },
    {
      "epoch": 4.193605370075958,
      "grad_norm": 4.0488481521606445,
      "learning_rate": 8.065712771595125e-06,
      "loss": 6.0029,
      "step": 23740
    },
    {
      "epoch": 4.197138314785374,
      "grad_norm": 4.398076057434082,
      "learning_rate": 8.030383324500972e-06,
      "loss": 6.0768,
      "step": 23760
    },
    {
      "epoch": 4.200671259494789,
      "grad_norm": 4.5628981590271,
      "learning_rate": 7.995053877406819e-06,
      "loss": 6.0115,
      "step": 23780
    },
    {
      "epoch": 4.2042042042042045,
      "grad_norm": 5.759737491607666,
      "learning_rate": 7.959724430312666e-06,
      "loss": 5.9745,
      "step": 23800
    },
    {
      "epoch": 4.207737148913619,
      "grad_norm": 5.803333282470703,
      "learning_rate": 7.924394983218512e-06,
      "loss": 6.0609,
      "step": 23820
    },
    {
      "epoch": 4.2112700936230345,
      "grad_norm": 8.272336959838867,
      "learning_rate": 7.88906553612436e-06,
      "loss": 6.0287,
      "step": 23840
    },
    {
      "epoch": 4.21480303833245,
      "grad_norm": 4.987042427062988,
      "learning_rate": 7.853736089030208e-06,
      "loss": 5.9875,
      "step": 23860
    },
    {
      "epoch": 4.218335983041865,
      "grad_norm": 6.827054023742676,
      "learning_rate": 7.818406641936055e-06,
      "loss": 6.0857,
      "step": 23880
    },
    {
      "epoch": 4.221868927751281,
      "grad_norm": 6.259749412536621,
      "learning_rate": 7.7830771948419e-06,
      "loss": 6.0967,
      "step": 23900
    },
    {
      "epoch": 4.225401872460696,
      "grad_norm": 4.372320652008057,
      "learning_rate": 7.747747747747749e-06,
      "loss": 6.2138,
      "step": 23920
    },
    {
      "epoch": 4.228934817170111,
      "grad_norm": 6.521145820617676,
      "learning_rate": 7.712418300653596e-06,
      "loss": 6.1033,
      "step": 23940
    },
    {
      "epoch": 4.232467761879526,
      "grad_norm": 6.621304512023926,
      "learning_rate": 7.677088853559441e-06,
      "loss": 6.0884,
      "step": 23960
    },
    {
      "epoch": 4.236000706588942,
      "grad_norm": 6.416168212890625,
      "learning_rate": 7.641759406465288e-06,
      "loss": 6.0276,
      "step": 23980
    },
    {
      "epoch": 4.239533651298357,
      "grad_norm": 4.786006450653076,
      "learning_rate": 7.6064299593711365e-06,
      "loss": 6.1477,
      "step": 24000
    },
    {
      "epoch": 4.243066596007773,
      "grad_norm": 6.807433128356934,
      "learning_rate": 7.5711005122769836e-06,
      "loss": 6.077,
      "step": 24020
    },
    {
      "epoch": 4.246599540717188,
      "grad_norm": 4.3355302810668945,
      "learning_rate": 7.53577106518283e-06,
      "loss": 6.0885,
      "step": 24040
    },
    {
      "epoch": 4.250132485426603,
      "grad_norm": 5.7077555656433105,
      "learning_rate": 7.500441618088677e-06,
      "loss": 5.9386,
      "step": 24060
    },
    {
      "epoch": 4.253665430136018,
      "grad_norm": 4.737129211425781,
      "learning_rate": 7.465112170994525e-06,
      "loss": 6.0638,
      "step": 24080
    },
    {
      "epoch": 4.257198374845434,
      "grad_norm": 7.336609363555908,
      "learning_rate": 7.429782723900372e-06,
      "loss": 6.0057,
      "step": 24100
    },
    {
      "epoch": 4.260731319554849,
      "grad_norm": 4.823758125305176,
      "learning_rate": 7.394453276806218e-06,
      "loss": 5.8974,
      "step": 24120
    },
    {
      "epoch": 4.2642642642642645,
      "grad_norm": 8.38454818725586,
      "learning_rate": 7.359123829712065e-06,
      "loss": 5.9967,
      "step": 24140
    },
    {
      "epoch": 4.267797208973679,
      "grad_norm": 5.859616756439209,
      "learning_rate": 7.3237943826179125e-06,
      "loss": 6.1964,
      "step": 24160
    },
    {
      "epoch": 4.2713301536830945,
      "grad_norm": 4.714649200439453,
      "learning_rate": 7.288464935523759e-06,
      "loss": 5.883,
      "step": 24180
    },
    {
      "epoch": 4.27486309839251,
      "grad_norm": 4.439967632293701,
      "learning_rate": 7.253135488429607e-06,
      "loss": 6.0804,
      "step": 24200
    },
    {
      "epoch": 4.2783960431019255,
      "grad_norm": 5.719338417053223,
      "learning_rate": 7.217806041335454e-06,
      "loss": 6.0387,
      "step": 24220
    },
    {
      "epoch": 4.281928987811341,
      "grad_norm": 7.939112663269043,
      "learning_rate": 7.182476594241301e-06,
      "loss": 6.0721,
      "step": 24240
    },
    {
      "epoch": 4.285461932520756,
      "grad_norm": 6.837405204772949,
      "learning_rate": 7.147147147147147e-06,
      "loss": 6.0242,
      "step": 24260
    },
    {
      "epoch": 4.288994877230172,
      "grad_norm": 5.028409481048584,
      "learning_rate": 7.111817700052994e-06,
      "loss": 6.041,
      "step": 24280
    },
    {
      "epoch": 4.292527821939586,
      "grad_norm": 7.206142902374268,
      "learning_rate": 7.076488252958842e-06,
      "loss": 5.9253,
      "step": 24300
    },
    {
      "epoch": 4.296060766649002,
      "grad_norm": 7.197932243347168,
      "learning_rate": 7.041158805864688e-06,
      "loss": 5.9413,
      "step": 24320
    },
    {
      "epoch": 4.299593711358417,
      "grad_norm": 6.074570655822754,
      "learning_rate": 7.005829358770536e-06,
      "loss": 6.0093,
      "step": 24340
    },
    {
      "epoch": 4.303126656067833,
      "grad_norm": 6.833858966827393,
      "learning_rate": 6.970499911676383e-06,
      "loss": 6.1977,
      "step": 24360
    },
    {
      "epoch": 4.306659600777248,
      "grad_norm": 5.416473388671875,
      "learning_rate": 6.93517046458223e-06,
      "loss": 6.0863,
      "step": 24380
    },
    {
      "epoch": 4.310192545486663,
      "grad_norm": 10.091863632202148,
      "learning_rate": 6.899841017488076e-06,
      "loss": 6.0956,
      "step": 24400
    },
    {
      "epoch": 4.313725490196078,
      "grad_norm": 4.627683162689209,
      "learning_rate": 6.864511570393924e-06,
      "loss": 6.0312,
      "step": 24420
    },
    {
      "epoch": 4.317258434905494,
      "grad_norm": 5.649838924407959,
      "learning_rate": 6.829182123299771e-06,
      "loss": 6.0261,
      "step": 24440
    },
    {
      "epoch": 4.320791379614909,
      "grad_norm": 6.53814697265625,
      "learning_rate": 6.793852676205617e-06,
      "loss": 5.8869,
      "step": 24460
    },
    {
      "epoch": 4.324324324324325,
      "grad_norm": 4.626598834991455,
      "learning_rate": 6.7585232291114645e-06,
      "loss": 5.9906,
      "step": 24480
    },
    {
      "epoch": 4.32785726903374,
      "grad_norm": 6.380804061889648,
      "learning_rate": 6.723193782017312e-06,
      "loss": 6.0747,
      "step": 24500
    },
    {
      "epoch": 4.331390213743155,
      "grad_norm": 5.670560836791992,
      "learning_rate": 6.6878643349231596e-06,
      "loss": 5.9841,
      "step": 24520
    },
    {
      "epoch": 4.33492315845257,
      "grad_norm": 5.048520088195801,
      "learning_rate": 6.652534887829005e-06,
      "loss": 6.0066,
      "step": 24540
    },
    {
      "epoch": 4.3384561031619855,
      "grad_norm": 6.134765148162842,
      "learning_rate": 6.617205440734853e-06,
      "loss": 6.1602,
      "step": 24560
    },
    {
      "epoch": 4.341989047871401,
      "grad_norm": 4.936121463775635,
      "learning_rate": 6.5818759936407e-06,
      "loss": 5.9828,
      "step": 24580
    },
    {
      "epoch": 4.345521992580816,
      "grad_norm": 4.81895112991333,
      "learning_rate": 6.546546546546547e-06,
      "loss": 5.9468,
      "step": 24600
    },
    {
      "epoch": 4.349054937290232,
      "grad_norm": 8.430758476257324,
      "learning_rate": 6.511217099452393e-06,
      "loss": 6.0333,
      "step": 24620
    },
    {
      "epoch": 4.352587881999646,
      "grad_norm": 5.1412153244018555,
      "learning_rate": 6.475887652358241e-06,
      "loss": 6.1073,
      "step": 24640
    },
    {
      "epoch": 4.356120826709062,
      "grad_norm": 5.53141975402832,
      "learning_rate": 6.4405582052640885e-06,
      "loss": 6.1145,
      "step": 24660
    },
    {
      "epoch": 4.359653771418477,
      "grad_norm": 7.321264743804932,
      "learning_rate": 6.405228758169935e-06,
      "loss": 6.1183,
      "step": 24680
    },
    {
      "epoch": 4.363186716127893,
      "grad_norm": 7.7475810050964355,
      "learning_rate": 6.369899311075782e-06,
      "loss": 5.9491,
      "step": 24700
    },
    {
      "epoch": 4.366719660837308,
      "grad_norm": 7.780623435974121,
      "learning_rate": 6.334569863981629e-06,
      "loss": 6.0546,
      "step": 24720
    },
    {
      "epoch": 4.370252605546723,
      "grad_norm": 6.536038875579834,
      "learning_rate": 6.299240416887477e-06,
      "loss": 5.9597,
      "step": 24740
    },
    {
      "epoch": 4.373785550256138,
      "grad_norm": 5.392999649047852,
      "learning_rate": 6.263910969793322e-06,
      "loss": 6.0645,
      "step": 24760
    },
    {
      "epoch": 4.377318494965554,
      "grad_norm": 8.820524215698242,
      "learning_rate": 6.22858152269917e-06,
      "loss": 6.0445,
      "step": 24780
    },
    {
      "epoch": 4.380851439674969,
      "grad_norm": 4.778463363647461,
      "learning_rate": 6.1932520756050165e-06,
      "loss": 5.9677,
      "step": 24800
    },
    {
      "epoch": 4.384384384384385,
      "grad_norm": 5.302971839904785,
      "learning_rate": 6.1579226285108645e-06,
      "loss": 5.9628,
      "step": 24820
    },
    {
      "epoch": 4.3879173290938,
      "grad_norm": 5.469659328460693,
      "learning_rate": 6.122593181416711e-06,
      "loss": 6.0729,
      "step": 24840
    },
    {
      "epoch": 4.391450273803215,
      "grad_norm": 6.50875997543335,
      "learning_rate": 6.087263734322559e-06,
      "loss": 6.0821,
      "step": 24860
    },
    {
      "epoch": 4.39498321851263,
      "grad_norm": 6.490943908691406,
      "learning_rate": 6.051934287228405e-06,
      "loss": 6.074,
      "step": 24880
    },
    {
      "epoch": 4.3985161632220455,
      "grad_norm": 5.148245811462402,
      "learning_rate": 6.016604840134252e-06,
      "loss": 6.0785,
      "step": 24900
    },
    {
      "epoch": 4.402049107931461,
      "grad_norm": 6.802369117736816,
      "learning_rate": 5.981275393040099e-06,
      "loss": 6.0375,
      "step": 24920
    },
    {
      "epoch": 4.405582052640876,
      "grad_norm": 5.505249977111816,
      "learning_rate": 5.945945945945946e-06,
      "loss": 6.2048,
      "step": 24940
    },
    {
      "epoch": 4.409114997350292,
      "grad_norm": 4.7757439613342285,
      "learning_rate": 5.910616498851793e-06,
      "loss": 6.0497,
      "step": 24960
    },
    {
      "epoch": 4.4126479420597065,
      "grad_norm": 6.446849822998047,
      "learning_rate": 5.87528705175764e-06,
      "loss": 6.057,
      "step": 24980
    },
    {
      "epoch": 4.416180886769122,
      "grad_norm": 5.169592380523682,
      "learning_rate": 5.839957604663488e-06,
      "loss": 5.9853,
      "step": 25000
    },
    {
      "epoch": 4.419713831478537,
      "grad_norm": 8.395010948181152,
      "learning_rate": 5.804628157569334e-06,
      "loss": 6.1053,
      "step": 25020
    },
    {
      "epoch": 4.423246776187953,
      "grad_norm": 5.52846097946167,
      "learning_rate": 5.769298710475182e-06,
      "loss": 6.0895,
      "step": 25040
    },
    {
      "epoch": 4.426779720897368,
      "grad_norm": 5.279209136962891,
      "learning_rate": 5.733969263381028e-06,
      "loss": 6.1413,
      "step": 25060
    },
    {
      "epoch": 4.430312665606783,
      "grad_norm": 4.659432411193848,
      "learning_rate": 5.698639816286876e-06,
      "loss": 6.0978,
      "step": 25080
    },
    {
      "epoch": 4.433845610316198,
      "grad_norm": 7.236688613891602,
      "learning_rate": 5.663310369192722e-06,
      "loss": 6.1546,
      "step": 25100
    },
    {
      "epoch": 4.437378555025614,
      "grad_norm": 6.577533721923828,
      "learning_rate": 5.627980922098569e-06,
      "loss": 6.1209,
      "step": 25120
    },
    {
      "epoch": 4.440911499735029,
      "grad_norm": 5.547929286956787,
      "learning_rate": 5.5926514750044165e-06,
      "loss": 5.9259,
      "step": 25140
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 5.288700580596924,
      "learning_rate": 5.557322027910264e-06,
      "loss": 6.0768,
      "step": 25160
    },
    {
      "epoch": 4.44797738915386,
      "grad_norm": 7.4870758056640625,
      "learning_rate": 5.521992580816111e-06,
      "loss": 6.044,
      "step": 25180
    },
    {
      "epoch": 4.451510333863275,
      "grad_norm": 6.1927170753479,
      "learning_rate": 5.486663133721957e-06,
      "loss": 6.0424,
      "step": 25200
    },
    {
      "epoch": 4.45504327857269,
      "grad_norm": 5.611730575561523,
      "learning_rate": 5.451333686627805e-06,
      "loss": 6.1159,
      "step": 25220
    },
    {
      "epoch": 4.458576223282106,
      "grad_norm": 4.117532253265381,
      "learning_rate": 5.416004239533651e-06,
      "loss": 6.1731,
      "step": 25240
    },
    {
      "epoch": 4.462109167991521,
      "grad_norm": 7.020809650421143,
      "learning_rate": 5.380674792439499e-06,
      "loss": 6.0145,
      "step": 25260
    },
    {
      "epoch": 4.4656421127009365,
      "grad_norm": 9.232316970825195,
      "learning_rate": 5.3453453453453454e-06,
      "loss": 5.9497,
      "step": 25280
    },
    {
      "epoch": 4.469175057410352,
      "grad_norm": 6.866179943084717,
      "learning_rate": 5.3100158982511925e-06,
      "loss": 6.0418,
      "step": 25300
    },
    {
      "epoch": 4.4727080021197665,
      "grad_norm": 6.265938758850098,
      "learning_rate": 5.27468645115704e-06,
      "loss": 6.103,
      "step": 25320
    },
    {
      "epoch": 4.476240946829182,
      "grad_norm": 5.092808723449707,
      "learning_rate": 5.239357004062887e-06,
      "loss": 5.9228,
      "step": 25340
    },
    {
      "epoch": 4.479773891538597,
      "grad_norm": 5.527928829193115,
      "learning_rate": 5.204027556968734e-06,
      "loss": 6.0161,
      "step": 25360
    },
    {
      "epoch": 4.483306836248013,
      "grad_norm": 4.411657333374023,
      "learning_rate": 5.168698109874581e-06,
      "loss": 5.9703,
      "step": 25380
    },
    {
      "epoch": 4.486839780957428,
      "grad_norm": 5.205930233001709,
      "learning_rate": 5.133368662780428e-06,
      "loss": 6.1135,
      "step": 25400
    },
    {
      "epoch": 4.490372725666843,
      "grad_norm": 6.549343585968018,
      "learning_rate": 5.098039215686274e-06,
      "loss": 5.9593,
      "step": 25420
    },
    {
      "epoch": 4.493905670376258,
      "grad_norm": 5.936781406402588,
      "learning_rate": 5.062709768592122e-06,
      "loss": 6.1009,
      "step": 25440
    },
    {
      "epoch": 4.497438615085674,
      "grad_norm": 5.753017902374268,
      "learning_rate": 5.0273803214979686e-06,
      "loss": 6.0784,
      "step": 25460
    },
    {
      "epoch": 4.500971559795089,
      "grad_norm": 6.166106700897217,
      "learning_rate": 4.992050874403816e-06,
      "loss": 5.9605,
      "step": 25480
    },
    {
      "epoch": 4.504504504504505,
      "grad_norm": 5.5917439460754395,
      "learning_rate": 4.956721427309663e-06,
      "loss": 6.1301,
      "step": 25500
    },
    {
      "epoch": 4.50803744921392,
      "grad_norm": 5.8387346267700195,
      "learning_rate": 4.92139198021551e-06,
      "loss": 6.133,
      "step": 25520
    },
    {
      "epoch": 4.511570393923336,
      "grad_norm": 5.358598709106445,
      "learning_rate": 4.886062533121357e-06,
      "loss": 6.2023,
      "step": 25540
    },
    {
      "epoch": 4.51510333863275,
      "grad_norm": 6.31521463394165,
      "learning_rate": 4.850733086027204e-06,
      "loss": 5.8729,
      "step": 25560
    },
    {
      "epoch": 4.518636283342166,
      "grad_norm": 6.1791863441467285,
      "learning_rate": 4.815403638933051e-06,
      "loss": 6.0421,
      "step": 25580
    },
    {
      "epoch": 4.522169228051581,
      "grad_norm": 7.0920915603637695,
      "learning_rate": 4.780074191838898e-06,
      "loss": 5.9992,
      "step": 25600
    },
    {
      "epoch": 4.5257021727609965,
      "grad_norm": 4.91792106628418,
      "learning_rate": 4.7447447447447454e-06,
      "loss": 5.9404,
      "step": 25620
    },
    {
      "epoch": 4.529235117470412,
      "grad_norm": 5.932011604309082,
      "learning_rate": 4.709415297650592e-06,
      "loss": 5.8939,
      "step": 25640
    },
    {
      "epoch": 4.5327680621798265,
      "grad_norm": 4.618721008300781,
      "learning_rate": 4.674085850556439e-06,
      "loss": 6.1292,
      "step": 25660
    },
    {
      "epoch": 4.536301006889242,
      "grad_norm": 5.7934250831604,
      "learning_rate": 4.638756403462286e-06,
      "loss": 6.0324,
      "step": 25680
    },
    {
      "epoch": 4.539833951598657,
      "grad_norm": 7.590107440948486,
      "learning_rate": 4.603426956368133e-06,
      "loss": 5.8666,
      "step": 25700
    },
    {
      "epoch": 4.543366896308073,
      "grad_norm": 5.450039386749268,
      "learning_rate": 4.56809750927398e-06,
      "loss": 5.9496,
      "step": 25720
    },
    {
      "epoch": 4.546899841017488,
      "grad_norm": 8.410272598266602,
      "learning_rate": 4.532768062179827e-06,
      "loss": 6.0252,
      "step": 25740
    },
    {
      "epoch": 4.550432785726903,
      "grad_norm": 6.737366676330566,
      "learning_rate": 4.497438615085674e-06,
      "loss": 6.0654,
      "step": 25760
    },
    {
      "epoch": 4.553965730436318,
      "grad_norm": 7.0563154220581055,
      "learning_rate": 4.4621091679915214e-06,
      "loss": 6.1848,
      "step": 25780
    },
    {
      "epoch": 4.557498675145734,
      "grad_norm": 4.390495777130127,
      "learning_rate": 4.426779720897368e-06,
      "loss": 5.9651,
      "step": 25800
    },
    {
      "epoch": 4.561031619855149,
      "grad_norm": 6.1921305656433105,
      "learning_rate": 4.391450273803216e-06,
      "loss": 5.98,
      "step": 25820
    },
    {
      "epoch": 4.564564564564565,
      "grad_norm": 5.6434245109558105,
      "learning_rate": 4.356120826709062e-06,
      "loss": 5.9262,
      "step": 25840
    },
    {
      "epoch": 4.56809750927398,
      "grad_norm": 5.562051773071289,
      "learning_rate": 4.320791379614909e-06,
      "loss": 5.9761,
      "step": 25860
    },
    {
      "epoch": 4.571630453983396,
      "grad_norm": 5.19767951965332,
      "learning_rate": 4.285461932520756e-06,
      "loss": 5.9778,
      "step": 25880
    },
    {
      "epoch": 4.57516339869281,
      "grad_norm": 7.84989070892334,
      "learning_rate": 4.250132485426603e-06,
      "loss": 6.0304,
      "step": 25900
    },
    {
      "epoch": 4.578696343402226,
      "grad_norm": 6.797354221343994,
      "learning_rate": 4.21480303833245e-06,
      "loss": 6.0625,
      "step": 25920
    },
    {
      "epoch": 4.582229288111641,
      "grad_norm": 5.714794635772705,
      "learning_rate": 4.1794735912382975e-06,
      "loss": 6.0472,
      "step": 25940
    },
    {
      "epoch": 4.5857622328210565,
      "grad_norm": 4.178284168243408,
      "learning_rate": 4.1441441441441446e-06,
      "loss": 6.1534,
      "step": 25960
    },
    {
      "epoch": 4.589295177530472,
      "grad_norm": 5.272725582122803,
      "learning_rate": 4.108814697049991e-06,
      "loss": 6.0129,
      "step": 25980
    },
    {
      "epoch": 4.592828122239887,
      "grad_norm": 4.4520769119262695,
      "learning_rate": 4.073485249955839e-06,
      "loss": 6.07,
      "step": 26000
    },
    {
      "epoch": 4.596361066949302,
      "grad_norm": 6.020585536956787,
      "learning_rate": 4.038155802861685e-06,
      "loss": 5.9772,
      "step": 26020
    },
    {
      "epoch": 4.5998940116587175,
      "grad_norm": 3.9503540992736816,
      "learning_rate": 4.002826355767533e-06,
      "loss": 6.0183,
      "step": 26040
    },
    {
      "epoch": 4.603426956368133,
      "grad_norm": 4.614776611328125,
      "learning_rate": 3.967496908673379e-06,
      "loss": 5.996,
      "step": 26060
    },
    {
      "epoch": 4.606959901077548,
      "grad_norm": 8.489394187927246,
      "learning_rate": 3.932167461579226e-06,
      "loss": 6.033,
      "step": 26080
    },
    {
      "epoch": 4.610492845786963,
      "grad_norm": 5.826636791229248,
      "learning_rate": 3.8968380144850735e-06,
      "loss": 5.9653,
      "step": 26100
    },
    {
      "epoch": 4.614025790496378,
      "grad_norm": 6.587015628814697,
      "learning_rate": 3.861508567390921e-06,
      "loss": 5.9727,
      "step": 26120
    },
    {
      "epoch": 4.617558735205794,
      "grad_norm": 5.7291154861450195,
      "learning_rate": 3.826179120296768e-06,
      "loss": 5.9653,
      "step": 26140
    },
    {
      "epoch": 4.621091679915209,
      "grad_norm": 6.752956390380859,
      "learning_rate": 3.7908496732026144e-06,
      "loss": 6.1028,
      "step": 26160
    },
    {
      "epoch": 4.624624624624625,
      "grad_norm": 4.364904403686523,
      "learning_rate": 3.7555202261084615e-06,
      "loss": 6.0953,
      "step": 26180
    },
    {
      "epoch": 4.62815756933404,
      "grad_norm": 6.288025379180908,
      "learning_rate": 3.7201907790143086e-06,
      "loss": 6.0525,
      "step": 26200
    },
    {
      "epoch": 4.631690514043456,
      "grad_norm": 4.844313621520996,
      "learning_rate": 3.6848613319201557e-06,
      "loss": 6.2321,
      "step": 26220
    },
    {
      "epoch": 4.63522345875287,
      "grad_norm": 7.598246097564697,
      "learning_rate": 3.6495318848260024e-06,
      "loss": 5.9563,
      "step": 26240
    },
    {
      "epoch": 4.638756403462286,
      "grad_norm": 7.003003120422363,
      "learning_rate": 3.61420243773185e-06,
      "loss": 6.105,
      "step": 26260
    },
    {
      "epoch": 4.642289348171701,
      "grad_norm": 5.066927433013916,
      "learning_rate": 3.5788729906376966e-06,
      "loss": 5.9478,
      "step": 26280
    },
    {
      "epoch": 4.645822292881117,
      "grad_norm": 5.470005035400391,
      "learning_rate": 3.5435435435435433e-06,
      "loss": 6.0031,
      "step": 26300
    },
    {
      "epoch": 4.649355237590532,
      "grad_norm": 6.9858293533325195,
      "learning_rate": 3.508214096449391e-06,
      "loss": 6.1327,
      "step": 26320
    },
    {
      "epoch": 4.652888182299947,
      "grad_norm": 6.8683695793151855,
      "learning_rate": 3.4728846493552375e-06,
      "loss": 6.0278,
      "step": 26340
    },
    {
      "epoch": 4.656421127009362,
      "grad_norm": 7.438451766967773,
      "learning_rate": 3.437555202261085e-06,
      "loss": 6.0637,
      "step": 26360
    },
    {
      "epoch": 4.6599540717187775,
      "grad_norm": 5.2035298347473145,
      "learning_rate": 3.4022257551669317e-06,
      "loss": 5.9409,
      "step": 26380
    },
    {
      "epoch": 4.663487016428193,
      "grad_norm": 6.144775867462158,
      "learning_rate": 3.366896308072779e-06,
      "loss": 6.0838,
      "step": 26400
    },
    {
      "epoch": 4.667019961137608,
      "grad_norm": 4.841994762420654,
      "learning_rate": 3.331566860978626e-06,
      "loss": 6.1116,
      "step": 26420
    },
    {
      "epoch": 4.670552905847024,
      "grad_norm": 5.091202735900879,
      "learning_rate": 3.296237413884473e-06,
      "loss": 6.0627,
      "step": 26440
    },
    {
      "epoch": 4.674085850556438,
      "grad_norm": 5.820194244384766,
      "learning_rate": 3.2609079667903197e-06,
      "loss": 6.1184,
      "step": 26460
    },
    {
      "epoch": 4.677618795265854,
      "grad_norm": 6.621971607208252,
      "learning_rate": 3.2255785196961664e-06,
      "loss": 6.058,
      "step": 26480
    },
    {
      "epoch": 4.681151739975269,
      "grad_norm": 5.698827266693115,
      "learning_rate": 3.190249072602014e-06,
      "loss": 6.0945,
      "step": 26500
    },
    {
      "epoch": 4.684684684684685,
      "grad_norm": 9.690982818603516,
      "learning_rate": 3.1549196255078606e-06,
      "loss": 5.7671,
      "step": 26520
    },
    {
      "epoch": 4.6882176293941,
      "grad_norm": 5.728270053863525,
      "learning_rate": 3.119590178413708e-06,
      "loss": 6.0198,
      "step": 26540
    },
    {
      "epoch": 4.691750574103516,
      "grad_norm": 5.288609027862549,
      "learning_rate": 3.084260731319555e-06,
      "loss": 6.1122,
      "step": 26560
    },
    {
      "epoch": 4.69528351881293,
      "grad_norm": 5.967676639556885,
      "learning_rate": 3.048931284225402e-06,
      "loss": 6.1711,
      "step": 26580
    },
    {
      "epoch": 4.698816463522346,
      "grad_norm": 7.640608787536621,
      "learning_rate": 3.013601837131249e-06,
      "loss": 5.957,
      "step": 26600
    },
    {
      "epoch": 4.702349408231761,
      "grad_norm": 6.527276039123535,
      "learning_rate": 2.978272390037096e-06,
      "loss": 6.0839,
      "step": 26620
    },
    {
      "epoch": 4.705882352941177,
      "grad_norm": 7.413229465484619,
      "learning_rate": 2.9429429429429433e-06,
      "loss": 5.9237,
      "step": 26640
    },
    {
      "epoch": 4.709415297650592,
      "grad_norm": 5.33296012878418,
      "learning_rate": 2.90761349584879e-06,
      "loss": 6.0914,
      "step": 26660
    },
    {
      "epoch": 4.712948242360007,
      "grad_norm": 5.717296123504639,
      "learning_rate": 2.872284048754637e-06,
      "loss": 6.1245,
      "step": 26680
    },
    {
      "epoch": 4.716481187069422,
      "grad_norm": 7.303501129150391,
      "learning_rate": 2.836954601660484e-06,
      "loss": 5.9258,
      "step": 26700
    },
    {
      "epoch": 4.7200141317788376,
      "grad_norm": 5.746130466461182,
      "learning_rate": 2.801625154566331e-06,
      "loss": 6.0165,
      "step": 26720
    },
    {
      "epoch": 4.723547076488253,
      "grad_norm": 5.668290615081787,
      "learning_rate": 2.766295707472178e-06,
      "loss": 5.99,
      "step": 26740
    },
    {
      "epoch": 4.7270800211976685,
      "grad_norm": 9.49775505065918,
      "learning_rate": 2.730966260378025e-06,
      "loss": 6.0462,
      "step": 26760
    },
    {
      "epoch": 4.730612965907084,
      "grad_norm": 4.230391979217529,
      "learning_rate": 2.695636813283872e-06,
      "loss": 5.962,
      "step": 26780
    },
    {
      "epoch": 4.7341459106164985,
      "grad_norm": 6.04472017288208,
      "learning_rate": 2.6603073661897193e-06,
      "loss": 6.1242,
      "step": 26800
    },
    {
      "epoch": 4.737678855325914,
      "grad_norm": 4.843251705169678,
      "learning_rate": 2.6249779190955664e-06,
      "loss": 6.0401,
      "step": 26820
    },
    {
      "epoch": 4.741211800035329,
      "grad_norm": 5.880774974822998,
      "learning_rate": 2.5896484720014135e-06,
      "loss": 6.0684,
      "step": 26840
    },
    {
      "epoch": 4.744744744744745,
      "grad_norm": 5.532395362854004,
      "learning_rate": 2.5543190249072606e-06,
      "loss": 5.9909,
      "step": 26860
    },
    {
      "epoch": 4.74827768945416,
      "grad_norm": 5.417763710021973,
      "learning_rate": 2.5189895778131073e-06,
      "loss": 5.9884,
      "step": 26880
    },
    {
      "epoch": 4.751810634163576,
      "grad_norm": 5.947273254394531,
      "learning_rate": 2.4836601307189544e-06,
      "loss": 5.9944,
      "step": 26900
    },
    {
      "epoch": 4.75534357887299,
      "grad_norm": 7.000248908996582,
      "learning_rate": 2.448330683624801e-06,
      "loss": 6.0632,
      "step": 26920
    },
    {
      "epoch": 4.758876523582406,
      "grad_norm": 7.330691814422607,
      "learning_rate": 2.413001236530648e-06,
      "loss": 5.8837,
      "step": 26940
    },
    {
      "epoch": 4.762409468291821,
      "grad_norm": 4.776039123535156,
      "learning_rate": 2.3776717894364953e-06,
      "loss": 6.1299,
      "step": 26960
    },
    {
      "epoch": 4.765942413001237,
      "grad_norm": 6.057163715362549,
      "learning_rate": 2.3423423423423424e-06,
      "loss": 6.0205,
      "step": 26980
    },
    {
      "epoch": 4.769475357710652,
      "grad_norm": 10.61470890045166,
      "learning_rate": 2.3070128952481895e-06,
      "loss": 5.9307,
      "step": 27000
    },
    {
      "epoch": 4.773008302420067,
      "grad_norm": 4.472878932952881,
      "learning_rate": 2.2716834481540366e-06,
      "loss": 6.0065,
      "step": 27020
    },
    {
      "epoch": 4.776541247129482,
      "grad_norm": 5.03165864944458,
      "learning_rate": 2.2363540010598837e-06,
      "loss": 6.0718,
      "step": 27040
    },
    {
      "epoch": 4.780074191838898,
      "grad_norm": 5.6568098068237305,
      "learning_rate": 2.2010245539657304e-06,
      "loss": 6.0446,
      "step": 27060
    },
    {
      "epoch": 4.783607136548313,
      "grad_norm": 5.35399055480957,
      "learning_rate": 2.1656951068715775e-06,
      "loss": 5.9531,
      "step": 27080
    },
    {
      "epoch": 4.7871400812577285,
      "grad_norm": 6.751039028167725,
      "learning_rate": 2.1303656597774246e-06,
      "loss": 6.0322,
      "step": 27100
    },
    {
      "epoch": 4.790673025967144,
      "grad_norm": 7.693950176239014,
      "learning_rate": 2.0950362126832717e-06,
      "loss": 6.1578,
      "step": 27120
    },
    {
      "epoch": 4.794205970676559,
      "grad_norm": 5.321056365966797,
      "learning_rate": 2.0597067655891184e-06,
      "loss": 5.8824,
      "step": 27140
    },
    {
      "epoch": 4.797738915385974,
      "grad_norm": 7.605291366577148,
      "learning_rate": 2.0243773184949655e-06,
      "loss": 6.0684,
      "step": 27160
    },
    {
      "epoch": 4.801271860095389,
      "grad_norm": 6.611882209777832,
      "learning_rate": 1.9890478714008126e-06,
      "loss": 5.894,
      "step": 27180
    },
    {
      "epoch": 4.804804804804805,
      "grad_norm": 8.272735595703125,
      "learning_rate": 1.9537184243066598e-06,
      "loss": 6.0244,
      "step": 27200
    },
    {
      "epoch": 4.80833774951422,
      "grad_norm": 6.207754611968994,
      "learning_rate": 1.918388977212507e-06,
      "loss": 5.9311,
      "step": 27220
    },
    {
      "epoch": 4.811870694223636,
      "grad_norm": 6.74852180480957,
      "learning_rate": 1.8830595301183535e-06,
      "loss": 6.0178,
      "step": 27240
    },
    {
      "epoch": 4.81540363893305,
      "grad_norm": 4.695666313171387,
      "learning_rate": 1.8477300830242007e-06,
      "loss": 6.1008,
      "step": 27260
    },
    {
      "epoch": 4.818936583642466,
      "grad_norm": 5.8315019607543945,
      "learning_rate": 1.8124006359300478e-06,
      "loss": 5.9492,
      "step": 27280
    },
    {
      "epoch": 4.822469528351881,
      "grad_norm": 5.6344194412231445,
      "learning_rate": 1.7770711888358949e-06,
      "loss": 6.0546,
      "step": 27300
    },
    {
      "epoch": 4.826002473061297,
      "grad_norm": 6.9679789543151855,
      "learning_rate": 1.7417417417417418e-06,
      "loss": 5.9665,
      "step": 27320
    },
    {
      "epoch": 4.829535417770712,
      "grad_norm": 6.422839164733887,
      "learning_rate": 1.7064122946475889e-06,
      "loss": 6.0332,
      "step": 27340
    },
    {
      "epoch": 4.833068362480127,
      "grad_norm": 6.712424278259277,
      "learning_rate": 1.671082847553436e-06,
      "loss": 5.9703,
      "step": 27360
    },
    {
      "epoch": 4.836601307189542,
      "grad_norm": 5.389289379119873,
      "learning_rate": 1.635753400459283e-06,
      "loss": 6.0891,
      "step": 27380
    },
    {
      "epoch": 4.840134251898958,
      "grad_norm": 6.213842868804932,
      "learning_rate": 1.6004239533651298e-06,
      "loss": 6.1201,
      "step": 27400
    },
    {
      "epoch": 4.843667196608373,
      "grad_norm": 4.931994438171387,
      "learning_rate": 1.5650945062709769e-06,
      "loss": 6.0684,
      "step": 27420
    },
    {
      "epoch": 4.8472001413177885,
      "grad_norm": 5.3393964767456055,
      "learning_rate": 1.5297650591768238e-06,
      "loss": 5.9579,
      "step": 27440
    },
    {
      "epoch": 4.850733086027204,
      "grad_norm": 4.263763904571533,
      "learning_rate": 1.4944356120826709e-06,
      "loss": 6.1458,
      "step": 27460
    },
    {
      "epoch": 4.854266030736619,
      "grad_norm": 4.068600177764893,
      "learning_rate": 1.459106164988518e-06,
      "loss": 5.9805,
      "step": 27480
    },
    {
      "epoch": 4.857798975446034,
      "grad_norm": 5.350805759429932,
      "learning_rate": 1.423776717894365e-06,
      "loss": 5.968,
      "step": 27500
    },
    {
      "epoch": 4.8613319201554495,
      "grad_norm": 6.197907447814941,
      "learning_rate": 1.3884472708002122e-06,
      "loss": 6.1876,
      "step": 27520
    },
    {
      "epoch": 4.864864864864865,
      "grad_norm": 6.547647476196289,
      "learning_rate": 1.3531178237060591e-06,
      "loss": 5.9603,
      "step": 27540
    },
    {
      "epoch": 4.86839780957428,
      "grad_norm": 5.294433116912842,
      "learning_rate": 1.317788376611906e-06,
      "loss": 5.9551,
      "step": 27560
    },
    {
      "epoch": 4.871930754283696,
      "grad_norm": 5.908529758453369,
      "learning_rate": 1.2824589295177531e-06,
      "loss": 6.0419,
      "step": 27580
    },
    {
      "epoch": 4.87546369899311,
      "grad_norm": 5.79386043548584,
      "learning_rate": 1.2471294824236002e-06,
      "loss": 6.0897,
      "step": 27600
    },
    {
      "epoch": 4.878996643702526,
      "grad_norm": 8.078362464904785,
      "learning_rate": 1.2118000353294471e-06,
      "loss": 6.0649,
      "step": 27620
    },
    {
      "epoch": 4.882529588411941,
      "grad_norm": 9.515291213989258,
      "learning_rate": 1.1764705882352942e-06,
      "loss": 6.0253,
      "step": 27640
    },
    {
      "epoch": 4.886062533121357,
      "grad_norm": 6.3858137130737305,
      "learning_rate": 1.1411411411411411e-06,
      "loss": 6.0616,
      "step": 27660
    },
    {
      "epoch": 4.889595477830772,
      "grad_norm": 5.761775970458984,
      "learning_rate": 1.1058116940469882e-06,
      "loss": 6.0798,
      "step": 27680
    },
    {
      "epoch": 4.893128422540187,
      "grad_norm": 5.54755973815918,
      "learning_rate": 1.0704822469528351e-06,
      "loss": 5.9229,
      "step": 27700
    },
    {
      "epoch": 4.896661367249602,
      "grad_norm": 6.766955852508545,
      "learning_rate": 1.0351527998586822e-06,
      "loss": 5.9047,
      "step": 27720
    },
    {
      "epoch": 4.900194311959018,
      "grad_norm": 4.986313819885254,
      "learning_rate": 9.998233527645293e-07,
      "loss": 6.0368,
      "step": 27740
    },
    {
      "epoch": 4.903727256668433,
      "grad_norm": 4.487370491027832,
      "learning_rate": 9.644939056703765e-07,
      "loss": 5.9931,
      "step": 27760
    },
    {
      "epoch": 4.907260201377849,
      "grad_norm": 6.288881301879883,
      "learning_rate": 9.291644585762232e-07,
      "loss": 6.0229,
      "step": 27780
    },
    {
      "epoch": 4.910793146087264,
      "grad_norm": 5.228642463684082,
      "learning_rate": 8.938350114820703e-07,
      "loss": 5.9797,
      "step": 27800
    },
    {
      "epoch": 4.9143260907966795,
      "grad_norm": 5.017771244049072,
      "learning_rate": 8.585055643879174e-07,
      "loss": 6.1829,
      "step": 27820
    },
    {
      "epoch": 4.917859035506094,
      "grad_norm": 8.070319175720215,
      "learning_rate": 8.231761172937645e-07,
      "loss": 5.9837,
      "step": 27840
    },
    {
      "epoch": 4.9213919802155095,
      "grad_norm": 7.60261869430542,
      "learning_rate": 7.878466701996114e-07,
      "loss": 5.9942,
      "step": 27860
    },
    {
      "epoch": 4.924924924924925,
      "grad_norm": 6.495754241943359,
      "learning_rate": 7.525172231054584e-07,
      "loss": 5.8978,
      "step": 27880
    },
    {
      "epoch": 4.92845786963434,
      "grad_norm": 7.455160617828369,
      "learning_rate": 7.171877760113055e-07,
      "loss": 5.9507,
      "step": 27900
    },
    {
      "epoch": 4.931990814343756,
      "grad_norm": 4.76368522644043,
      "learning_rate": 6.818583289171525e-07,
      "loss": 6.0061,
      "step": 27920
    },
    {
      "epoch": 4.93552375905317,
      "grad_norm": 4.8043999671936035,
      "learning_rate": 6.465288818229995e-07,
      "loss": 6.1278,
      "step": 27940
    },
    {
      "epoch": 4.939056703762586,
      "grad_norm": 5.998010158538818,
      "learning_rate": 6.111994347288466e-07,
      "loss": 6.0346,
      "step": 27960
    },
    {
      "epoch": 4.942589648472001,
      "grad_norm": 5.878358364105225,
      "learning_rate": 5.758699876346936e-07,
      "loss": 6.1121,
      "step": 27980
    },
    {
      "epoch": 4.946122593181417,
      "grad_norm": 4.677191734313965,
      "learning_rate": 5.405405405405406e-07,
      "loss": 5.9964,
      "step": 28000
    },
    {
      "epoch": 4.949655537890832,
      "grad_norm": 5.511831760406494,
      "learning_rate": 5.052110934463876e-07,
      "loss": 6.061,
      "step": 28020
    },
    {
      "epoch": 4.953188482600248,
      "grad_norm": 5.1195478439331055,
      "learning_rate": 4.6988164635223464e-07,
      "loss": 6.0546,
      "step": 28040
    },
    {
      "epoch": 4.956721427309662,
      "grad_norm": 5.243521690368652,
      "learning_rate": 4.345521992580816e-07,
      "loss": 5.997,
      "step": 28060
    },
    {
      "epoch": 4.960254372019078,
      "grad_norm": 7.586874485015869,
      "learning_rate": 3.992227521639287e-07,
      "loss": 5.9502,
      "step": 28080
    },
    {
      "epoch": 4.963787316728493,
      "grad_norm": 4.657619476318359,
      "learning_rate": 3.638933050697757e-07,
      "loss": 6.0497,
      "step": 28100
    },
    {
      "epoch": 4.967320261437909,
      "grad_norm": 7.080089092254639,
      "learning_rate": 3.285638579756227e-07,
      "loss": 5.9812,
      "step": 28120
    },
    {
      "epoch": 4.970853206147324,
      "grad_norm": 5.7111310958862305,
      "learning_rate": 2.9323441088146976e-07,
      "loss": 5.9322,
      "step": 28140
    },
    {
      "epoch": 4.9743861508567395,
      "grad_norm": 6.066977024078369,
      "learning_rate": 2.5790496378731676e-07,
      "loss": 6.1484,
      "step": 28160
    },
    {
      "epoch": 4.977919095566154,
      "grad_norm": 6.788083553314209,
      "learning_rate": 2.2257551669316376e-07,
      "loss": 6.0078,
      "step": 28180
    },
    {
      "epoch": 4.9814520402755695,
      "grad_norm": 5.2032999992370605,
      "learning_rate": 1.872460695990108e-07,
      "loss": 5.9898,
      "step": 28200
    },
    {
      "epoch": 4.984984984984985,
      "grad_norm": 5.720491409301758,
      "learning_rate": 1.5191662250485782e-07,
      "loss": 6.1394,
      "step": 28220
    },
    {
      "epoch": 4.9885179296944,
      "grad_norm": 6.385091304779053,
      "learning_rate": 1.1658717541070483e-07,
      "loss": 5.8674,
      "step": 28240
    },
    {
      "epoch": 4.992050874403816,
      "grad_norm": 5.857601642608643,
      "learning_rate": 8.125772831655185e-08,
      "loss": 5.8412,
      "step": 28260
    },
    {
      "epoch": 4.9955838191132305,
      "grad_norm": 6.892728805541992,
      "learning_rate": 4.592828122239887e-08,
      "loss": 5.921,
      "step": 28280
    },
    {
      "epoch": 4.999116763822646,
      "grad_norm": 7.595795154571533,
      "learning_rate": 1.0598834128245893e-08,
      "loss": 6.159,
      "step": 28300
    }
  ],
  "logging_steps": 20,
  "max_steps": 28305,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 49553291504640.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
